\chapter{Preliminaries}
\label{c:preliminaries}



\section{Koopman Theory of Dynamical Systems}
	\todo{Rewrite this section on Koopman theory. Also use h as the observation function!}

	Out first contribution is based on the need of a linearization technique that generalized globally. We have seen this need in the previous chapter when looking at simple nonlinear systems and small angle approximations. This brings us directly to Koopman theory, original introduced by B.~Koopman in 1931~\cite{koopmanHamiltonianSystemsTransformation1931} in the context of Hamiltonian systems and transformations in Hilbert spaces. Considering a nonlinear discrete-time dynamical system
	\begin{equation*}
		\vec{s}_{t + 1} = \vec{F}(\vec{s}_t),\quad \vec{F} : \R^k \to \R^k
	\end{equation*}
	and observations \( \vec{g} : \R^p \to \R^p \) of this system, \ac{ie} \( \vec{g}_t \coloneqq \vec{g}(\vec{s}_t) \), the infinite-dimensional \emph{Koopman operator} \(\mathcal{K}\) advances all of the measurements forward in time. This relation can also be expressed as the composition
	\begin{equation*}
		\mathcal{K} \vec{g} \coloneqq \vec{g} \circ \vec{F} \qquad\iff\qquad \mathcal{K} \vec{g}(\vec{s}_t) = \vec{g}\big( \vec{F}(\vec{s}_t) \big) = \vec{g}(\vec{s}_{t + 1})
	\end{equation*}
	This relation is true for every possible measurement function \( \vec{g} \) at any point of the system space \( \R^k \)~\cite{bruntonKoopmanInvariantSubspaces2016}. All possible measurement functions \( \vec{g} \) span an infinite-dimensional Hilbert space \( \mathcal{G} \). A finite set of measurements \( \vec{g}_1, \vec{g}_2, \cdots, \vec{g}_p \) that span an invariant subspace \( \mathcal{G}' \subset \mathcal{G} \), \ac{ie} applying the Koopman operator to a linear combination of these functions keeps them in the subspace
	\begin{align*}
		\vec{g} &= \alpha_1 \vec{g}_1 + \alpha_2 \vec{g}_2 + \cdots + \alpha_p \vec{g}_p \\
		\mathcal{K} \vec{g} &= \beta_1 \vec{g}_1 + \beta_2 \vec{g}_2 + \cdots + \beta_p \vec{g}_p
	\end{align*}
	can be considered as a basis of that subspace. If that is possible, we can restrict the Koopman operator onto \( \mathcal{G}' \). Functions that both span the subspace \(\mathcal{G}'\) and do only scale when applying the Koopman operator, \ac{ie} \( \mathcal{K} \vec{\varphi} = \lambda \vec{\varphi} \) are called \emph{Eigenfunctions} of the Koopman operator. Finding these Eigenfunctions is extremely desirable, as it allows us to get a finite Koopman operator \( \mat{K} \) globally linearizing the dynamical system \( \vec{F} \).

	For this thesis, we do not focus on finding the actual Eigenfunctions but to just approximate them.
% end

\section{The Expectation-Maximization Algorithm}
	The \ac{em} algorithm, first introduced by Ceppellini et al. in 1955~\cite{ceppelliniEstimationGeneFrequencies1955} and popularized by Dempster et al. in 1977~\cite{dempsterMaximumLikelihoodIncomplete1977a} can be used for tackling the following problem: Assuming some model with latent (hidden) states \(\vec{x}\), observations \(\vec{y}\) and model parameters \(\vec{\theta}\), we want to maximize the likelihood \( p(\vec{y} \given \vec{\theta}) \) \ac{wrt} the latent states \(\vec{x}\) and the parameters \(\vec{\theta}\). However, the marginal distribution
	\begin{align*}
		p(\vec{y} \given \vec{\theta}) = \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}
	\end{align*}
	is generally intractable. As usual on maximum likelihood approaches, it is useful to not maximize the likelihood directly, but to maximize the log-likelihood
	\begin{align*}
		\mathcal{L}(\vec{\theta}) \coloneqq \log p(\vec{y} \given \vec{\theta}) = \log \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}
	\end{align*}
	instead. This yields the same maximum as the logarithm is strictly increasing. By introducing an auxiliary probability distribution \( q(\vec{x} \given \vec{y}) \) over the latent variables, we can rewrite the marginal and find a lower bound on \(\mathcal{L}\) by using Jensen's inequality~\cite{jensenFonctionsConvexesInegalites1906}:
	\begin{align}
		\mathcal{L}(\vec{\theta})
			&= \log \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}  \nonumber \\
			&= \log \int\! q(\vec{x} \given \vec{y}) \frac{p(\vec{x}, \vec{y} \given \vec{\theta})}{q(\vec{x} \given \vec{y})} \dd{\vec{x}}  \nonumber \\
			&\geq \int\! q(\vec{x} \given \vec{y}) \log \frac{p(\vec{x}, \vec{y} \given \vec{\theta})}{q(\vec{x} \given \vec{y})} \dd{\vec{x}}  \nonumber \\
			&= \int\! q(\vec{x} \given \vec{y}) \log p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}} - \int\! q(\vec{x} \given \vec{y}) \log q(\vec{x} \given \vec{y}) \dd{\vec{x}} \eqqcolon \mathcal{L}_\mathrm{EM}[q, \vec{\theta}]  \label{eq:emLowerBound}
	\end{align}
	This draws a lower bound \( \mathcal{L}_\mathrm{EM}[q, \vec{\theta}] \) on the log-likelihood \( \mathcal{L}(\vec{\theta}) \) we can maximize instead, maximizing the log-likelihood simultaneously. Note that this lower bound is in fact a functional of the distribution \( q(\vec{x} \given \vec{y}) \).

	The \ac{em} algorithm now iteratively maximizes the lower bound and thus indirectly maximizes the original objective, the likelihood \( p(\vec{y} \given \vec{\theta}) \). The E and M step are as follows:
	\begin{description}
		\item[E-Step] Infers the auxiliary distribution \( q(\vec{x} \given \vec{y}) \) using the current estimations of the parameters \( \vec{\theta} \) by maximizing the lower bound \ac{wrt} the auxiliary distribution.
		\item[M-Step] Infers the parameters \(\vec{\theta}\) using the current auxiliary distribution by maximizing the lower bound \ac{wrt} the parameters.
	\end{description}
	Expressed in equations with the index \( \cdot^{(n)} \) to denote the values of the \(n\)-th iteration, we get the procedure which will be repeated until convergence:
	\begin{description}
		\item[E-Step] \eqparbox[r]{emSteps}{\(\displaystyle q^{(n + 1)} \)} \(\displaystyle = \arg\max_{q}\, \mathcal{L}_\mathrm{EM}\big[ q, \vec{\theta}^{(n)} \big] \)
		\item[M-Step] \eqparbox[r]{emSteps}{\(\displaystyle \vec{\theta}^{(n + 1)} \)} \(\displaystyle = \arg\max_{\vec{\theta}}\, \mathcal{L}_\mathrm{EM}\big[ q^{(n + 1)}, \vec{\theta} \big] \)
	\end{description}
	Additionally, the E-step has the constraint that \(q(\vec{x} \given \vec{y})\) really is a probability distribution, so it must integrate to one:
	\begin{align*}
		\int\! q(\vec{x} \given \vec{y}) \dd{\vec{x}} = 1
	\end{align*}
	We can incorporate this into the maximization, \ac{eg} using Lagrange multipliers. Using a bit of variational calculus, it can be shown~\cite{bealVariationalAlgorithmsApproximate2003a} that the maximization is gained by choosing the auxiliary distribution \(q(\vec{x} \given \vec{y})\) to be the same as the distribution \( p(\vec{x} \given \vec{y}, \vec{\theta}) \). That is, we set
	\begin{align*}
		q^{(n + 1)}(\vec{x} \given \vec{y}) = p\big(\vec{x} \biggiven \vec{y}, \vec{\theta}^{(n)}\big)
	\end{align*}
	while holding the parameters \(\vec{\theta}\) fixed. This maximization turns the lower bound into an equality with the actual likelihood.

	For the M-step, we keep the auxiliary distribution fixed and maximize the lower bound \ac{wrt} the parameters \(\vec{\theta}\). As this involves taking the derivative of \(\mathcal{L}_\mathrm{EM}\) \ac{wrt} \(\vec{\theta}\), we can safely omit the right hand side of the lower bound as it does not depend on \(\vec{\theta}\). Hence, we get the M-step as:
	\begin{align*}
		\vec{\theta}^{(n + 1)}
			&= \arg\max_{\vec{\theta}}\, \mathcal{L}_\mathrm{EM}\big[ q^{(n)}, \vec{\theta} \big] \\
			&= \arg\max_{\vec{\theta}} \int\! q^{(n)}(\vec{x} \given \vec{y}) \log p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}} \\
			&= \arg\max_{\vec{\theta}} \int\! p\big(\vec{x} \biggiven \vec{y}, \vec{\theta}^{(n)}\big) \log p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}} \\
			&= \arg\max_{\vec{\theta}}\, \E_{p\big(\vec{x} \given \vec{y}, \vec{\theta}^{(n)}\big)}[\log p(\vec{x}, \vec{y} \given \vec{\theta})]
	\end{align*}
	The quantity to optimize, \( Q(\vec{\theta}) \coloneqq \E[\log p(\vec{x}, \vec{y} \given \vec{\theta})] \), is also called the \emph{expected complete log-likelihood} as it involves both the observables and the latent variables.

	As shown in~\cite{bealVariationalAlgorithmsApproximate2003a}, the lower bound turns into an equality after the E-step. Hence we are guaranteed to always rise the log-likelihood after each \ac{em} iteration if we do not already have the optimal auxiliary distribution and parameters. If this would be the case, both the E- and the M-step would not change anything, so our log-likelihood is monotonically increasing. Also we might not want to calculate the whole distribution in the E-step, but we might want to restrict our computations to sufficient statistics that cover our whole distribution. For the rest of this thesis, we know our distribution \( p(\vec{x} \given \vec{y}, \vec{\theta}) \) is Gaussian, so we can assume the auxiliary distribution to be Gaussian too, given that we set them equal. Hence, we only need to calculate the mean and correlation or covariance of each variable to cover the whole distribution and to be able to proceed. This will turn out to be really useful later on.

	We will now take a look at variational autoencoders and justify the title of this thesis even if we, as we will see in~\autoref{c:nonlinearGaussianKoopman}, derived an \ac{em} algorithm.
% end

\section{Variational Autoencoders and the Evidence Lower Bound}
	\acp{vae} have been first introduced in the context of the \ac{aevb} algorithm by Kingma and Welling in 2014~\cite{kingmaAutoEncodingVariationalBayes2014}. They tackle inference and learning in probabilistic models with latent variables, similar to the \ac{em} algorithm. To keep the notation analogous to the derivation of the \ac{em} algorithm, we deviate from~\cite{kingmaAutoEncodingVariationalBayes2014} in terms that we keep \(\vec{x}\) as our latent variables and \(\vec{y}\) as the observables.

	To perform inference, we want to maximize the (log-) likelihood
	\begin{align*}
		\mathcal{L}(\vec{\theta}) \coloneqq \log p(\vec{y} \given \vec{\theta}) = \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}
	\end{align*}
	by maximizing the \ac{elbo} \( \mathcal{L}_\mathrm{ELBO} \) which we can find by using Jensen's inequality~\cite{jensenFonctionsConvexesInegalites1906}:
	\begin{align}
		\mathcal{L}(\vec{\theta})
			&= \log \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}  \nonumber \\
			&= \log \int\! q(\vec{x} \given \vec{y}, \vec{\phi}) \frac{p(\vec{x}, \vec{y} \given \vec{\theta})}{q(\vec{x} \given \vec{y}, \vec{\phi})} \dd{\vec{x}}  \nonumber \\
			&\geq \int\! q(\vec{x} \given \vec{y}, \vec{\phi}) \log \frac{p(\vec{x}, \vec{y} \given \vec{\theta})}{q(\vec{x} \given \vec{y}, \vec{\phi})} \dd{\vec{x}}  \nonumber \\
			&= \int\! q(\vec{x} \given \vec{y}, \vec{\phi}) \log \frac{p(\vec{y} \given \vec{x}, \vec{\theta}) p(\vec{x} \given \vec{\theta})}{q(\vec{x} \given \vec{y}, \vec{\phi})} \dd{\vec{x}}  \nonumber \\
			&= \int\! q(\vec{x} \given \vec{y}, \vec{\phi}) \frac{p(\vec{y} \given \vec{x}, \vec{\theta})}{q(\vec{x} \given \vec{y}, \vec{\phi})} \dd{\vec{x}} + \int\! q(\vec{x} \given \vec{y}, \vec{\phi}) p(\vec{y} \given \vec{x}, \vec{\theta}) \dd{\vec{x}}  \nonumber \\
			&= -\KL{q(\vec{x} \given \vec{y}, \vec{\phi})}{p(\vec{y} \given \vec{x}, \vec{\theta})} + \E_{q(\vec{x} \given \vec{y}, \vec{\phi})}[p(\vec{y} \given \vec{x}, \vec{\theta})] \eqqcolon \mathcal{L}_\mathrm{ELBO}(\vec{\theta}, \vec{\psi})  \label{eq:elbo}
	\end{align}
	Note that we have introduced an auxiliary parametric distribution \( q(\vec{x} \given \vec{y}, \vec{\phi}) \) which originally leads to the first integral to be an expectation and makes the application of Jensen's inequality possible. We now maximize the \ac{elbo}~\eqref{eq:elbo} \ac{wrt} the variational and generative parameters \(\vec{\phi}\) and \(\vec{\theta}\), respectively using the \emph{reparametrization trick}~\cite{kingmaAutoEncodingVariationalBayes2014}.

	Shifting to \acp{vae}, we now represent the auxiliary distribution \( q(\vec{x} \given \vec{y}, \vec{\phi}) \) using a neural network with some prior \( p(\vec{x} \given \vec{\theta}) \), e.g. a standard Gaussian \( p(\vec{x} \given \vec{\theta}) = \normal(\vec{0}, \mat{I}) \) to keep the latents "close to the center", enforced by the \ac{kl} divergences in the \ac{elbo}. For a Gaussian latent distribution, the neural network, called the \emph{amortization network}, produces the mean and the diagonal covariance of \( q \). A second neural network is used for decoding the latent dimension, representing the decoding distribution \( p(\vec{y} \given \vec{x}, \vec{\theta}) \). If we assume a Gaussian decoding distribution with constant variance, the right side of the \ac{elbo}~\eqref{eq:elbo} just becomes the squared error between the decoding mean and the input \(\vec{y}\). Such a network architecture is illustrated in~\autoref{fig:vae}.

	\begin{figure}
		\centering
		\tikzVariationalAutoEncoder
		\caption{Illustration of a Variational Auto-Encoder with the amortization network on the left and the decoder network on the right. Notice that the green-ish neurons in the middle are not "real", deterministic, neurons, but represent the sampling section of the \ac{vae} where the reparametrization takes place. The input and output size (on the left and right, respectively) are the same as we want to reconstruct our original data from the smaller latent state in the middle. Also the "latent neurons" are smaller than the in- and output neurons to enforce an encoding into a lower dimension.}
		\label{fig:vae}
	\end{figure}

	\subsection{Connection between EM and VAEs}
		As we have seen, the log-likelihood
		\begin{align*}
			\mathcal{L}(\vec{\theta}) = \log \int\! p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}}
		\end{align*}
		gives rise to a lower bound \( \mathcal{L}_\mathrm{EM} \)~\eqref{eq:emLowerBound} and the \ac{elbo} \( \mathcal{L}_\mathrm{ELBO} \)~\eqref{eq:elbo}:
		\begin{gather*}
			\mathcal{L}_\mathrm{EM} = \int\! q(\vec{x} \given \vec{y}) \log p(\vec{x}, \vec{y} \given \vec{\theta}) \dd{\vec{x}} - \int\! q(\vec{x} \given \vec{y}) \log q(\vec{x} \given \vec{y}) \dd{\vec{x}} \\
			\mathcal{L}_\mathrm{ELBO} = -\KL{q(\vec{x} \given \vec{y}, \vec{\phi})}{p(\vec{y} \given \vec{x}, \vec{\theta})} + \E_{q(\vec{x} \given \vec{y}, \vec{\phi})}[p(\vec{y} \given \vec{x}, \vec{\theta})] \eqqcolon \mathcal{L}_\mathrm{ELBO}(\vec{\theta}, \vec{\psi})
		\end{gather*}
		The lower bounds look quite different, but they are in fact equivalent,
		\begin{align*}
			\mathcal{L}_\mathrm{EM} = \mathcal{L}_\mathrm{ELBO}
		\end{align*}
		as the whole difference is just that the \ac{elbo} uses a factorization \( p(\vec{x}, \vec{y} \given \vec{\theta}) = p(\vec{y} \given \vec{x}, \vec{\theta}) p(\vec{x} \given \vec{\theta}) \) and the \ac{em} lower bound does not.

		The maximization procedures differ in the following way:
		\begin{itemize}
			\item In the \ac{em} algorithm, we separately maximize the components of the lower bound, firstly finding the next auxiliary distribution \(q\) and then maximizing the lower bound \ac{wrt} the parameters.
			\item In \acp{vae}, we use an amortization network to model the auxiliary distribution \(q\) in a parameterized way. Then we maximize the lower bound \ac{wrt} to both the variational and the generative parameters at once.
		\end{itemize}
		An obvious advantage of the \ac{em} algorithm is that we are guaranteed to always rise our lower bound and we will never get worse. Also the \ac{em} algorithm requires less computation and has less parameters, depending on the model choices.
	% end
% end
