\chapter{The Nonlinear Gaussian Koopman Algorithm}  % TODO: Find better name.
\label{c:nonlinearGaussianKoopman}
\IMRADlabel{methods}



In this chapter we will introduce out contribution and the theoretical background of the \algname algorithm that we will implement as a proof of concept in~\autoref{c:experiments}. We will start by introducing the ideas that lead to the idea, then formulate and also solve the arising (approximate) inference problem.

By looking at the graphical model for an \ac{lgds} and the state transition model for a Koopman dynamical system in~\autoref{fig:lgdsKoopmanRelation}, we can see that there are lots of similarities. The first and most interesting similarity is that both systems assume a latent state that transitions linearly, either with a state dynamics matrix \(\mat{A}\) for the \ac{lgds} or with the Koopman operator\footnote{From now on, we assume a finite-dimensional matrix approximation \(\mat{K}\) of the Koopman operator \(\mathcal{K}\).} \(\mat{K}\). The greatest difference is that measurements in classic \ac{lgds} are taken linearly with an observation matrix \( \mat{C} \) and nonlinear in the Koopman system with the measurement function \( \vec{h}(\cdot) \).

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\resizebox{\linewidth}{!}{\tikzLinearGaussianDynamicalSystem}
		\caption{Graphical model of a linear Gaussian dynamical system with the latent state \(\vec{s}_t\) and the (linear) observations \(\vec{y}_t\). In contrast to the Koopman system, this system is not deterministic and the arrows represent probabilistic dependency rather than hard transitions.}
	\end{subfigure}%
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\resizebox{\linewidth}{!}{\tikzKoopmanOperator}
		\caption{State transition model of a Koopman dynamical system with the Koopman operator \( \mathcal{K} \) that can be approximated with a matrix \(\mat{K}\). In contrast to the \ac{lgds}, the state is represented via nonlinear transitions and the observations are the linear dynamics. \\ Adopted from~\cite{bruntonKoopmanInvariantSubspaces2016}.}
	\end{subfigure}
	\caption{Side-by-side comparison of the a \ac{lgds} on the left and a Koopman dynamical system on the right. This side-by-side view highlights our idea of interpreting an Koopman system as a semi-linear dynamical system (\ac{ie} an \ac{lgds} with nonlinear observations).}
	\label{fig:lgdsKoopmanRelation}
\end{figure}

Our idea is to "flip" the Koopman dynamical system and replace the observation matrix \( \mat{C} \) with a nonlinear observation function \( \vec{g}(\cdot) \), that takes the linear states \( \vec{s}_t \) and maps them into a nonlinear observation space \( \vec{y}_t \). In other words, we seek to find the inverse function of \( \vec{h}(\cdot) \) to map out of the linear embedding that Koopman theory guarantees us to exist. Our belief that such an inverse function exists is backed by previous accomplishments in data-driven Koopman analysis~\cite{luschDeepLearningUniversal2018} that also seek and find such an inverse mapping (see~\ref{c:relatedWork} for more information).

Additionally, we contribute a probabilistic view on the Koopman operator, being able to gauche our uncertainty about the embedding and the inverse mapping to the observation space. Speaking of the observation space, this is a good point to clear up chaos of notation and names that builds up when working with two systems where "observation" means contrary concepts. From now on, we will work on the graphical system shown in~\autoref{fig:nonlinearGaussianKoopman} characterized by the dynamics
\begin{align*}
	\vec{s}_{t + 1} &= \eqmakebox[ngkIntro][l]{\( \mat{A} \vec{s}_t + \vec{w}_t,\quad \vec{w}_t \)} \sim \normal(\vec{0}, \mat{Q}) \\
	\vec{y}_t       &= \eqmakebox[ngkIntro][l]{\( \vec{g}(\vec{s}_t) + \vec{v}_t,\quad \vec{v}_t \)} \sim \normal(\vec{0}, \mat{R})
\end{align*}
that can equivalently be formulated as
\begin{align*}
	\vec{s}_{t + 1} &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
	\vec{y}_t       &\sim \normal\big(\vec{g}(\vec{s}_t), \mat{R}\big)
\end{align*}
We call \( \vec{s}_t \) the \emph{latent variables} or \emph{latents} in the \emph{latent space} with dimensionality \(k\), \( \vec{y}_t \) the \emph{observation variables} or \emph{observations} in the \emph{observation space} with dimensionality \(p\), \( \vec{g} : \R^k \to \R^p \) the \emph{observation function} mapping latents to observations, \( \mat{A} \) the \emph{(state) dynamics matrix}, \( \mat{Q} \) the state covariance matrix and \( \mat{R} \) the observation covariance matrix.

\begin{figure}
	\centering
	\tikzNonlinearGaussianKoopman
	\caption{The graphical model of the \algname model. Given an observation sequence \( \vec{y}_{1:T} \), we seek the latent dynamics and the corresponding nonlinear mapping \( \vec{g}(\cdot) \) from the latent space to the observations.}
	\label{fig:nonlinearGaussianKoopman}
\end{figure}

%\section{Formulating and Solving the Nonlinear Approximate Inference Problem using an Approximate EM Algorithm}
\section{Formulating and Solving the Inference Problem using an EM Algorithm}
	% Formulate likelihood, expected likelihood.
	% Shortly outline how to do the derivation and reference appendix.
	% Summarize M-step equations.
	% Combine ideas from ch. 2 (cubature and sqrt) and summarize.

	%Instead of tackling the inference problem with some kind of \ac{vae}, we choose to use an \ac{em} algorithm as it requires less training because of putting more prior knowledge and assumptions into the model, like the Gaussian noise or the specific

	We seek to find the matrices \(\mat{A}\), \(\mat{Q}\) and \(\mat{R}\) and the observation function \( \vec{g}_{\vec{\theta}}(\cdot) \), parameterized by \(\vec{\theta}\) and the initial state distribution \( \vec{s}_1 \sim \normal(\vec{m}_0, \mat{V}_0) \) that maximize the likelihood
	\begin{equation*}
		p(\vec{s}_{1:T}, \vec{y}_{1:T}) = p(\vec{s}_1) \prod_{t = 2}^{T} p(\vec{s}_t \given \vec{s}_{t - 1}) \prod_{t = 1}^{T} p(\vec{y}_t \given \vec{s}_t)
	\end{equation*}
	which factors over the conditional distributions due to the Markovian assumption. For brevity, we will keep the parameters \( \vec{\theta} \) of the observation function \( \vec{g}_{\vec{\theta}}(\cdot) \) implicit from now on. Subsequently, we can proceed by not maximizing the likelihood but the log-likelihood which is easier to maximize as the Gaussian lies in the exponential family, exhibiting a convex optimization problem\footnote{Strictly speaking, the optimization problem is only convex \ac{wrt} \(\mat{A}\) and \(\mat{Q}\). For \(\mat{R}\) and \(\vec{\theta}\), the problem is still non-convex, but numerically more stable as sums is easier to differentiate and to compute than products.}:
	\begin{equation*}
		\log p(\vec{s}_{1:T}, \vec{y}_{1:T}) = \log p(\vec{s}_1) + \sum_{t = 2}^{T} \log p(\vec{s}_t \given \vec{s}_{t - 1}) + \sum_{t = 1}^{T} \log p(\vec{y}_t \given \vec{s}_t)
	\end{equation*}
	Inserting the Gaussian distributions
	\begin{equation*}
		p(\vec{s}_1) = \normal(\vec{s}_1 \given \vec{m}_0, \mat{V}_0) \qquad\qquad
		p(\vec{s}_t \given \vec{s}_{t - 1}) = \normal(\vec{s}_t \given \mat{A} \vec{s}_{t - 1}, \mat{Q}) \qquad\qquad
		p(\vec{y}_t \given \vec{s}_t) = \normal\big(\vec{y}_t \given \vec{g}(\vec{s}_t), \mat{R}\big)
	\end{equation*}
	yields the log-likelihood
	\begin{align*}
		\log p(\vec{s}_{1:T}, \vec{y}_{1:T})
			&= -\frac{T(k + p)}{2} \log(2\pi) - \frac{1}{2} \log \lvert \mat{V}_0 \rvert - \frac{T - 1}{2} \log \lvert \mat{Q} \rvert - \frac{T}{2} \log \lvert \mat{R} \rvert \\
			&\qquad\qquad -\frac{1}{2} (\vec{s}_1 - \vec{m}_0)^T \mat{V}_0^{-1} (\vec{s}_1 - \vec{m}_0) \\
			&\qquad\qquad -\frac{1}{2} \sum_{t = 2}^{T} (\vec{s}_t - \mat{A} \vec{s}_{t - 1})^T \mat{Q}^{-1} (\vec{s}_t - \mat{A} \vec{s}_{t - 1}) \\
			&\qquad\qquad -\frac{1}{2} \sum_{t = 1}^{T} (\vec{y}_t - \vec{g}_t)^T \mat{R}^{-1} (\vec{y}_t - \vec{g}_t)
	\end{align*}
	with the abbreviation \( \vec{g}_t \coloneqq \vec{g}(\vec{s}_t) \).

	But maximizing this quantity is impossible as we do not have the latent states \( \vec{s}_{1:T} \). Hence, as usual in \ac{em} \todo{see section xy}, we instead maximize the expected log-likelihood and estimate the latent states based on out previous guess for the state dynamics. These two steps form our E- and M-step of the \algname algorithm:
	\begin{itemize}
		\item E-Step: Estimate the latent states \( \vec{s}_{1:T} \).
		\item M-Step: Maximize the expected log-likelihood \ac{wrt} the state dynamics and observation function parameters.
	\end{itemize}
	We will proceed with deriving the M-step and finish this section off by deriving the E-step. Please be aware that we only briefly touch the derivation here and focus on the important steps. See~\autoref{app:fullNgkDerivation} for the complete derivation with explanations throughout every step.

	For the M-step, we first need the expected log-likelihood \( Q \coloneqq \E_{\vec{s}_{1:T}}\big[ p(\vec{s}_{1:T}, \vec{y}_{1:T}) \big] \) as stated previously. This quantity depends on three expectations that be estimate in the E-step:
	\begin{align*}
		\hat{\vec{s}}_{t \subgiven t_0}^{(n)}  & \coloneqq \E\Big[\vec{s}_t^{(n)} \Biggiven \vec{y}_{1:t_0}\Big]                             & \hat{\vec{s}}_{t \subgiven t_0}           & \coloneqq \frac{1}{N} \sum_{n = 1}^{N} \hat{\vec{s}}_{t \subgiven t_0}^{(n)}  \\
		\mat{P}_{t \subgiven t_0}^{(n)}        & \coloneqq \E\Big[\vec{s}_t^{(n)} \vec{s}_t^{(n), T} \bigggiven \vec{y}_{1:t_0}\Big]       & \mat{P}_{t \subgiven t_0}        & \coloneqq \frac{1}{N} \sum_{n = 1}^{N} \mat{P}_{t \subgiven t_0}^{(n)}        \\
		\mat{P}_{t, t - 1 \subgiven t_0}^{(n)} & \coloneqq \E\Big[\vec{s}_t^{(n)} \vec{s}_{t - 1}^{(n), T} \bigggiven \vec{y}_{1:t_0}\Big] & \mat{P}_{t, t - 1 \subgiven t_0} & \coloneqq \frac{1}{N} \sum_{n = 1}^{N} \mat{P}_{t, t - 1 \subgiven t_0}^{(n)}
	\end{align*}
	These expectations are called the expected state, the self-correlation and the cross-correlation, respectively. We also silently introduced an upper index, \( \cdot^{(n)} \), which indicated which observation sequence our observation is from, because the algorithm can simultaneously learn on \(N\) observation sequences. For brevity, we may write \( \hat{\vec{s}}_t \) instead of \( \hat{\vec{s}}_{t \subgiven T} \) (analogous for all other values). Additionally, we have to define two quantities for the expectations of the nonlinear observation function \( \vec{g} \) that we cannot evaluate in closed form:
	\begin{align*}
		\hat{\vec{g}}_t^{(n)} &\coloneqq \E\Big[\vec{g}_t^{(n)} \Biggiven \vec{y}_{1:T}\Big] \\
		\mat{G}_t^{(n)}       &\coloneqq \E\Big[\vec{g}_t^{(n)} \vec{g}_t^{(n), T} \Biggiven \vec{y}_{1:T}\Big]
	\end{align*}
	We can now evaluate the expected log-likelihood depending on the above quantities using the trace-trick\footnote{The trace of a product of matrices/vectors is invariant under even permutations of the matrices/vectors within the trace.}, giving the following (enormous) quantity:
	\begin{align*}
		Q
			&= \E_{\vec{s}_{1:T}}\big[ p(\vec{s}_{1:T}, \vec{y}_{1:T}) \big] \\
			&= -\frac{NT(k + p)}{2} \ln(2\pi) - \frac{N}{2} \ln \lvert \mat{V}_0 \rvert - \frac{N(T - 1)}{2} \ln \lvert \mat{Q} \rvert - \frac{NT}{2} \ln \lvert \mat{R} \rvert \\
			&\qquad\qquad -\frac{N}{2} \tr\!\Big( \mat{P}_1 \mat{V}_0^{-1} \Big) + \frac{N}{2} \tr\!\Big( \hat{\vec{s}}_1 \vec{m}_0^T \mat{V}_0^{-1} \Big) + \frac{N}{2} \tr\!\Big( \vec{m}_0 \hat{\vec{s}}_1^T \mat{V}_0^{-1} \Big) - \frac{N}{2} \tr\!\Big(\vec{m}_0 \vec{m}_0^T \mat{V}_0^{-1} \Big) \\
			&\qquad\qquad -\frac{N}{2} \sum_{t = 2}^{T} \tr\!\Big( \mat{P}_t \mat{Q}^{-1} \Big) - \tr\!\Big( \mat{P}_{t, t - 1} \mat{A}^T \mat{Q}^{-1} \Big) - \tr\!\Big( \mat{A} \mat{P}_{t, t - 1} \mat{Q}^{-1} \Big) - \tr\!\Big( \mat{A} \mat{P}_{t - 1} \mat{A}^T \mat{Q}^{-1} \Big) \\
			&\qquad\qquad -\frac{1}{2} \sum_{n = 1}^{N} \sum_{t = 1}^{T} \tr\!\Big( \vec{y}_t^{(n)} \vec{y}_t^{(n), T} \mat{R}^{-1} \Big) - \tr\!\Big( \vec{y}_t^{(n)} \hat{\vec{g}}_t^{(n), T} \mat{R}^{-1} \Big) - \tr\!\Big( \hat{\vec{g}}_t^{(n)} \vec{y}_t^{(n), T} \mat{R}^{-1} \Big) + \tr\!\Big( \mat{G}_t^{(n)} \mat{R}^{-1} \Big)
	\end{align*}
	By maximizing this quantity \ac{wrt} the dynamics matrix \(\mat{A}\), the covariance matrices \(\mat{Q}\) and \(\mat{R}\), the initial state mean \(\vec{m}_0\) and the initial state covariance \(\mat{V}_0\), \ac{ie} setting the derivative \ac{wrt} these parameters to zero and rearranging the equations, we get the closed-form solutions
	\begin{align*}
		\mat{A}^\new   &= \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \!\Bigg) \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t - 1} \!\Bigg)^{-1} \\
		\mat{Q}^\new   &= \frac{1}{T - 1} \Bigg(\! \sum_{t = 2}^{T} \mat{P}_t - \mat{A}^\new \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \!\Bigg) \\
		\vec{m}_0^\new &= \hat{\vec{s}}_1 = \frac{1}{N} \sum_{n = 1}^{N} \hat{\vec{s}}_1^{(n)} \\
		\mat{V}_0^\new &= \mat{P}_1 - \hat{\vec{s}}_1 \hat{\vec{s}}_1^T \\
		\mat{R}^\new   &= \frac{1}{NT} \sum_{n = 1}^{N} \sum_{t = 1}^{T} \vec{y}_t^{(n)} \vec{y}_t^{(n), T} - \hat{\vec{g}}_t^{(n)} \vec{y}_t^{(n), T} - \vec{y}_t^{(n)} \hat{\vec{g}}_t^{(n), T} + \mat{G}_t^{(n), T}
	\end{align*}
	where we assume an optimal \( \vec{g}(\cdot) \) for the last equation for \(\mat{R}\). To maximize the expected log-likelihood \ac{wrt} the observation function \( \vec{g} \), we choose a learnable function approximator, \ac{eg} a neural network and maximize \(Q\) using simple gradient descent or a more evolved optimizer like Adam~\cite{kingmaAdamMethodStochastic2017}. We should note that we do not need to fully maximize \(Q\) in every M-step but that it is enough to just get better in every M-step for the convergence properties to still hold~\cite{dempsterMaximumLikelihoodIncomplete1977a}.

	We now turn to the problem of evaluating \( \hat{\vec{g}}_t^{(n)} \) and \( \mat{G}_t^{(n)} \). Instead of using Monte Carlo methods that are both very computationally expensive (especially as the gradient has to flow through the evaluation), we are employing the spherical-radial cubature rule that we have introduced in~\autoref{sec:cubatureRules}. That way, we can approximate the integrals both efficient and deterministically (as compared to Monte Carlo methods):
	\begin{align}
		\hat{\vec{g}}_t^{(n)} &= \int\! \vec{g}_t^{(n)} p\Big(\vec{s}_t^{(n)} \Biggiven \vec{y}_{1:T}\Big) \dd{\vec{s}_{1:T}^{(n)}} \approx \SRC\Big[\vec{g};\, \hat{\vec{s}}_t^{(n)}, \mat{V}_t^{(n)}\Big]  \label{eq:ngkgeval} \\
		\mat{G}_t^{(n)}       &= \int\! \vec{g}_t^{(n)} \vec{g}\vec{s}_t^{(n), T} p\Big(\vec{s}_t^{(n)} \Biggiven \vec{y}_{1:T} \Big) \dd{\vec{s}_{1:T}^{(n)}} \approx \SRC\Big[\vec{g} \vec{g}^T;\, \hat{\vec{s}}_t^{(n)}, \mat{V}_t^{(n)}\Big]  \label{eq:ngkGeval}
	\end{align}
	This concludes the derivation of the M-step, where we do not have explicit formulas for the observation parameters \(\vec{\theta}\) as we do learn these using a numerical optimizer.

	Deriving the E-step is quite straightforward given the groundwork we developed in~\autoref{sec:filteringSmoothing} and~\autoref{subsec:cubatureFiltering}. As we heavily employ cubature rules in the M-step (in fact, in every \ac{gd} iteration in the M-step), we chose to use a square-root Kalman filter and a square-root \ac{rts} smoother to directly get the Cholesky decomposition of the used covariance matrices. We apply~\autoref{alg:sqrtCubatureKalmanFilter} after afterwards~\autoref{alg:sqrtCubatureRtsSmoother} with the state dynamics function \( \vec{f} : \R^k \to \R^k : \vec{s} \mapsto \mat{A} \vec{s} \) and the observations function \( \vec{g}_{\vec{\theta}} \). Additionally to the covariances, we have to compute the self- and cross-correlation. Following~\cite{minkaHiddenMarkovModels1999}, they are given as
	\begin{align*}
		\mat{P}_t^{(n)} &= \hat{\mat{V}}_t^{(n)} - \hat{\vec{s}}_t^{(n)} \hat{\vec{s}}_t^{(n), T} \\
		\mat{P}_{t, t - 1}^{(n)} &= \hat{\mat{J}}_{t - 1} \hat{\mat{V}}_t - \hat{\vec{s}}_{t}^{(n)} \hat{\vec{s}}_{t - 1}^{(n), T}
	\end{align*}

	The whole \algname algorithm is outline in~\autoref{alg:ngk}.

	\begin{algorithm}  \DontPrintSemicolon
		\KwData{\(N\) observation sequences \( \vec{y}_{1:T}^{(n)} \in \R^{p \times T} \).}
		\textbf{Initialize} \(\mat{A}\), \(\mat{Q}\), \(\mat{R}\), \(\vec{m}_0\), \(\mat{V}_0\) and \(\vec{\theta}\) somehow (\ac{eg} identity matrices). \;
		\While{not converged}{
			\tcp{\textbf{E-Step:}}
			\( \displaystyle \vec{s}_{1:T}^{(1:N)},\, \hat{\mat{V}}_{1:T} \gets \text{\ac{rts}-Smoother} \) \;
			\eqmakebox[ngkestep][r]{\( \displaystyle \mat{P}_t^{(n)} \gets \)} \( \displaystyle \hat{\mat{V}}_t^{(n)} - \hat{\vec{s}}_t^{(n)} \hat{\vec{s}}_t^{(n), T} \) for \( t = 1, 2, \rangedots, T \) and \( n = 1, 2, \rangedots, N \) \;
			\eqmakebox[ngkestep][r]{\( \displaystyle \mat{P}_{t, t - 1}^{(n)} \gets \)} \( \displaystyle \hat{\mat{J}}_{t - 1} \hat{\mat{V}}_t - \hat{\vec{s}}_{t}^{(n)} \hat{\vec{s}}_{t - 1}^{(n), T} \) for \( t = 2, 3, \rangedots, T \) and \( n = 1, 2, \rangedots, N \) \;
			\tcp{\textbf{M-Step:}}
			\eqmakebox[ngkmstep][r]{\( \displaystyle \vec{\theta} \gets \)} \( \text{\ac{gd}}\big(Q(\vec{\theta})\big) \) \;
			\eqmakebox[ngkmstep][r]{\( \displaystyle \mat{R} \gets \)} \( \displaystyle \frac{1}{NT} \sum_{n = 1}^{N} \sum_{t = 1}^{T} \vec{y}_t^{(n)} \vec{y}_t^{(n), T} - \hat{\vec{g}}_t^{(n)} \vec{y}_t^{(n), T} - \vec{y}_t^{(n)} \hat{\vec{g}}_t^{(n), T} + \mat{G}_t^{(n), T} \) \;
			\eqmakebox[ngkmstep][r]{\( \displaystyle \mat{A} \gets \)} \( \displaystyle \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \!\Bigg) \Bigg(\! \sum_{t = 2}^{T} \mat{P}_{t - 1} \!\Bigg)^{-1} \) \;
			\eqmakebox[ngkmstep][r]{\( \displaystyle \mat{Q} \gets \)} \( \displaystyle \frac{1}{T - 1} \Bigg(\! \sum_{t = 2}^{T} \mat{P}_t - \mat{A}^\new \sum_{t = 2}^{T} \mat{P}_{t, t - 1} \!\Bigg) \) \;
			\eqmakebox[ngkmstep][r]{\( \displaystyle \vec{m}_0 \gets \)} \( \displaystyle \frac{1}{N} \sum_{n = 1}^{N} \hat{\vec{s}}_1^{(n)} \) \;
			\eqmakebox[ngkmstep][r]{\( \displaystyle \mat{V}_0 \gets \)} \( \displaystyle \mat{P}_1 - \hat{\vec{s}}_1 \hat{\vec{s}}_1^T \) \;
		}
		\caption{\algname}
		\label{alg:ngk}
	\end{algorithm}
% end

\section{Implementation}
	\label{sec:implementation}

	We implemented the \algname algorithm in Python using the popular array processing library NumPy~\cite{harrisArrayProgrammingNumPy2020} throughout the whole implementation, the deep learning library PyTorch~\cite{paszkePyTorchImperativeStyle2019} for the numerical optimization of \(\vec{\theta}\) and Sacred~\cite{greffSacredInfrastructureComputational2017} for experiment management. The whole implementation can be found on GitHub\footnote{See \href{https://github.com/fdamken/bachelors-thesis_code}{\texttt{https://github.com/fdamken/bachelors-thesis\_code}}.}.

	During the implementation we focused on making the algorithm as reusable as possible, separating all data generation and investigation code from the actual implementation. Separating the data generation also has the advantage that we can run benchmarks on the exact same data to compare model performance. We have separated the optimization of \(\vec{\theta}\) as much as possible (as outlined in~\autoref{alg:ngk}) and made it possible for the user of the algorithm to easily change the optimization algorithm to use in conjunction with the parameters of the algorithm like the learning rate (defaulting to Adam~\cite{kingmaAdamMethodStochastic2017} with a learning rate of \( 0.01 \)). We also made all of the following configurable:
	\begin{itemize}
		\item Whether to perform whitening of the observation data or not. That is, if we perform whitening, we normalize and standardize the training observations beforehand using \( \hat{\vec{y}}_{1:T, i} = (\vec{y}_{1:T, i} - \bar{\vec{y}}_i) / \sigma_{y_i} \) where \( \bar{\vec{y}}_i \) is the mean and \( \sigma_{y_i} \) is the standard deviation of the \(i\)-th component of all measurements. Doing the whitening first sometimes improves the performance as we will see in~\ref{c:experiments}.
		\item The convergence precision, \ac{ie} how much the log-likelihood has to change in order to continue the \ac{em} iterations. We will see that, due to numerical instabilities and approximation errors in the cubature rules, the algorithm rarely really converges before becoming numerically unstable.
		\item That is why we also made the maximum number of iterations configurable to support \emph{early stopping}. Once the algorithm performed these number of \ac{em} iterations, it will abort and report the results.
		\item Similarly it is possible to configure the convergence precision and maximum number of iterations for the subsequent optimization of \(\vec{\theta}\).
	\end{itemize}
	These are only the parameters that can be configured directly on the \ac{em} algorithm, but the user is also able to specify how to initialize everything, \ac{ie} the matrices \( \mat{A} \), \( \mat{Q} \), \( \mat{R} \), \( \mat{V}_0 \), \( \vec{m}_0 \) and the parameters \( \vec{\theta} \) of the observation function \( \vec{g}_{\vec{\theta}} \). Speaking of the observation function, we used a neural network with \( \tanh \) activation function and a single hidden layer for all experiments (see~\autoref{c:experiments} for more information about the experiment setup). We made it also possible to pass arbitrary Torch modules as the function approximator that can be learned via a \ac{gd}-like algorithm (\ac{ie} they must be differentiable and expose parameters).

	For learning multiple observation sequences, as outlined in the derivation of the \algname algorithm in~\autoref{c:nonlinearGaussianKoopman}, it turns out to be useful to add noise to the training data which has the effect of regularizing the algorithm a bit. This effect is visible in smoother log-likelihood curves as we will see in~\autoref{c:experiments}.

	\subsection{Insights, Problems and Solutions}
		In this section we will focus on interesting insights and problems we found during the implementation and how to (possibly) solve them.

		\subsubsection{Slow QR Decomposition on GPU}
			During our implementation we found found that QR decomposition is relatively slow on the \ac{gpu} due to a bug in PyTorch\footnote{See \href{https://web.archive.org/web/20201110121407/https://github.com/pytorch/pytorch/issues/22573}{\texttt{https://github.com/pytorch/pytorch/issues/22573}}.} at the time of writing. While faster QR decomposition methods on a \ac{gpu} have been proposed~\cite{andersonCommunicationAvoidingQRDecomposition2011a}, we stuck to copying the data back and forth between \ac{cpu} and \ac{gpu} in every \ac{em} iteration.
		% end

		\subsubsection{Diagonal Covariance}
			As we assumed the noise to be independent\todo{Include that assumption in the theoretical section!}, we can safely omit the non-diagonal entries of the covariance \( \mat{Q} \), \( \mat{R} \) and \( \mat{V}_0 \). That way we further reduce the computational overhead that would other wise be generated as we still (even with square-root filtering) have to compute the Cholesky decomposition of \( \mat{Q} \) and \( \mat{R} \). We can now just take the square-root of the diagonal entries and get the Cholesky decomposition around ten times faster\todo{Cite something or add experiment?}.

			% >>> As = [np.diag(np.random.random(size=(1000,)) ** 2) for n in range(1000)]
			% >>> timeit.timeit(lambda: [np.linalg.cholesky(A) for A in As], number=10)
			% 50.62359843800368
			% >>> timeit.timeit(lambda: [np.diag(np.sqrt(np.diag(A))) for A in As], number=10)
			% 4.582914652011823
			% >>> 50.62359843800368 / 4.582914652011823
			% 11.046157801734486
		% end

		\subsubsection{Regularization and Learning Cholesky Decompositions}
			There are also a few points where our code is extensible to make it faster, improve numerical stability and so on (see also~\autoref{sec:futureWork}). The main points are regularization and/or learning the Cholesky decomposition directly.

			As we will see in~\autoref{c:experiments}, having singular and non-positive definite matrices is a big problem that should be assessed. We already implemented the square-root smoothing that increased the numerical stability through the smoothing by not allowing non-positive definite matrices to be produced by the smoothing at all (as we directly learn the Cholesky decomposition). Additional regularization might make sense at the computation of the covariance matrices \(\mat{Q}\), \(\mat{R}\) and \(\mat{V}_0\). Another approach would be to learn the Cholesky decomposition of the covariance matrices \(\mat{Q}\), \(\mat{R}\) and \(\mat{V}_0\) directly by replacing the explicit formulas with a \ac{gd} algorithm that directly learn the Cholesky or the square-root diagonal. As the optimization problem is still convex\todo{This is a hot take.}, finding the global optimum should be easily possible.

			However we sadly did not have more time to look into this idea.
		% end
	% end

	\subsection{Hyperparameters}
		One of the largest hyperparameter with high influence is obviously the architectural choice for \( \vec{g}_{\vec{\theta}} \). As it turns out, even simple models like a single hidden layer produce good results as we will see in~\autoref{c:experiments} and~\autoref{c:discussion}. A \( \tanh \) activation function turns out to be really suitable as it produces smooth results compared to \ac{eg} a step function or \ac{relu}. The other high-influence hyperparameter is the dimensionality \(k\) of the latent space, \ac{ie} how many dimensions the Koopman operator has to build up a linear system that, taking the observations, behaves the same as the original nonlinear system. We will further investigate into the dependence on this hyperparameter in~\autoref{subsec:experimentLatentDim}.

		On the same page is the maximum number of iterations in the numerical optimization of \( \vec{\theta} \). High maximum iterations seem to lead to overfitting on the neural network, causing the whole algorithm to not generalize well. On the other hand, too less iterations do not raise the likelihood enough to get reasonable results.
	% end
% end
