\chapter{Related Work}
\label{c:relatedWork}



In this chapter we put our work into the context of other papers, most of which use the Koopman operator or a similar approach for learning a linear embedding of nonlinear dynamical systems.

Starting with work from 2018~\cite{luschDeepLearningUniversal2018}, Lusch et al. utilize a classic autoencoder for finding a linear embedding. They use the \ac{mse} of the output of the decoder with the input to the encoder to force the embedding to be decodeable. In addition to this simple loss function, they add a matrix to approximate the Koopman operator between the two neural networks and put a loss function on the outcome compared with the next time step, forcing the embedding to advance linearly. A similar loss function is put on the embedding directly to enforce that not the autoencoder but the Koopman matrix forwards the states in time. They also add another loss function for regularization. All these loss functions are then put together with hand-tuned weights to a single loss function the two neural networks are trained on via gradient descent. Compared to our work, however, they do not impose a probabilistic view on the embedding or the decoded values, making uncertainty-aware control impossible. Another common approach of identifying the Koopman eigenfunctions is using \ac{dmd} or \ac{edmd}~\cite{bruntonKoopmanInvariantSubspaces2016,kaiserDatadrivenDiscoveryKoopman2020,williamsDataDrivenApproximation2015}. These approaches seek for a parsimonious representation in the linear embedding rather than reaching for high forecasting and state estimation precision.

An approach closer to ours is from 2019~\cite{mortonDeepVariationalKoopman2019a} and also treats the Koopman embedding probabilistically by not modeling the Koopman observations directly but treating the linear embedding as a series of states to be inferred. Additionally, they assume control inputs to be present in the linear embedding in the form of an additive control with a control matrix. Their model uses six neural networks for encoding the dynamics:
\begin{itemize}
	\item the \emph{decoder network} to model the decoding of the embedding states back to the nonlinear manifold,
	\item the \emph{temporal encoder network} using a bidirectional \ac{lstm} summarizing the time evolution,
	\item the \emph{initial observation inference network} to infer the initial observation,
	\item the \emph{observation encoder network} that is a \ac{rnn} taking the observations and outputting an encoding of the time evolution,
	\item an \emph{observation inference network} encoding a distribution over the observations given the output of the temporal and observation encoder networks and
	the \emph{conditional prior network} which outputs the next observation distribution conditioned on the previous one.
\end{itemize}
Compared to our work, their model is far more complex with more neural networks and way more parameters, possibly leading to higher overfitting. On the other hand, it is possible using (uncertainty-aware) \ac{mpc} on their model which is (currently) not possible using the \algname algorithm.

Non-Koopman-related work includes the recurrent Kalman filter~\cite{beckerRecurrentKalmanNetworks2019a} which introduces a new type of \acp{rnn}, the recurrent Kalman network. This network learns high-dimensional linear embeddings with factorized covariances, leading to simple Kalman update rules with scalar operations.
