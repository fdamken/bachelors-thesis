\chapter{Inference in Dynamical Systems}
\label{c:inferenceInDynamicalSystems}



Our second contribution is a probabilistic perspective on "classical" Koopman theory as presented in various publications~\cite{bruntonKoopmanInvariantSubspaces2016,hanDeepLearningKoopman2020,kaiserDatadrivenDiscoveryKoopman2020,luschDeepLearningUniversal2018,williamsDataDrivenApproximation2015}. In this chapter we will build the groundwork for variational inference to combine them to the single \algname algorithm presented in~\autoref{c:nonlinearGaussianKoopman}.

\section{Hidden Markov Models and LGDS}
	\acp{hmm} are simple Bayesian networks described by a non-observable (\emph{hidden}) Markov chain. This non-observable discrete chain can be indirectly observed with observations that are emitted by every state. One of the key features of a Markov chain is that a state does only depend on the previous state, but not states before that. That is, knowing only the previous state is sufficient and no more information can be gathered by knowing every other state coming before. This is described by the state transition distribution
	\begin{equation*}
		s_{k + 1} \sim p(s_{k + 1} \given s_k)
	\end{equation*}
	being only dependent on the previous state \(s_k\). Analogous, a observation \(\vec{y}_k\) of a state \(s_k\) is only dependent on that specific state:
	\begin{equation*}
		\vec{y}_k \sim p(\vec{y}_k \given s_k)
	\end{equation*}
	These assumptions are called the \emph{Markov property} and a system fulfilling this property is called \emph{Markovian}. These conditional distributions can be written in a graphical model as shown in~\autoref{fig:hiddenMarkovModel}. Note that, in general, no assumption has to be made on the "type" of state/observation (\ie whether it is a scalar, a vector or something completely different). Also, no assumption is made on the specific transition distributions, \acp{hmm} can also be used to model deterministic transitions using a Dirac delta distribution.

	\begin{figure}
		\centering
		\tikzHiddenMarkovModel
		\caption[Illustration of a hidden Markov model]{Illustration of a Hidden Markov Model with states \(s_k\) and emis\-sions/observations \(\vec{y}_k\).}
		\label{fig:hiddenMarkovModel}
	\end{figure}

	A set of closely related dynamics models are \acp{lgds} which are described by a noisy state transition
	\begin{equation*}
		\vec{s}_{t + 1} = \mat{A} \vec{s}_t + \vec{w}_t,\quad \vec{w}_t \sim \normal(\vec{0}, \mat{Q})
	\end{equation*}
	with covariance matrix \(\mat{Q}\). Similarly, the observations \( \vec{y}_t = \mat{C} \vec{s}_t + \vec{v}_t \) underlie a Gaussian noise \( \vec{v}_t \sim \normal(\vec{0}, \mat{R}) \) too. Together they form the probabilistic model
	\begin{align*}
		\vec{s}_{t + 1} &= p(\mat{A} \vec{s}_t, \vec{Q}) \\
		\vec{y}_t &= p(\mat{C} \vec{s}_t, \mat{R})
	\end{align*}
	which fulfills the Markov property, hence the system is Markovian:
	\begin{equation*}
		p(\vec{s}_{1:T}, \vec{y}_{1:T}) = p(\vec{s}_1) \prod_{t = 2}^{T} p(\vec{s}_t \given \vec{s}_{t - 1}) \prod_{t = 1}^{T} p(\vec{y}_t \given \vec{s}_t)
	\end{equation*}
	Note that we write \( \vec{s}_{t_0:t_1} \) for the sequence \( \big( \vec{s}_{t_0}, \vec{s}_{t_0 + 1}, \rangedots, \vec{s}_{t_1} \big) \) and analogous for the observation sequence \( \vec{y}_{t_0:t_1} \). The probabilistic model of a \ac{lgds} is shown in~\autoref{fig:lgds}.

	This system is similar to a \ac{hmm} and in fact a \ac{lgds} really is an \ac{hmm}, just with continuous states \( \vec{s}_t \in \R^k \) and observations \( \vec{y}_t \in \R^p \).

	\begin{figure}
		\centering
		\tikzLinearGaussianDynamicalSystem
		\caption[Illustration of a LGDS]{Illustration of a Linear Gaussian Dynamical System with states \(\vec{x}_t\) and observations \(\vec{y}_t\). Solid arrows represent probabilistic dependency, where the matrix \(\mat{A}\) is the dynamics matrix indicating that the mean transitions linearly, as do the observations with the observation matrix \(\mat{C}\).}
		\label{fig:lgds}
	\end{figure}
% end

\section{Inference: Filtering and Smoothing}
	\label{sec:filteringSmoothing}

	When looking at an \ac{lgds}, one of the first problems that one might consider is how to get the latent states back? This procedure is called \emph{inference}. In this section we will give an overview on how to perform inference on an \ac{lgds}, meaning on a linear system that is Markovian.

	For time-series data, the inference procedure is widely know as \emph{filtering} and \emph{smoothing}, where the following distributions on the latent states \( \vec{s}_t \) are calculated:
	\begin{itemize}
		\item Filtering: \tabto{2.5cm} \( p(\vec{s}_t \given \vec{y}_{1:t}) \)
		\item Smoothing: \tabto{2.5cm} \( p(\vec{s}_t \given \vec{y}_{1:T}) \)
	\end{itemize}
	That is, the filtered distribution only depends on the observations until that time, where in contrast the smoothed distribution has used all data to the last time step \(T\) to estimate the latent states. An intermediate step in the filtering algorithm is called \emph{prediction} where the distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \) is being calculated, using only the data to the previous time step. The differences between what data is used in the three steps is shown in~\autoref{fig:predictionFilteringSmoothing}.

	We will now take a look on the most used filter, the well-known \emph{Kalman Filter} and the \emph{Rauch-Tung-Striebel Smoother}, which is also sometimes called the \emph{Kalman Smoother} as it requires a run of the Kalman filter first.

	\begin{figure}
		\centering
		\tikzPredictionFilteringSmoothing
		\caption[Illustration of the differences between prediction, filtering and smoothing]{This diagram shows the main differences between prediction, filtering and smoothing. The shaded region describes the observations that have been used to estimate the state at time step \(t\), indicated by a solid line. The dotted line represents time step \(t - 1\), to which all observations are used in all three procedures. \\ Adopted from~\cite{solinCubatureIntegrationMethods2010}.}
		\label{fig:predictionFilteringSmoothing}
	\end{figure}

	\subsection{Kalman Filter}
		The Kalman filter was originally introduced by R.~Kalman in 1960~\cite{kalmanNewApproachLinear1960} in the context of signal processing to predict random signals, separate them from noise and detect signals of known form in the presence of noise~\cite{kalmanNewApproachLinear1960}. For the normal Kalman filter we assume an underlying model of the form of an \ac{lgds} where the observations are detected and the state has to be estimated from the data. The process of state estimation is done in a two-step fashion:
		\begin{enumerate}
			\item Prediction: \tabto{2.5cm} Estimate the distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \)
			\item Correction: \tabto{2.5cm} Estimate the distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \)
		\end{enumerate}
		For a simple linear system with Gaussian noise, \ie an \ac{lgds}, the equations for both the prediction and the correction can be given in closed form and are fairly straightforward. For the prediction step, the equations are given as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven t - 1} &= \mat{A} \hat{\vec{s}}_{t - 1 \subgiven t - 1}  \label{eqn:kfStatePre} \\
			\hat{\mat{V}}_{t \subgiven t - 1} &= \mat{A} \hat{\mat{V}}_{t - 1 \subgiven t - 1} \mat{A}^T + \mat{Q}  \label{eqn:kfCovPre}
		\end{align}
		where \( \hat{\vec{s}}_{t \given t - 1} \) and \( \hat{\mat{V}}_{t \given t - 1} \) are the mean and covariance of the prediction distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \), respectively. The correction step, which integrates new knowledge gained from the observation \( \vec{y}_t \) into the estimate, is given as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven t} &= \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t \tilde{\vec{y}}_t  \label{eqn:kfStatePost} \\
			\hat{\mat{V}}_{t \subgiven t} &= \hat{\mat{V}}_{t \subgiven t - 1} - \mat{K}_t \mat{S}_t \mat{K}_t^T  \label{eqn:kfCovPost}
		\end{align}
		with the auxiliary variables
		\begin{align}
			\tilde{\vec{y}}_t &= \vec{y}_t - \mat{C} \hat{\vec{s}}_{t \subgiven t - 1}  \label{eqn:kfInno} \\
			\mat{S}_t &= \mat{C} \hat{\mat{V}}_{t \subgiven t - 1} \mat{C}^T + \mat{R}  \label{eqn:kfResCov} \\
			\mat{K}_t &= \hat{\mat{V}}_{t \subgiven t - 1} \mat{C}^T \mat{S}_t^{-1}  \label{eqn:kfKalmanGain}
		\end{align}
		known as the innovation \(\tilde{\vec{y}}\), residual covariance \(\mat{S}_t\) and Kalman gain \(\mat{K}_t\). The resulting estimates \( \hat{\vec{s}}_{t \subgiven t} \) and \( \hat{\mat{V}}_{t \subgiven t} \) then form the mean and covariance of the filtered distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \) respectively. These equations form a recursive algorithm starting off with initial state estimates \( \hat{\vec{s}}_{0 \subgiven 0} = \vec{m}_0 \) and \( \hat{\mat{V}}_{0 \subgiven 0} = \mat{V}_0 \) that have to be determined in externally.

		But these equations only work for linear systems. Lots of extensions of the regular Kalman filter have been proposed to extend the filter to nonlinear systems. One of them is the \ac{ekf} which assumes locally linear dynamics and uses first-order Taylor expansions to linearize them. Throughout this thesis we will use the cubature Kalman filter, which we will introduce later in~\autoref{subsec:cubatureFiltering}. We have decided against using the \ac{ekf} as it is slightly less accurate as the cubature Kalman filter and as we need the Cholesky decompositions of the covariance matrices, which we can estimate directly using a square-root cubature Kalman filter (see~\autoref{subsec:cubatureFiltering}).
	% end

	\subsection{Rauch-Tung-Striebel\,/\,Kalman Smoother}
		The \ac{rts} was been introduced after the original Kalman filter by H.~E.~Rauch, F.~Tung and C.~T.~Striebel~\cite{rauchMaximumLikelihoodEstimates1965} for optimal smoothing of linear systems with Gaussian noise, \ie \ac{lgds}. With the smoother, we want to estimate the smoothed distribution, also called the \emph{posterior}
		\begin{equation*}
			p(\vec{s}_t \given \vec{y}_{1:T})
		\end{equation*}
		which uses all observations (see again~\autoref{fig:predictionFilteringSmoothing} for an illustration). By letting the Kalman filter "run" beforehand, we get the filtered distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \). The smoothing equations are given as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven T} &= \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t})  \label{eqn:rtsState} \\
			\hat{\mat{V}}_{t \subgiven T} &= \hat{\mat{V}}_{t \subgiven t} + \mat{J}_t (\hat{\mat{V}}_{t + 1 \subgiven T} - \hat{\mat{V}}_{t + 1 \subgiven t}) \mat{J}_t^T  \label{eqn:rtsCov}
		\end{align}
		with the auxiliary variable \( \mat{J}_t = \hat{\mat{V}}_{t \subgiven t} \mat{A}^T \hat{\mat{V}}_{t + 1 \subgiven t}^{-1} \). The resulting estimates \( \hat{\vec{s}}_{t \subgiven T} \) and \( \hat{\mat{V}}_{t \subgiven T} \) then form the mean and covariance of the smoothed/posterior distribution \( p(\vec{s}_t \given \vec{y}_{1:T}) \). These equations form a recursive algorithm starting off with the final state and covariance estimates of the filter.

		Hence, we need the Kalman filter before using the \ac{rts} smoother. Due to this hard wiring between the two algorithms, the \ac{rts} smoother is also often referred to as the Kalman smoother.

		Note that these equations only depend on the state dynamics matrix \( \mat{A} \), so a system that is linear in the state and nonlinear in the observations can still use the standard \ac{rts} smoother and no further modification is required.
	% end
% end

\section{Cubature Rules and Filtering}
	\label{sec:cubatureRules}

	As we have already discussed, the plain Kalman filter not applicable to nonlinear systems without modifications. In this section we want to motivate the need for \emph{cubature rules} and outline their derivation and how to apply them.

	Often when deriving probabilistic algorithms like \ac{em} or the Kalman filter, we have to evaluate expectations \( \E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] \) and hence integrals of the form:
	\begin{equation*}
		\E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] = \int\! \vec{f}(\vec{x}) p(\vec{x}) \dd{\vec{x}}
	\end{equation*}
	In this thesis and lots of other literature, we have Gaussian distributions \( p(\vec{x}) = \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \) with mean \(\vec{\mu}\) and covariance \(\mat{\Sigma}\), leading to Gaussian integrals of the form
	\begin{equation*}
		\E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] = \int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}}
	\end{equation*}
	which do not have closed form solutions for arbitrary, nonlinear, functions \( \vec{f}(\vec{x}) \). Two common approaches for approximating the expectations/integrals are Monte Carlo Estimation, leading to \ac{mcmc}, particle filters, and numerical integration methods, known as cubature\footnote{"Classical" numerical integration methods are called "quadrature" as they form a rectangle under the curve to approximate the area, while in higher dimensions this is more or less a "cube" or "hypercube", hence it is called "cubature".} rules.

	To approximate Gaussian integrals, we need a set of \(m\) \emph{sigma points} \( \vec{x}_i \) to evaluate \( \vec{f}(\vec{x}) \) at and a set weights \( w_i \) to form a sum over the sigma points:
	\begin{equation*}
		\int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \approx \sum_{i = 1}^{m} w_i \vec{f}(\vec{x}_i)
	\end{equation*}
	In this thesis we utilize the spherical-radial cubature rule~\cite{solinCubatureIntegrationMethods2010} which is a third-degree approximation for Gaussian integrals. A third-degree approximation means that "this rule is exact for all monomials up to degree three"~\cite[p. 18]{solinCubatureIntegrationMethods2010}. In this cubature rule, the approximation is a finite sum over \( 2n \) points, where \( n \) is the dimensionality of \(\vec{x}\), \ie \( \vec{x} \in \R^n \). With \( \mat{\Sigma}^{1/2} \) being the Cholesky decomposition of \( \mat{\Sigma} \), so \( \mat{\Sigma} = \mat{\Sigma}^{1/2} \mat{\Sigma}^{1/2, T} \), the approximation is given as
	\begin{equation*}
		\int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \approx \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\mat{\Sigma}^{1/2} \vec{\xi}_i - \vec{\mu})
	\end{equation*}
	with the cubature points \( \vec{\xi}_i = \sqrt{n} [\vec{1}]_i \). Here the points \( [\vec{1}]_i \) are "from the intersections between the Cartesian axes and the \(n\)-dimensional unit hypersphere"~\cite{solinCubatureIntegrationMethods2010}. That is, the point \( [\vec{1}]_i \) is the \(i\)-th row vector of the block matrix \( \begin{bmatrix} \mat{I} & -\mat{I} \end{bmatrix} \), where \( \mat{I} \) is the \(n\)-dimensional identity matrix. In the rest of this thesis we will write
	\begin{equation*}
		\SRC[\vec{f}; \vec{\mu}, \mat{\Sigma}] \coloneqq \sum_{i = 1}^{m} w_i \vec{f}(\vec{x}_i)
	\end{equation*}
	for evaluations of the spherical-radial cubature rule as it is much shorter. Also, it highlights that the usage of the cubature rule and not the formula on how to compute it.

	\autoref{fig:cubature} shows the spherical-radial cubature rule working on a transition from polar coordinates to Cartesian coordinates where the transformation is given by:
	\begin{equation*}
		x = r \cos\theta \qquad\qquad y = r \sin\theta
	\end{equation*}
	The polar coordinate particles have been sampled from a multivariate Gaussian with diagonal covariance:
	\begin{equation*}
		r \sim \normal(80,\, 40) \qquad\qquad \theta \sim \normal(0,\, 0.4)
	\end{equation*}

	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.5\linewidth}
			\centering
			\includegraphics[width = \linewidth]{figures/inference/cubature/spherical-radial-cubature-original.png}
			\caption[Original samples in polar coordinates to illustrate cubature rules]{The original samples in polar coordinates with the real mean shown as an orange star in the middle.}
		\end{subfigure}%
		~
		\begin{subfigure}[t]{0.5\linewidth}
			\centering
			\includegraphics[width = \linewidth]{figures/inference/cubature/spherical-radial-cubature-transformed.png}
			\caption[Transformed in Cartesian coordinates to illustrate cubature rules]{The approximate mean calculated via the spherical-radial cubature rule is shown as an orange star, positioned at the mean of the transformed cubature points. This plot also shows the Monte Carlo estimate as a black cross right above the cubature estimate, showing that the cubature estimate is really accurate with significantly less computational effort.}
		\end{subfigure}
		\caption[Illustrative application of the spherical-radial cubature rule to the mapping from polar coordinate to Cartesian coordinates]{These plots show how the spherical-radial cubature rule is used to estimate the mean of the Gaussian on the left after it has been transformed with a polar transformation to the right. To illustrate the shape of the Gaussian after it has been transformed, we added the blue dots as sample values of the Gaussian. These are 1000 sample points also used to calculate the Monte Carlo estimate of the mean. The sigma points are shown as green pluses. \\ Adopted from~\cite{solinCubatureIntegrationMethods2010}.}
		\label{fig:cubature}
	\end{figure}

	\subsection{The Unscented Transform}
		The \emph{unscented\footnote{The name "unscented" was given by Uhlmann as he did not want people to call the corresponding filter the "Uhlmann Filter"~\cite{FirstHandUnscentedTransform}.} transform}, originally introduced by S.~J.~Julier and J.~K.~Uhlmann~\cite{julierNewApproachFiltering1995} is another method for approximating nonlinear Gaussian integrals with a parameterized transformation, the \ac{ut}. As shown by~\cite{solinCubatureIntegrationMethods2010}, the spherical-radial cubature rule is just a special case of the more general \ac{ut}. We will follow Solins derivation of connection here. For \ac{ut}, \( 2n + 1 \) sigma points are formed as a matrix
		\begin{equation}
			\mat{\mathcal{X}} =
				\begin{bmatrix}
					\vec{\mu} & \vec{\mu} \vec{1}^T + \gamma \mat{\Sigma}^{1/2} & \vec{\mu} \vec{1}^T - \gamma \mat{\Sigma}^{1/2}
				\end{bmatrix}  \label{eqn:utSigmaPoints}
		\end{equation}
		where \( \gamma \coloneqq \sqrt{n + \lambda} \) and \( \lambda \coloneqq \alpha^2 (n + \kappa) - n \) are scaling parameters defined using the parameters of the \ac{ut} \( \alpha \) and \( \kappa \). The approximation for the transformed mean and covariance is then given as
		\begin{align}
			\hat{\vec{\mu}} \coloneqq \E_{\vec{x} \sim \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big] &\approx \sum_{i = 1}^{2n + 1} w_{i - 1}^{(m)} \vec{f}(\mat{\mathcal{X}}_i)  \label{eqn:utMean} \\
			\hat{\mat{\Sigma}} \coloneqq \Cov_{\vec{x} \sim \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big] &\approx \sum_{i = 1}^{2n + 1} w_{i - 1}^{(c)} \big( \vec{f}(\mat{\mathcal{X}}_i) - \hat{\vec{\mu}} \big) \big( \mat{\mathcal{X}}_i - \hat{\vec{\mu}} \big)^T  \label{eqn:utCov}
		\end{align}
		where \( \mat{\mathcal{X}}_i \) is the \(i\)-th column of the sigma points matrix \( \mat{\mathcal{X}} \). The weights \( w_i^{(m)} \) for the mean and \( w_i^{(c)} \) for the covariance are defined as
		\begin{align*}
			w_0^{(m)} &\coloneqq \frac{\lambda}{n + \lambda} & w_0^{(c)} &\coloneqq \frac{\lambda}{n + \lambda} + (1 - \alpha^2 + \beta) \\
			w_i^{(m)} &\coloneqq \frac{1}{2(n + \lambda)}    & w_i^{(c)} &\coloneqq \frac{1}{2(n + \lambda)}
		\end{align*}
		where \(\beta\) is another parameter of the \ac{ut}. To get back the spherical radial cubature rule, where all weights \( w_i = \flatfrac{1}(2n) \) are the same for each sigma point and the sigma point \( \mat{\mathcal{X}}_0 \) has a weight of \(0\), \ie the mean is ignored, we have to set \( \lambda = 0 \). Hence, the spherical-radial cubature rule is a special case of the \ac{ut} with the parameters set as \( \alpha = \pm 1 \), \( \beta = \kappa = 0 \). That means we can apply all theory developed for the \ac{ut} to the cubature rule by setting the parameters accordingly.
	% end

	\subsection{Cubature Kalman Filtering and Smoothing}
		\label{subsec:cubatureFiltering}

		Following~\cite{deisenrothProbabilisticPerspectiveGaussian2011,solinCubatureIntegrationMethods2010}, we can use the spherical-radial cubature rule to extend the Kalman filter and \ac{rts} smoother to nonlinear systems with Gaussian noise, \ie
		\begin{align*}
			\vec{s}_{t + 1} &\sim \normal\big( \vec{f}(\vec{s}_t), \mat{Q} \big) \\
			\vec{y}_t &\sim \normal\big( \vec{g}(\vec{s}_t), \mat{R} \big)
		\end{align*}
		In~\autoref{alg:cubatureKalmanFilter} we show the cubature Kalman filter, where the code line/equation correspondence to the linear Kalman filter is shown in~\autoref{tab:cubatureKalmanFilter}. Analogous, the cubature \ac{rts} smoother is shown in~\autoref{alg:cubatureRtsSmoother} and the correspondence table is shown in~\autoref{tab:cubatureRtsSmoother}.

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{Initial state \( \vec{m}_0 \in \R^k \), covariance \( \mat{V}_0 \) and observations \( \vec{y}_{1:T} \in \R^{p \times T} \).}
			\eqmakebox[ckfinit][r]{\( \hat{\vec{s}}_{0 \subgiven 0} \gets \)} \( \vec{m}_0 \) \;
			\eqmakebox[ckfinit][r]{\( \hat{\mat{V}}_{0 \subgiven 0} \gets \)} \( \mat{V}_0 \) \;
			\For{\( t = 1, 2, \rangedots, T \)}{
				\( \vec{\xi}_i \gets \sqrt{n} [\vec{1}]_i \) \;
				\tcp{\textbf{Prediction:}} \vspace{-0.2cm}
				\quad\eqmakebox[ckf][r]{\( \vec{x}_{i, t - 1 \subgiven t - 1} \gets \)} \( \hat{\mat{V}}_{t - 1 \subgiven t - 1}^{1/2} \vec{\xi}_i + \hat{\vec{s}}_{t - 1 \subgiven t - 1} \) \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{s}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \)  \label{algline:ckfStatePre} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\mat{V}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \vec{f}^T(\vec{x}_{i, t - 1 \subgiven t - 1}) - \hat{\vec{s}}_{t \subgiven t - 1} \hat{\vec{s}}_{t \subgiven t - 1}^T + \mat{Q} \)  \label{algline:ckfCovPre} \;
				\tcp{\textbf{Correction:}} \vspace{-0.2cm}
				\quad\eqmakebox[ckf][r]{\( \vec{x}_{i, t \subgiven t - 1} \gets \)} \( \hat{\mat{V}}_{t \subgiven t - 1}^{1/2} + \hat{\vec{s}}_{t \subgiven t - 1} \) \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{y}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{g}(\vec{x}_{i, t \subgiven t - 1}) \)  \label{algline:ckfInno} \;
				\quad\eqmakebox[ckf][r]{\( \mat{S}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{g}(\vec{x}_{i, t \subgiven t - 1}) \vec{g}^T(\vec{x}_{i, t \subgiven t - 1}) - \hat{\vec{y}}_{t \subgiven t - 1} \hat{\vec{y}}_{t \subgiven t - 1}^T + \mat{R} \)  \label{algline:ckfResCov} \;
				\quad\eqmakebox[ckf][r]{\( \mat{J}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \vec{g}^T(\vec{x}_{i, t \subgiven t - 1}) - \hat{\vec{s}}_{t \subgiven t - 1} \hat{\vec{y}}_{t \subgiven t - 1}^T \) \;
				\quad\eqmakebox[ckf][r]{\( \mat{K}_t \gets \)} \( \mat{J}_t \mat{S}_t^{-1} \)  \label{algline:ckfKalmanGain} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{s}}_{t \subgiven t} \gets \)} \( \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t (\vec{y}_t - \hat{\vec{y}}_{t \subgiven t - 1}) \)  \label{algline:ckfStatePost} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\mat{V}}_{t \subgiven t} \gets \)} \( \hat{\mat{V}}_{t \subgiven t - 1} + \mat{K}_t \mat{S}_t \mat{K}_t^T \)  \label{algline:ckfCovPost} \;
			}
			\caption{Spherical-Radial Cubature Kalman Filter}
			\label{alg:cubatureKalmanFilter}
		\end{algorithm}
		\begin{table}
			\centering
			\begin{tabular}{l|c|c}
				\textbf{Name}        &     \textbf{Code Line}      & \textbf{Linear Equation} \\ \hline
				Predicted State      &  \ref{algline:ckfStatePre}  &  \eqref{eqn:kfStatePre}  \\
				Predicted Covariance &   \ref{algline:ckfCovPre}   &   \eqref{eqn:kfCovPre}   \\
				Filtered State       & \ref{algline:ckfStatePost}  & \eqref{eqn:kfStatePost}  \\
				Filtered Covariance  &  \ref{algline:ckfCovPost}   &  \eqref{eqn:kfCovPost}   \\
				Innovation           &    \ref{algline:ckfInno}    &    \eqref{eqn:kfInno}    \\
				Residual Covariance  &   \ref{algline:ckfResCov}   &   \eqref{eqn:kfResCov}   \\
				Kalman Gain          & \ref{algline:ckfKalmanGain} & \eqref{eqn:kfKalmanGain}
			\end{tabular}
			\caption[Correspondence between the cubature and non-cubature Kalman filter]{This table shows how the code lines of the cubature Kalman filer relate to the corresponding linear Kalman filter equations.}
			\label{tab:cubatureKalmanFilter}
		\end{table}

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{All values from the Kalman filter.}
			\For{\( t = T - 1, T - 2, \rangedots, 1 \)}{
				\eqmakebox[crts][r]{\( \vec{\xi}_i \gets \)} \(\sqrt{n} [\vec{1}]_i \) \;
				\eqmakebox[crts][r]{\( \vec{x}_i \gets \)} \( \hat{\mat{V}}_{t \subgiven t}^{1/2} \vec{\xi}_i + \hat{\vec{s}}_{t \subgiven t} \) \;
				\eqmakebox[crts][r]{\( \mat{D}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} (\vec{x}_i - \hat{\vec{s}}_{t \subgiven t}) \big( \vec{f}(\vec{x}_i) - \hat{\vec{s}}_{t + 1 \subgiven t} \big)^T \) \;
				\eqmakebox[crts][r]{\( \mat{J}_t \gets \)} \( \mat{D}_t \hat{\mat{V}}_{t + 1 \subgiven t}^{-1} \) \;
				\eqmakebox[crts][r]{\( \hat{\vec{s}}_{t \subgiven T} \gets \)} \( \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t}) \)  \label{algline:crtsState} \;
				\eqmakebox[crts][r]{\( \hat{\mat{V}}_{t \subgiven T} \gets \)} \( \hat{\mat{V}}_{t \subgiven t} + \mat{J}_t (\hat{\mat{V}}_{t + 1 \subgiven T} - \hat{\mat{V}}_{t + 1 \subgiven t}) \mat{C}_t^T \)  \label{algline:crtsCov} \;
			}
			\caption{Spherical-Radial Cubature Rauch-Tung-Striebel Smoother}
			\label{alg:cubatureRtsSmoother}
		\end{algorithm}
		\begin{table}
			\centering
			\begin{tabular}{l|c|c}
				\textbf{Name}       &   \textbf{Code Line}    & \textbf{Linear Equation} \\ \hline
				Smoothed State      & \ref{algline:crtsState} &   \eqref{eqn:rtsState}   \\
				Smoothed Covariance &  \ref{algline:crtsCov}  &   \eqref{eqn:rtsState}
			\end{tabular}
			\caption[Correspondence between the cubature and non-cubature RTS filter]{This table shows how the code lines of the cubature \ac{rts} smoother relate to the corresponding linear RTS smoother equations.}
			\label{tab:cubatureRtsSmoother}
		\end{table}
	% end

	\subsection{Square-Root Filtering\,/\,Smoothing}
		\label{subsec:sqrtSmoothing}

		While cubature rules are extremely handy for approximating Gaussian integrals, they require the Cholesky decomposition of the covariance \( \mat{\Sigma} \). The Cholesky decomposition might lead to numerical instabilities if the matrix is ill-conditioned or even not positive definite, in which case no Cholesky decomposition exists. As we will see in~\autoref{c:nonlinearGaussianKoopman}, we need the Cholesky decomposition of every smoothed covariance \( \hat{\mat{V}}_{t \subgiven T} \), \( t = 1, 2, \rangedots, T \), leading to numerical instabilities if the cubature approximation in the filtering does not "create" valid covariance matrices, \ie positive definite and symmetric matrices. One approach for solving this problem is estimating the Cholesky decomposition directly, without explicitly decomposing the matrix. Such filtering/smoothing is known as \emph{square-root filtering/smoothing}~\cite{vandermerweSquarerootUnscentedKalman2001,ruttenSquarerootUnscentedFiltering2013}.

		We will mostly follow the derivation in~\cite{ruttenSquarerootUnscentedFiltering2013} and only outline the key results shortly. With the sigma points \( \mat{\mathcal{X}} \) from the \ac{ut} equation~\eqref{eqn:utSigmaPoints} and the corresponding weights, we can define the transformed sigma points
		\begin{equation*}
			\mat{\mathcal{X}}_i' = \vec{f}(\mat{\mathcal{X}}_i)
		\end{equation*}
		and utilize the \ac{ut} mean \( \hat{\vec{\mu}} \)~\eqref{eqn:utMean} and covariance \( \hat{\mat{\Sigma}} \)~\eqref{eqn:utCov} estimates. By applying another transformation to the transformed sigma points,
		\begin{equation*}
			\mat{\mathcal{Y}}_i = \sqrt{w_i^{(c)}} (\mat{\mathcal{X}}' - \hat{\vec{\mu}})
		\end{equation*}
		which are the "scaled residuals" of the sigma points, we can choose an orthogonal matrix \( \mat{\Theta} \) such that
		\begin{equation}
			\mat{\mathcal{Y}} \mat{\Theta} = \begin{bmatrix} \mat{B} & \mat{O}_{k \times (k + 1)} \end{bmatrix}  \label{eqn:sqrtFilteringQR}
		\end{equation}
		Squaring both sides of this equation yields the covariance estimate
		\begin{equation*}
			\hat{\mat{\Sigma}} = \mat{\mathcal{Y}} \mat{\mathcal{Y}}^T = \mat{\mathcal{Y}} \mat{\Theta} \mat{\Theta}^T \mat{\mathcal{Y}} = \begin{bmatrix} \mat{B} & \mat{O}_{k \times (k + 1)} \end{bmatrix} \begin{bmatrix} \mat{B}^T \\ \mat{O}_{(k + 1) \times n} \end{bmatrix} = \mat{B} \mat{B}^T
		\end{equation*}
		so \( \mat{B} \) is the Cholesky decomposition of \( \hat{\mat{\Sigma}} \). But the equation~\eqref{eqn:sqrtFilteringQR}, where we neither now the orthogonal matrix \( \mat{\Theta} \) nor the right side of the equation, is exactly the type of matrix decomposition known as the \emph{QR decomposition} that is numerically more stable than the Cholesky decomposition as it does not require any special structure like positive definiteness of the matrix. By applying this argumentation in an analogous way to every covariance computation in the cubature Kalman filter and \ac{rts} smoother, we find a complete square-root filtering and smoothing algorithm propagating the Cholesky decomposition of the covariance matrices \( \hat{\mat{V}}_{t \subgiven t - 1} \), \( \hat{\mat{V}}_{t \subgiven t} \) and \( \hat{\mat{V}}_{t \subgiven T} \).

		The square-root cubature Kalman filter is summarized in~\autoref{alg:sqrtCubatureKalmanFilter} and the square-root cubature \ac{rts} smoother is summarized in~\autoref{alg:sqrtCubatureRtsSmoother}.

		The square-root filtering now allows us to pass the Cholesky decomposition of the covariances directly through the smoothing and filtering process, allowing higher numerical stability because we guarantee the covariances to be positive definite even with approximations.

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{Initial state \( \vec{m}_0 \in \R^k \), covariance sqrt \( \mat{V}_0^{1/2} \) and observations \( \vec{y}_{1:T} \in \R^{p \times T} \).}
			\eqmakebox[sqrtckfinit][r]{\( \hat{\vec{s}}_{0 \subgiven 0} \gets \)} \( \vec{m}_0 \) \;
			\eqmakebox[sqrtckfinit][r]{\( \hat{\mat{V}}_{0 \subgiven 0}^{1/2} \gets \)} \( \mat{V}_0^{1/2} \) \;
			\For{\( t = 1, 2, \rangedots, T \)}{
				\eqmakebox[sqrtckf][r]{\( \mat{\Lambda} \gets \)} \( \sqrt{n} \cdot \hat{\mat{V}}_{t - 1 \subgiven t - 1}^{1/2} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t - 1 \subgiven t - 1} \gets \)} \( \begin{bmatrix} \hat{\vec{s}}_{t - 1 \subgiven t - 1} \vec{1}^T + \mat{\Lambda} & \hat{\vec{s}}_{t - 1 \subgiven t - 1} \vec{1}^T - \mat{\Lambda} \end{bmatrix} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t \subgiven t - 1, i}^s \gets \)} \( \vec{f}(\mat{\mathcal{X}}_{t - 1 \subgiven t - 1, i}) + (-1)^i \cdot \mat{Q} \),\quad \( i = 1, 2, \rangedots, 2k \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t \subgiven t - 1, i}^y \gets \)} \( \vec{g}\big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^s\big) + (-1)^i \cdot \mat{R} \),\quad \( i = 1, 2, \rangedots, 2k \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{s}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2k} \sum_{i = 1}^{2k} \mat{\mathcal{X}}_{t \subgiven t - 1, i}^s \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{y}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2k} \sum_{i = 1}^{2k} \mat{\mathcal{X}}_{t \subgiven t - 1, i}^y \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{Y}}_{t \subgiven t - 1, i}^s \gets \)} \( \big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^s - \hat{\vec{s}}_{t \subgiven t - 1}\big)/\sqrt{2k} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{Y}}_{t \subgiven t - 1, i}^y \gets \)} \( \big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^y - \hat{\vec{y}}_{t \subgiven t - 1}\big)/\sqrt{2k} \) \;
				\tcp{Find \( \mat{S}_t^{1/2} \), \( \mat{J}_t \) and \( \hat{\mat{V}}_{t \subgiven t}^{1/2} \) using the QR decomposition:}
				\(
					\begin{bmatrix}
						\mat{\mathcal{Y}}_{t \subgiven t - 1}^y \\
						\mat{\mathcal{Y}}_{t \subgiven t - 1}^s
					\end{bmatrix}
					\mat{\Theta}
					=
					\begin{bmatrix}
						\mat{S}_t^{1/2} & \mat{O} & \mat{O} \\
						\mat{J}_t       & \hat{\mat{V}}_{t \subgiven t}^{1/2} & \mat{O}
					\end{bmatrix}
				\) \;
				\eqmakebox[sqrtckf][r]{\( \mat{K}_t \gets \)} \( \mat{J}_t \mat{S}_t^{-1/2} \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{s}}_{t \subgiven t} \gets \)} \( \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t (\vec{y}_t - \hat{\vec{y}}_{t \subgiven t - 1}) \) \;
			}
			\caption{Square-Root Cubature Kalman Filter}
			\label{alg:sqrtCubatureKalmanFilter}
		\end{algorithm}

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{All values from the Kalman filter.}
			\For{\( t = T - 1, T - 2, \rangedots, 1 \)}{
				\eqmakebox[sqrtcrts][r]{\( \mat{\mathcal{Y}}_{t \subgiven t, i}^s \gets \)} \( (\mat{\mathcal{X}}_{t \subgiven t, i} - \hat{\vec{s}}_{t \subgiven t}) \) \;
				\tcp{Find \( \mat{D}_t \) and \( \mat{Z}_t \) using the QR decomposition:}
				\(
					\begin{bmatrix}
						\mat{\mathcal{Y}}_{t + 1 \subgiven t}^s \\
						\mat{\mathcal{Y}}_{t \subgiven t}^s
					\end{bmatrix}
					\mat{\Theta}
					=
					\begin{bmatrix}
						\hat{\mat{V}}_{t + 1 \subgiven t}^{1/2} & \mat{O}   & \mat{O} \\
						\mat{D}_t                               & \mat{Z}_t & \mat{O}
					\end{bmatrix}
				\) \;
				\eqmakebox[sqrtcrts][r]{\( \mat{J}_t \gets \)} \( \mat{D}_t \hat{\mat{V}}_{t + 1 \subgiven t}^{-1/2} \) \;
				\eqmakebox[sqrtcrts][r]{\( \hat{\vec{s}}_{t \subgiven T} \gets \)} \( \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t}) \) \;
				\tcp{Find \( \hat{\mat{V}}_{t \subgiven T}^{1/2} \) using the QR decomposition:}
				\(
					\begin{bmatrix}
						\mat{Z}_t & \mat{J}_t \hat{\mat{V}}_{t + 1 \subgiven T}^{1/2}
					\end{bmatrix}
					\mat{\Theta}
					=
					\begin{bmatrix}
						\hat{\mat{V}}_{t \subgiven T}^{1/2} & \mat{O}
					\end{bmatrix}
				\) \;
			}
			\caption{Square-Root Cubature Rauch-Tung-Striebel Smoother}
			\label{alg:sqrtCubatureRtsSmoother}
		\end{algorithm}
	% end
% end
