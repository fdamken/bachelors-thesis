\chapter{Inference in Dynamical Systems}
\label{c:inferenceInDynamicalSystems}



Our second contribution is to take a probabilistic perspective on "classical" Koopman theory as presented in various publications~\cite{bruntonKoopmanInvariantSubspaces2016,hanDeepLearningKoopman2020,kaiserDatadrivenDiscoveryKoopman2020,luschDeepLearningUniversal2018,williamsDataDrivenApproximation2015}. In this chapter we will build the groundwork for variational inference to combine them to the single \algname algorithm presented in~\autoref{c:nonlinearGaussianKoopman}.

\section{Hidden Markov Models and LGDS}
	\acp{hmm} are simple Bayesian networks described by a non-observable (\emph{hidden}) Markov chain. This non-observable discrete chain can be indirectly observed with measurements that are emitted by every state. One of the key features of a Markov chain is that a state does only depend on the previous state, but not on the second previous, third previous, and so on, state. That is, knowing only the previous state is sufficient and no more information can be gathered by knowing every other state before. This is described by the state transition distribution
	\begin{equation*}
		s_{k + 1} \sim p(s_{k + 1} \given s_k)
	\end{equation*}
	being only dependent on the previous state \(s_k\). Analogous, a measurement \(\vec{y}_k\) of a state \(s_k\) is only dependent on that specific state:
	\begin{equation*}
		\vec{y}_k \sim p(\vec{y}_k \given s_k)
	\end{equation*}
	These assumptions are called the \emph{Markov property} and a system fulfilling this property is called \emph{Markovian}. These conditional distributions can be written in a graphical model as shown in~\autoref{fig:hiddenMarkovModel}. Note that, in general, no assumption has to be made on the "type" of state/observation (\ie whether it is a scalar, a vector or something completely different). Also, no assumption is made on the specific transition distributions, \acp{hmm} can also be used to model deterministic transitions using a Dirac delta distribution.

	\begin{figure}
		\centering
		\tikzHiddenMarkovModel
		\caption{Illustration of a completely general Hidden Markov Model with states \(s_k\) and emis\-sions/observations \(\vec{y}_k\).}
		\label{fig:hiddenMarkovModel}
	\end{figure}

	A set of closely related dynamics models are \acp{lgds} which are described by a noisy state transition
	\begin{equation*}
		\vec{s}_{t + 1} = \mat{A} \vec{s}_t + \vec{w}_t,\quad \vec{w}_t \sim \normal(\vec{0}, \mat{Q})
	\end{equation*}
	with covariance matrix \(\mat{Q}\). Similarly, the measurements \( \vec{y}_t = \mat{C} \vec{s}_t + \vec{v}_t \) underlie a Gaussian noise \( \vec{v}_t \sim \normal(\vec{0}, \mat{R}) \) too. Together they form the probabilistic model
	\begin{align*}
		\vec{s}_{t + 1} &= p(\mat{A} \vec{s}_t, \vec{Q}) \\
		\vec{y}_t &= p(\mat{C} \vec{s}_t, \mat{R})
	\end{align*}
	which fulfills the Markov property, hence the system is Markovian:
	\begin{equation*}
		p(\vec{s}_{1:T}, \vec{y}_{1:T}) = p(\vec{s}_1) \prod_{t = 2}^{T} p(\vec{s}_t \given \vec{s}_{t - 1}) \prod_{t = 1}^{T} p(\vec{y}_t \given \vec{s}_t)
	\end{equation*}
	Note that we write \( \vec{s}_{t_0:t_1} \) for the sequence \( \big( \vec{s}_{t_0}, \vec{s}_{t_0 + 1}, \rangedots, \vec{s}_{t_1} \big) \) and analogous for the observation sequence \( \vec{y}_{t_0:t_1} \). The probabilistic model of a \ac{lgds} is shown in~\autoref{fig:lgds}.

	This system is similar to a \ac{hmm} and in fact a \ac{lgds} really is an \ac{hmm}, just with continuous states \( \vec{s}_t \in \R^k \) and measurements \( \vec{y}_t \in \R^p \).

	\begin{figure}
		\centering
		\tikzLinearGaussianDynamicalSystem
		\caption{Illustration of a Linear Gaussian Dynamical System with states \(\vec{x}_t\) and observations \(\vec{y}_t\). Solid arrows represent probabilistic dependency, where the matrix \(\mat{A}\) is the dynamics matrix indicating that the mean transitions linearly, so do the observations with the observation matrix \(\mat{C}\).}
		\label{fig:lgds}
	\end{figure}
% end

\section{Inference: Filtering and Smoothing}
	\label{sec:filteringSmoothing}

	When looking at an \ac{lgds}, one of the first problems that one might consider is how to get the states (also called \emph{latent states} as we cannot observe) back? This procedure is called \emph{inference}. In this section we will give an overview on how to perform inference on an \ac{lgds}.

	For time-series data, the inference procedure is widely know as \emph{filtering} and \emph{smoothing}, where the following distributions on the latent states \( \vec{s}_t \) are calculated:
	\begin{itemize}
		\item Filtering: \tabto{2.5cm} \( p(\vec{s}_t \given \vec{y}_{1:t}) \)
		\item Smoothing: \tabto{2.5cm} \( p(\vec{s}_t \given \vec{y}_{1:T}) \)
	\end{itemize}
	That is, the filtered distribution only depends on the measurements \emph{to that time} where in contrast the smoothed distribution has used all data to the last time step \(T\) to estimate the latent states. An intermediate step in the filtering algorithm is called \emph{prediction} where the distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \) is being calculated, using only the data to the previous time step. The differences between what data is used in the three steps is shown in~\autoref{fig:predictionFilteringSmoothing}.

	We will now take a look on the most used filter, the well-known \emph{Kalman Filter} and its colleague, the \emph{Rauch-Tung-Striebel Smoother}, which is also sometimes called the \emph{Kalman Smoother} as it requires a run of the Kalman filter first.

	\begin{figure}
		\centering
		\tikzPredictionFilteringSmoothing
		\caption{This diagram shows the main differences between prediction, filtering and smoothing. The shaded region describes the measurements that has been used to estimate the state at time step \(t\), indicated by a solid line. The dotted line represents time step \(t - 1\), to which all measurements are used in all three procedures. \\ Adopted from~\cite{solinCubatureIntegrationMethods2010}.}
		\label{fig:predictionFilteringSmoothing}
	\end{figure}

	\subsection{Kalman Filter}
		The Kalman filter as originally introduced by R.~Kalman in 1960~\cite{kalmanNewApproachLinear1960} in the context of signal processing to predict random signals, separate the signals from noise and detect signals of known form in the presence of noise~\cite{kalmanNewApproachLinear1960}. The normal Kalman filter assumes an underlying model of the form of an \ac{lgds} where the measurements are detected and the state is to be estimated from the data. The process of estimated the states is done in a two-step fashion:
		\begin{enumerate}
			\item Prediction: \tabto{2.5cm} Estimate the distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \)
			\item Correction: \tabto{2.5cm} Estimate the distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \)
		\end{enumerate}
		For a simple linear system with Gaussian noise, \ie an \ac{lgds}, the equations for both the prediction and the correction can be given in closed form and are fairly straightforward. For the prediction step, the equations are given as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven t - 1} &= \mat{A} \hat{\vec{s}}_{t - 1 \subgiven t - 1}  \label{eqn:kfStatePre} \\
			\hat{\mat{V}}_{t \subgiven t - 1} &= \mat{A} \hat{\mat{V}}_{t - 1 \subgiven t - 1} \mat{A}^T + \mat{Q}  \label{eqn:kfCovPre}
		\end{align}
		where \( \hat{\vec{s}}_{t \given t - 1} \) and \( \hat{\mat{V}}_{t \given t - 1} \) are the mean and covariance of the prediction distribution \( p(\vec{s}_t \given \vec{y}_{1:t - 1}) \), respectively. The correction step, which integrated new knowledge gained from the observation \( \vec{y}_t \) into the estimated, is given as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven t} &= \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t \tilde{\vec{y}}_t  \label{eqn:kfStatePost} \\
			\hat{\mat{V}}_{t \subgiven t} &= \hat{\mat{V}}_{t \subgiven t - 1} - \mat{K}_t \mat{S}_t \mat{K}_t^T  \label{eqn:kfCovPost}
		\end{align}
		with the auxiliary variables
		\begin{align}
			\tilde{\vec{y}}_t &= \vec{y}_t - \mat{C} \hat{\vec{s}}_{t \subgiven t - 1}  \label{eqn:kfInno} \\
			\mat{S}_t &= \mat{C} \hat{\mat{V}}_{t \subgiven t - 1} \mat{C}^T + \mat{R}  \label{eqn:kfResCov} \\
			\mat{K}_t &= \hat{\mat{V}}_{t \subgiven t - 1} \mat{C}^T \mat{S}_t^{-1}  \label{eqn:kfKalmanGain}
		\end{align}
		known as the innovation, residual covariance and Kalman gain. The resulting estimates \( \hat{\vec{s}}_{t \subgiven t} \) and \( \hat{\mat{V}}_{t \subgiven t} \) then form the mean and covariance of the filtered distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \), respectively. These equations form a recursive algorithm "forward in time" starting off with initial state estimates \( \hat{\vec{s}}_{0 \subgiven 0} = \vec{m}_0 \) and \( \hat{\mat{V}}_{0 \subgiven 0} = \mat{V}_0 \) that have to be determined in another way.

		But these equations only work for linear systems. Lots of extensions of the regular Kalman filter have been proposed to extend the filter to nonlinear systems. The most used one is the \ac{ekf} which assumes locally linear dynamics and uses first-order Taylor expansions to linearize them. Throughout this thesis we will use the cubature Kalman filter, which we will introduce later in~\autoref{subsec:cubatureFiltering}.
	% end

	\subsection{Rauch-Tung-Striebel\,/\,Kalman Smoother}
		The \ac{rts} smoother has been introduced five years after the original Kalman filter in the 1965 by H.~E.~Rauch, F.~Tung and C.~T.~Striebel~\cite{rauchMaximumLikelihoodEstimates1965} exhibiting the equations for optimal smoothing of linear systems with Gaussian noise, \ie \ac{lgds}. With the smoother, we want to estimate the smoothed distribution, also called the \emph{posterior}
		\begin{equation*}
			p(\vec{s}_t \given \vec{y}_{1:T})
		\end{equation*}
		which uses all measurements (see again~\autoref{fig:predictionFilteringSmoothing} for an illustration). With letting the Kalman filter "run" beforehand, we have the filtered distribution \( p(\vec{s}_t \given \vec{y}_{1:t}) \) at hand. Then we have the smoothing equations as
		\begin{align}
			\hat{\vec{s}}_{t \subgiven T} &= \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t})  \label{eqn:rtsState} \\
			\hat{\mat{V}}_{t \subgiven T} &= \hat{\mat{V}}_{t \subgiven t} + \mat{J}_t (\hat{\mat{V}}_{t + 1 \subgiven T} - \hat{\mat{V}}_{t + 1 \subgiven t}) \mat{J}_t^T  \label{eqn:rtsCov}
		\end{align}
		with the auxiliary variable \( \mat{J}_t = \hat{\mat{V}}_{t \subgiven t} \mat{A}^T \hat{\mat{V}}_{t + 1 \subgiven t}^{-1} \). The resulting estimates \( \hat{\vec{s}}_{t \subgiven T} \) and \( \hat{\mat{V}}_{t \subgiven T} \) then form the mean and covariance of the smoothed/posterior distribution \( p(\vec{s}_t \given \vec{y}_{1:T}) \). These equations form a recursive algorithm "backward in time" starting off with the final state and covariance estimates of the filter.

		Hence, we need the Kalman filter before using the \ac{rts} smoother. Due to this hard wiring between the two algorithms, the \ac{rts} smoother is also often referred to as the Kalman smoother.

		Note that these equations only depend on the state dynamics matrix \( \mat{A} \), so a system that is linear in the state and nonlinear in the measurements can still use the standard \ac{rts} smoother and no further modification is required.
	% end
% end

\section{Cubature Rules and Filtering}
	\label{sec:cubatureRules}

	As we have already discussed, the plain Kalman filter not applicable to nonlinear systems without modifications. In this section we want to motivate the need for \emph{cubature rules} and outline their derivation and how to apply them.

	Often when deriving probabilistic algorithms like \ac{em} or the Kalman filter, we face us to evaluate expectations \( \E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] \) and hence integrals of the form
	\begin{equation*}
		\E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] = \int\! \vec{f}(\vec{x}) p(\vec{x}) \dd{\vec{x}}
	\end{equation*}
	In this thesis and lots of other literature, we experience Gaussian distributions \( p(\vec{x}) = \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \) with mean \(\vec{\mu}\) and covariance \(\mat{\Sigma}\), leading to Gaussian integrals of the form
	\begin{equation*}
		\E_{\vec{x} \sim p(\vec{x})}\big[ \vec{f}(\vec{x}) \big] = \int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}}
	\end{equation*}
	which do not have closed form solutions for arbitrary, nonlinear, functions \( \vec{f}(\vec{x}) \). Two common approaches for approximating the expectations/integrals are Monte Carlo Estimation, leading to particle filters and numerical integration methods, known as cubature\footnote{"Classical" numerical integration methods are called "quadrature" as they form a rectangle under the curve to approximate the area, while in higher dimensions this is more or less a "cube", hence it is called "cubature".} rules.

	To approximate Gaussian integrals, we need a set of \(m\) \emph{sigma points} \( \vec{x}_i \) to evaluate \( \vec{f}(\vec{x}) \) at and a set weights \( w_i \) to form a sum over the sigma points:
	\begin{equation*}
		\int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \approx \sum_{i = 1}^{m} w_i \vec{f}(\vec{x}_i)
	\end{equation*}
	In this thesis we utilize the spherical-radial cubature rule~\cite{solinCubatureIntegrationMethods2010} which is a third-degree approximation for Gaussian integrals. A third-degree approximation means that "this rule is exact for all monomials up to degree three"~\cite[p. 18]{solinCubatureIntegrationMethods2010}. In this cubature rule, the approximation a finite sum over \( 2n \) points, where \( n \) is the dimensionality of \(\vec{x}\), \ie \( \vec{x} \in \R^n \). With \( \mat{\Sigma}^{1/2} \) being the Cholesky decomposition of \( \mat{\Sigma} \), so \( \mat{\Sigma} = \mat{\Sigma}^{1/2} \mat{\Sigma}^{1/2, T} \), the approximation is given as
	\begin{equation*}
		\int\! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \approx \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\mat{\Sigma}^{1/2} \vec{\xi}_i - \vec{\mu})
	\end{equation*}
	with the cubature points \( \vec{\xi}_i = \sqrt{n} [\vec{1}]_i \). Here the points \( [\vec{1}]_i \) are "from the intersections between the Cartesian axes and the \(n\)-dimensional unit hypersphere"~\cite{solinCubatureIntegrationMethods2010}. That is, the point \( [\vec{1}]_i \) is the \(i\)-th row vector of the block matrix \( \begin{bmatrix} \mat{I} & -\mat{I} \end{bmatrix} \), where \( \mat{I} \) is the \(n\)-dimensional identity matrix. In the rest of this thesis we will write
	\begin{equation*}
		\SRC[\vec{f}; \vec{\mu}, \mat{\Sigma}] \coloneqq \sum_{i = 1}^{m} w_i \vec{f}(\vec{x}_i)
	\end{equation*}
	for evaluations of the spherical-radial cubature rule as it is much shorter and emphasizes the usage of the rule instead of the rule itself.

	\autoref{fig:cubature} shows the spherical-radial cubature rule working on a transition from polar coordinates to Cartesian coordinates where the transformation is given by:
	\begin{equation*}
		x = r \cos\theta \qquad\qquad y = r \sin\theta
	\end{equation*}
	The polar coordinate particles have been sampled from a multivariate Gaussian with diagonal covariance:
	\begin{equation*}
		r \sim \normal(80,\, 40) \qquad\qquad \theta \sim \normal(0,\, 0.4)
	\end{equation*}

	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.5\linewidth}
			\centering
			\includegraphics[width = \linewidth]{figures/inference/cubature/spherical-radial-cubature-original}
			\caption{The original samples in polar coordinates with the real mean shown as an orange star in the middle.}
		\end{subfigure}%
		~
		\begin{subfigure}[t]{0.5\linewidth}
			\centering
			\includegraphics[width = \linewidth]{figures/inference/cubature/spherical-radial-cubature-transformed}
			\caption{The approximate mean calculated via the spherical-radial cubature rule is shown as an orange star, positioned at the mean of the transformed cubature points. This plot also shows the Monte Carlo estimate as a black cross right above the cubature estimate, showing that the cubature estimate is really accurate with significantly less computational effort.}
		\end{subfigure}
		\caption{These plots show how the spherical-radial cubature rule is used to estimate the mean of the Gaussian on the left after it has been transformed with a polar transformation to the right. To illustrate the shape of the Gaussian after it has been transformed, we added the blue dots as sample values of the Gaussian. These are 1000 sample points also used to calculate the Monte Carlo estimate of the mean. The sigma points are shown as green pluses. \\ Adopted from~\cite{solinCubatureIntegrationMethods2010}.}
		\label{fig:cubature}
	\end{figure}

	\subsection{The Unscented Transform}
		The \emph{unscented\footnote{The name "unscented", meaning "without perfume", was given by Uhlmann as he did not want people to call the corresponding filter the "Uhlmann Filter"~\cite{FirstHandUnscentedTransform}.} transform}, originally introduced by S.~J.~Julier and J.~K.~Uhlmann in 1995~\cite{julierNewApproachFiltering1995} is another method for approximating nonlinear Gaussian integrals with a parameterized transformation, the \ac{ut}. As shown by~\cite{solinCubatureIntegrationMethods2010}, the spherical-radial cubature rule is just a special case of the more general \ac{ut}. We will follow Solins derivation of this fact here. For \ac{ut}, \( 2n + 1 \) sigma points are formed as a matrix
		\begin{equation}
			\mat{\mathcal{X}} =
				\begin{bmatrix}
					\vec{\mu} & \vec{\mu} \vec{1}^T + \gamma \mat{\Sigma}^{1/2} & \vec{\mu} \vec{1}^T - \gamma \mat{\Sigma}^{1/2}
				\end{bmatrix}  \label{eqn:utSigmaPoints}
		\end{equation}
		where \( \vec{1} \) is a column vector with ones everywhere and
		\begin{equation*}
			\gamma \coloneqq \sqrt{n + \lambda} \qquad\qquad \lambda \coloneqq \alpha^2 (n + \kappa) - n
		\end{equation*}
		are a scaling parameters defined using the parameters of the \ac{ut} \( \alpha \) and \( \kappa \). The approximation for the transformed mean and covariance is then given as
		\begin{align}
			\hat{\vec{\mu}} \coloneqq \E_{\vec{x} \sim \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big] &\approx \sum_{i = 1}^{2n + 1} w_{i - 1}^{(m)} \vec{f}(\mat{\mathcal{X}}_i)  \label{eqn:utMean} \\
			\hat{\mat{\Sigma}} \coloneqq \Cov_{\vec{x} \sim \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big] &\approx \sum_{i = 1}^{2n + 1} w_{i - 1}^{(c)} \big( \vec{f}(\mat{\mathcal{X}}_i) - \hat{\vec{\mu}} \big) \big( \mat{\mathcal{X}}_i - \hat{\vec{\mu}} \big)^T  \label{eqn:utCov}
		\end{align}
		where \( \mat{\mathcal{X}}_i \) is the \(i\)-th column of the sigma points matrix \( \mat{\mathcal{X}} \). The weights \( w_i^{(m)} \) for the mean and \( w_i^{(c)} \) for the covariance are defined as
		\begin{align*}
			w_0^{(m)} &\coloneqq \frac{\lambda}{n + \lambda} & w_0^{(c)} &\coloneqq \frac{\lambda}{n + \lambda} + (1 - \alpha^2 + \beta) \\
			w_i^{(m)} &\coloneqq \frac{1}{2(n + \lambda)}    & w_i^{(c)} &\coloneqq \frac{1}{2(n + \lambda)}
		\end{align*}
		where \(\beta\) is another parameter of the \ac{ut}. To get back the spherical radial cubature rule, where all weights \( w_i = \flatfrac{1}(2n) \) are the same for each sigma point and the sigma point \( \mat{\mathcal{X}}_0 \) has a weight of \(0\), \ie the mean is ignored, we have to set \( \lambda = 0 \). Hence, the spherical-radial cubature rule is a special case of the \ac{ut} with the parameters set as \( \alpha = \pm 1 \), \( \beta = \kappa = 0 \). That means we can apply all theory developed for the \ac{ut} to the cubature rule by setting the parameters accordingly.
	% end

	\subsection{Cubature Kalman Filtering and Smoothing}
		\label{subsec:cubatureFiltering}

		Following~\cite{deisenrothProbabilisticPerspectiveGaussian2011,solinCubatureIntegrationMethods2010}, we can use the spherical-radial cubature rule to extend the Kalman filter and \ac{rts} smoother to nonlinear systems with Gaussian noise, \ie
		\begin{align*}
			\vec{s}_{t + 1} &\sim \normal\big( \vec{f}(\vec{s}_t), \mat{Q} \big) \\
			\vec{y}_t &\sim \normal\big( \vec{g}(\vec{s}_t), \mat{R} \big)
		\end{align*}
		In~\autoref{alg:cubatureKalmanFilter} we show the cubature Kalman filter, where the code line/equation correspondence to the linear Kalman filter is shown in~\autoref{tab:cubatureKalmanFilter}. Analogous, the cubature \ac{rts} smoother is shown in~\autoref{alg:cubatureRtsSmoother} and the correspondence table is shown in~\autoref{tab:cubatureRtsSmoother}.

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{Initial state \( \vec{m}_0 \in \R^k \), covariance \( \mat{V}_0 \) and observations \( \vec{y}_{1:T} \in \R^{p \times T} \).}
			\eqmakebox[ckfinit][r]{\( \hat{\vec{s}}_{0 \subgiven 0} \gets \)} \( \vec{m}_0 \) \;
			\eqmakebox[ckfinit][r]{\( \hat{\mat{V}}_{0 \subgiven 0} \gets \)} \( \mat{V}_0 \) \;
			\For{\( t = 1, 2, \rangedots, T \)}{
				\( \vec{\xi}_i \gets \sqrt{n} [\vec{1}]_i \) \;
				\textbf{Prediction:} \;
				\quad\eqmakebox[ckf][r]{\( \vec{x}_{i, t - 1 \subgiven t - 1} \gets \)} \( \hat{\mat{V}}_{t - 1 \subgiven t - 1}^{1/2} \vec{\xi}_i + \hat{\vec{s}}_{t - 1 \subgiven t - 1} \) \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{s}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \)  \label{algline:ckfStatePre} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\mat{V}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \vec{f}^T(\vec{x}_{i, t - 1 \subgiven t - 1}) - \hat{\vec{s}}_{t \subgiven t - 1} \hat{\vec{s}}_{t \subgiven t - 1}^T + \mat{Q} \)  \label{algline:ckfCovPre} \;
				\textbf{Correction:} \;
				\quad\eqmakebox[ckf][r]{\( \vec{x}_{i, t \subgiven t - 1} \gets \)} \( \hat{\mat{V}}_{t \subgiven t - 1}^{1/2} + \hat{\vec{s}}_{t \subgiven t - 1} \) \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{y}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{g}(\vec{x}_{i, t \subgiven t - 1}) \)  \label{algline:ckfInno} \;
				\quad\eqmakebox[ckf][r]{\( \mat{S}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{g}(\vec{x}_{i, t \subgiven t - 1}) \vec{g}^T(\vec{x}_{i, t \subgiven t - 1}) - \hat{\vec{y}}_{t \subgiven t - 1} \hat{\vec{y}}_{t \subgiven t - 1}^T + \mat{R} \)  \label{algline:ckfResCov} \;
				\quad\eqmakebox[ckf][r]{\( \mat{J}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}(\vec{x}_{i, t - 1 \subgiven t - 1}) \vec{g}^T(\vec{x}_{i, t \subgiven t - 1}) - \hat{\vec{s}}_{t \subgiven t - 1} \hat{\vec{y}}_{t \subgiven t - 1}^T \) \;
				\quad\eqmakebox[ckf][r]{\( \mat{K}_t \gets \)} \( \mat{J}_t \mat{S}_t^{-1} \)  \label{algline:ckfKalmanGain} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\vec{s}}_{t \subgiven t} \gets \)} \( \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t (\vec{y}_t - \hat{\vec{y}}_{t \subgiven t - 1}) \)  \label{algline:ckfStatePost} \;
				\quad\eqmakebox[ckf][r]{\( \hat{\mat{V}}_{t \subgiven t} \gets \)} \( \hat{\mat{V}}_{t \subgiven t - 1} + \mat{K}_t \mat{S}_t \mat{K}_t^T \)  \label{algline:ckfCovPost} \;
			}
			\caption{Spherical-Radial Cubature Kalman Filter}
			\label{alg:cubatureKalmanFilter}
		\end{algorithm}
		\begin{table}
			\centering
			\begin{tabular}{l|c|c}
				\textbf{Name}        &     \textbf{Code Line}      & \textbf{Linear Equation} \\ \hline
				Predicted State      &  \ref{algline:ckfStatePre}  &  \eqref{eqn:kfStatePre}  \\
				Predicted Covariance &   \ref{algline:ckfCovPre}   &   \eqref{eqn:kfCovPre}   \\
				Filtered State       & \ref{algline:ckfStatePost}  & \eqref{eqn:kfStatePost}  \\
				Filtered Covariance  &  \ref{algline:ckfCovPost}   &  \eqref{eqn:kfCovPost}   \\
				Innovation           &    \ref{algline:ckfInno}    &    \eqref{eqn:kfInno}    \\
				Residual Covariance  &   \ref{algline:ckfResCov}   &   \eqref{eqn:kfResCov}   \\
				Kalman Gain          & \ref{algline:ckfKalmanGain} & \eqref{eqn:kfKalmanGain}
			\end{tabular}
			\caption{This table shows how the code lines of the cubature Kalman filer relate to the corresponding linear Kalman filter equations.}
			\label{tab:cubatureKalmanFilter}
		\end{table}

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{All values from the Kalman filter.}
			\For{\( t = T - 1, T - 2, \rangedots, 1 \)}{
				\eqmakebox[crts][r]{\( \vec{\xi}_i \gets \)} \(\sqrt{n} [\vec{1}]_i \) \;
				\eqmakebox[crts][r]{\( \vec{x}_i \gets \)} \( \hat{\mat{V}}_{t \subgiven t}^{1/2} \vec{\xi}_i + \hat{\vec{s}}_{t \subgiven t} \) \;
				\eqmakebox[crts][r]{\( \mat{D}_t \gets \)} \( \frac{1}{2n} \sum_{i = 1}^{2n} (\vec{x}_i - \hat{\vec{s}}_{t \subgiven t}) \big( \vec{f}(\vec{x}_i) - \hat{\vec{s}}_{t + 1 \subgiven t} \big)^T \) \;
				\eqmakebox[crts][r]{\( \mat{J}_t \gets \)} \( \mat{D}_t \hat{\mat{V}}_{t + 1 \subgiven t}^{-1} \) \;
				\eqmakebox[crts][r]{\( \hat{\vec{s}}_{t \subgiven T} \gets \)} \( \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t}) \)  \label{algline:crtsState} \;
				\eqmakebox[crts][r]{\( \hat{\mat{V}}_{t \subgiven T} \gets \)} \( \hat{\mat{V}}_{t \subgiven t} + \mat{J}_t (\hat{\mat{V}}_{t + 1 \subgiven T} - \hat{\mat{V}}_{t + 1 \subgiven t}) \mat{C}_t^T \)  \label{algline:crtsCov} \;
			}
			\caption{Spherical-Radial Cubature Rauch-Tung-Striebel Smoother}
			\label{alg:cubatureRtsSmoother}
		\end{algorithm}
		\begin{table}
			\centering
			\begin{tabular}{l|c|c}
				\textbf{Name}       &   \textbf{Code Line}    & \textbf{Linear Equation} \\ \hline
				Smoothed State      & \ref{algline:crtsState} &   \eqref{eqn:rtsState}   \\
				Smoothed Covariance &  \ref{algline:crtsCov}  &   \eqref{eqn:rtsState}
			\end{tabular}
			\caption{This table shows how the code lines of the cubature \ac{rts} smoother relate to the corresponding linear \ac{rts} smoother equations.}
			\label{tab:cubatureRtsSmoother}
		\end{table}
	% end

	\subsection{Square-Root Filtering\,/\,Smoothing}
		\label{subsec:sqrtSmoothing}

		While cubature rules are extremely handy, they require the Cholesky decomposition of the covariance \( \mat{\Sigma} \). The Cholesky decomposition might lead to numerical instabilities if the matrix is ill-conditioned or even not positive definite, in which case no Cholesky decomposition exists. As we will see in~\autoref{c:nonlinearGaussianKoopman}, we need the Cholesky decomposition of every smoothed covariance \( \hat{\mat{V}}_{t \subgiven T} \), \( t = 1, 2, \rangedots, T \), leading to numerical instabilities if the cubature approximation in the filtering does not "create" valid covariance matrices, \ie positive definite and symmetric matrices. One approach for solving this problem is estimating the Cholesky decomposition directly, without explicitly decomposing the matrix. Such filtering/smoothing is known as \emph{square-root filtering/smoothing}~\cite{vandermerweSquarerootUnscentedKalman2001,ruttenSquarerootUnscentedFiltering2013} and we will look into it now.

		We will mostly follow the derivation in~\cite{ruttenSquarerootUnscentedFiltering2013} and only shortly outline the key results. With the sigma points \( \mat{\mathcal{X}} \) from the \ac{ut} equation~\eqref{eqn:utSigmaPoints} and the corresponding weights, we can define the transformed sigma points
		\begin{equation*}
			\mat{\mathcal{X}}_i' = \vec{f}(\mat{\mathcal{X}}_i)
		\end{equation*}
		and utilize the \ac{ut} mean \( \hat{\vec{\mu}} \)~\eqref{eqn:utMean} and covariance \( \hat{\mat{\Sigma}} \)~\eqref{eqn:utCov} estimates. By applying another transformation to the transformed sigma points,
		\begin{equation*}
			\mat{\mathcal{Y}}_i = \sqrt{w_i^{(c)}} (\mat{\mathcal{X}}' - \hat{\vec{\mu}})
		\end{equation*}
		which are the "scaled residuals" of the sigma points, we can choose an orthogonal matrix \( \mat{\Theta} \) such that
		\begin{equation}
			\mat{\mathcal{Y}} \mat{\Theta} = \begin{bmatrix} \mat{B} & \mat{O}_{k \times (k + 1)} \end{bmatrix}  \label{eqn:sqrtFilteringQR}
		\end{equation}
		Squaring both sides of this equation yields the covariance estimate
		\begin{equation*}
			\hat{\mat{\Sigma}} = \mat{\mathcal{Y}} \mat{\mathcal{Y}}^T = \mat{\mathcal{Y}} \mat{\Theta} \mat{\Theta}^T \mat{\mathcal{Y}} = \begin{bmatrix} \mat{B} & \mat{O}_{k \times (k + 1)} \end{bmatrix} \begin{bmatrix} \mat{B}^T \\ \mat{O}_{(k + 1) \times n} \end{bmatrix} = \mat{B} \mat{B}^T
		\end{equation*}
		so \( \mat{B} \) is the Cholesky decomposition of \( \hat{\mat{\Sigma}} \). But the equation~\eqref{eqn:sqrtFilteringQR}, where we neither now the orthogonal matrix \( \mat{\Theta} \) nor the right side of the equation, is exactly the type of matrix decomposition known as the \emph{QR decomposition} that is numerically more stable than the Cholesky decomposition as it does not require any special structure like positive definiteness of the matrix. In NumPy~\cite{harrisArrayProgrammingNumPy2020}, we can easily compute the matrix \( \mat{B} \) from \( \mathbb{\mathcal{Y}} \) with
		\begin{lstlisting}
Y = np.linalg.qr(Y.T).T
		\end{lstlisting}
		By applying this argumentation in an analogous way to every covariance computation in the cubature Kalman filter and \ac{rts} smoother, we find a complete square-root filtering and smoothing algorithm propagating the Cholesky decomposition of the covariance matrices \( \hat{\mat{V}}_{t \subgiven t - 1} \), \( \hat{\mat{V}}_{t \subgiven t} \) and \( \hat{\mat{V}}_{t \subgiven T} \).

		The square-root cubature Kalman filter is summarized in~\autoref{alg:sqrtCubatureKalmanFilter} and the square-root cubature \ac{rts} smoother is summarized in~\autoref{alg:sqrtCubatureRtsSmoother}.

		The square-root filtering now allows us to pass the Cholesky decomposition of the covariances directly through the smoothing and filtering process, allowing higher numerical stability because we guarantee the covariances to be positive definite even with approximations.

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{Initial state \( \vec{m}_0 \in \R^k \), covariance sqrt \( \mat{V}_0^{1/2} \) and observations \( \vec{y}_{1:T} \in \R^{p \times T} \).}
			\eqmakebox[sqrtckfinit][r]{\( \hat{\vec{s}}_{0 \subgiven 0} \gets \)} \( \vec{m}_0 \) \;
			\eqmakebox[sqrtckfinit][r]{\( \hat{\mat{V}}_{0 \subgiven 0}^{1/2} \gets \)} \( \mat{V}_0^{1/2} \) \;
			\For{\( t = 1, 2, \rangedots, T \)}{
				\eqmakebox[sqrtckf][r]{\( \mat{\Lambda} \gets \)} \( \sqrt{n} \cdot \hat{\mat{V}}_{t - 1 \subgiven t - 1}^{1/2} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t - 1 \subgiven t - 1} \gets \)} \( \begin{bmatrix} \hat{\vec{s}}_{t - 1 \subgiven t - 1} \vec{1}^T + \mat{\Lambda} & \hat{\vec{s}}_{t - 1 \subgiven t - 1} \vec{1}^T - \mat{\Lambda} \end{bmatrix} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t \subgiven t - 1, i}^s \gets \)} \( \vec{f}(\mat{\mathcal{X}}_{t - 1 \subgiven t - 1, i}) + (-1)^i \cdot \mat{Q} \),\quad \( i = 1, 2, \rangedots, 2k \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{X}}_{t \subgiven t - 1, i}^y \gets \)} \( \vec{g}\big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^s\big) + (-1)^i \cdot \mat{R} \),\quad \( i = 1, 2, \rangedots, 2k \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{s}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2k} \sum_{i = 1}^{2k} \mat{\mathcal{X}}_{t \subgiven t - 1, i}^s \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{y}}_{t \subgiven t - 1} \gets \)} \( \frac{1}{2k} \sum_{i = 1}^{2k} \mat{\mathcal{X}}_{t \subgiven t - 1, i}^y \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{Y}}_{t \subgiven t - 1, i}^s \gets \)} \( \big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^s - \hat{\vec{s}}_{t \subgiven t - 1}\big)/\sqrt{2k} \) \;
				\eqmakebox[sqrtckf][r]{\( \mat{\mathcal{Y}}_{t \subgiven t - 1, i}^y \gets \)} \( \big(\mat{\mathcal{X}}_{t \subgiven t - 1, i}^y - \hat{\vec{y}}_{t \subgiven t - 1}\big)/\sqrt{2k} \) \;
				\begin{minipage}{\linewidth}
					Find \( \mat{S}_t^{1/2} \), \( \mat{J}_t \) and \( \hat{\mat{V}}_{t \subgiven t}^{1/2} \) using the QR decomposition: \\
					\null\hspace{1cm} \(
						\begin{bmatrix}
							\mat{\mathcal{Y}}_{t \subgiven t - 1}^y \\
							\mat{\mathcal{Y}}_{t \subgiven t - 1}^s
						\end{bmatrix}
						\mat{\Theta}
						=
						\begin{bmatrix}
							\mat{S}_t^{1/2} & \mat{O} & \mat{O} \\
							\mat{J}_t       & \hat{\mat{V}}_{t \subgiven t}^{1/2} & \mat{O}
						\end{bmatrix}
					\)
				\end{minipage} \; \vspace{-0.5cm}
				\eqmakebox[sqrtckf][r]{\( \mat{K}_t \gets \)} \( \mat{J}_t \mat{S}_t^{-1/2} \) \;
				\eqmakebox[sqrtckf][r]{\( \hat{\vec{s}}_{t \subgiven t} \gets \)} \( \hat{\vec{s}}_{t \subgiven t - 1} + \mat{K}_t (\vec{y}_t - \hat{\vec{y}}_{t \subgiven t - 1}) \) \;
			}
			\caption{Square-Root Cubature Kalman Filter}
			\label{alg:sqrtCubatureKalmanFilter}
		\end{algorithm}

		\begin{algorithm}  \DontPrintSemicolon
			\KwData{All values from the Kalman filter.}
			\For{\( t = T - 1, T - 2, \rangedots, 1 \)}{
				\eqmakebox[sqrtcrts][r]{\( \mat{\mathcal{Y}}_{t \subgiven t, i}^s \gets \)} \( (\mat{\mathcal{X}}_{t \subgiven t, i} - \hat{\vec{s}}_{t \subgiven t}) \) \;
				\begin{minipage}{\linewidth}
					Find \( \mat{D}_t \) and \( \mat{Z}_t \) using the QR decomposition: \\
					\null\hspace{1cm} \(
						\begin{bmatrix}
							\mat{\mathcal{Y}}_{t + 1 \subgiven t}^s \\
							\mat{\mathcal{Y}}_{t \subgiven t}^s
						\end{bmatrix}
						\mat{\Theta}
						=
						\begin{bmatrix}
							\hat{\mat{V}}_{t + 1 \subgiven t}^{1/2} & \mat{O}   & \mat{O} \\
							\mat{D}_t                               & \mat{Z}_t & \mat{O}
						\end{bmatrix}
					\)
				\end{minipage} \; \vspace{-0.5cm}
				\eqmakebox[sqrtcrts][r]{\( \mat{J}_t \gets \)} \( \mat{D}_t \hat{\mat{V}}_{t + 1 \subgiven t}^{-1/2} \) \;
				\eqmakebox[sqrtcrts][r]{\( \hat{\vec{s}}_{t \subgiven T} \gets \)} \( \hat{\vec{s}}_{t \subgiven t} + \mat{J}_t (\hat{\vec{s}}_{t + 1 \subgiven T} - \hat{\vec{s}}_{t + 1 \subgiven t}) \) \;
				\begin{minipage}{\linewidth}
					Find \( \hat{\mat{V}}_{t \subgiven T}^{1/2} \) using the QR decomposition: \\
					\null\hspace{1cm} \(
						\begin{bmatrix}
							\mat{Z}_t & \mat{J}_t \hat{\mat{V}}_{t + 1 \subgiven T}^{1/2}
						\end{bmatrix}
						\mat{\Theta}
						=
						\begin{bmatrix}
							\hat{\mat{V}}_{t \subgiven T}^{1/2} & \mat{O}
						\end{bmatrix}
					\)
				\end{minipage} \; \vspace{-0.5cm}
			}
			\caption{Square-Root Cubature Rauch-Tung-Striebel Smoother}
			\label{alg:sqrtCubatureRtsSmoother}
		\end{algorithm}
	% end
% end
