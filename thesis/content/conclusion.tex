\chapter{Conclusion}
\label{c:conclusion}



Linearization is very important in control to handle nonlinear systems that exhibit complicated dynamics. We have looked at Koopman theory, a technique to find globally linear embedding for a nonlinear system. Unfortunately, the Koopman operator, the core of Koopman theory, is generally infinite-dimensional. Prior work~\cite{bruntonKoopmanInvariantSubspaces2016,kaiserDatadrivenDiscoveryKoopman2020,luschDeepLearningUniversal2018} has shown that we can find a finite-dimensional representation of the operator. These representations seek Koopman eigenfunctions that span an invariant subspace of the embedding, and do not transform under the influence of the Koopman operator other than being scaled. Nevertheless, most of the existing approaches work on deterministic models, seeking the Koopman eigenfunctions. While methods have been proposed which take a probabilistic perspective on the Koopman operator, they use complicated model setups with multiple neural networks and do not directly gauge the uncertainty of model predictions.

In this thesis we present a combination of existing theory for \aclp{lgds} with cubature rules to approximate arising nonlinear Gaussian integrals in a deterministic way to build up an \acl{em} algorithm. We called the resulting algorithm the \emph{Koopman inference} algorithm. It alternates between estimating the states in the E-step and optimizing the linear latent state dynamics and a nonlinear observation function that maps the linear latent states back to the nonlinear dynamics in the M-step. This way, we are able to predict the nonlinear dynamics and learned the Koopman observation functions without hand-tuned loss functions like in~\cite{luschDeepLearningUniversal2018}.

We evaluated the model performance on multiple different environments: A simple pendulum, a damped pendulum, the cartpole environment and a double pendulum. We also proved, and showed empirically, that our algorithm extends the common algorithm for learning \ac{lgds} by showing that the used  integral approximations are exact for linear functions and by validating the result on a regular \ac{lgds}. We discovered out that systems that lose energy (\eg the damped pendulum) are generally easier to learn than systems that conserve energy (\eg the undamped pendulum), as the latent dynamics have to be stable but should not approach zero to keep moving. Additionally, we discovered that the pendulum with sine/cosine features is a lot harder to learn, presumably because of the additional nonlinearity added by the feature transformation. Also, the algorithm was able to provide a rough approximation for the dynamics of the double pendulum environment, even though this is a chaotic system and it was not able to predict further movements beyond the training data.

Finally, we compared our Koopman inference algorithm against the \acl{dvk} model proposed by~\cite{mortonDeepVariationalKoopman2019a}. We saw that our algorithm performs similar, however, the \ac{dvk} model is better at predicting the dynamics beyond the training data, while our model is primarily able to predict until the end of the training data. We should note that the \ac{dvk} performs reconstruction from the latents that were learned during the training while our model performs the rollout from the beginning.

Overall we are satisfied with the performance of the Koopman inference algorithm. It allows further improvements to learning the influence of control inputs on actuated systems or employing a Bayesian view on the latent dimensions and further gauge the uncertainty on the state dynamics.
