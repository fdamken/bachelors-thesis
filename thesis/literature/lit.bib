
@inproceedings{andersonCommunicationAvoidingQRDecomposition2011a,
  title = {Communication-{{Avoiding QR Decomposition}} for {{GPUs}}},
  booktitle = {2011 {{IEEE International Parallel Distributed Processing Symposium}}},
  author = {Anderson, Michael and Ballard, Grey and Demmel, James and Keutzer, Kurt},
  year = {2011},
  month = may,
  pages = {48--58},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2011.15},
  abstract = {We describe an implementation of the Communication-Avoiding QR (CAQR) factorization that runs entirely on a single graphics processor (GPU). We show that the reduction in memory traffic provided by CAQR allows us to outperform existing parallel GPU implementations of QR for a large class of tall-skinny matrices. Other GPU implementations of QR handle panel factorizations by either sending the work to a general-purpose processor or using entirely bandwidth-bound operations, incurring data transfer overheads. In contrast, our QR is done entirely on the GPU using compute-bound kernels, meaning performance is good regardless of the width of the matrix. As a result, we outperform CULA, a parallel linear algebra library for GPUs by up to 17x for tall-skinny matrices and Intel's Math Kernel Library (MKL) by up to 12x. We also discuss stationary video background subtraction as a motivating application. We apply a recent statistical approach, which requires many iterations of computing the singular value decomposition of a tall-skinny matrix. Using CAQR as a first step to getting the singular value decomposition, we are able to get the answer 3x faster than if we use a traditional bandwidth-bound GPU QR factorization tuned specifically for that matrix size, and 30x faster than if we use Intel's Math Kernel Library (MKL) singular value decomposition routine on a multicore CPU.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/TWHMT39R/Anderson et al. - 2011 - Communication-Avoiding QR Decomposition for GPUs.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/SHP5S74T/6012824.html},
  keywords = {bandwidth-bound operations,communication-avoiding QR decomposition,communication-avoiding QR factorization,compute-bound kernels,computer graphic equipment,coprocessors,CULA parallel linear algebra library,data transfer overheads,general-purpose processor,GPU,graphics processing unit,Graphics processing unit,Instruction sets,Intel math kernel library,Kernel,Libraries,Matrix decomposition,principal component analysis,QR handle panel factorization,singular value decomposition,statistical approach,tall-skinny matrix,Vegetation,video background subtraction,video signal processing}
}

@article{arasaratnamCubatureKalmanFilters2009,
  title = {Cubature {{Kalman Filters}}},
  author = {Arasaratnam, Ienkaran and Haykin, Simon},
  year = {2009},
  month = jun,
  volume = {54},
  pages = {1254--1269},
  issn = {1558-2523},
  doi = {10.1109/TAC.2009.2019800},
  abstract = {In this paper, we present a new nonlinear filter for high-dimensional state estimation, which we have named the cubature Kalman filter (CKF). The heart of the CKF is a spherical-radial cubature rule, which makes it possible to numerically compute multivariate moment integrals encountered in the nonlinear Bayesian filter. Specifically, we derive a third-degree spherical-radial cubature rule that provides a set of cubature points scaling linearly with the state-vector dimension. The CKF may therefore provide a systematic solution for high-dimensional nonlinear filtering problems. The paper also includes the derivation of a square-root version of the CKF for improved numerical stability. The CKF is tested experimentally in two nonlinear state estimation problems. In the first problem, the proposed cubature rule is used to compute the second-order statistics of a nonlinearly transformed Gaussian random variable. The second problem addresses the use of the CKF for tracking a maneuvering aircraft. The results of both experiments demonstrate the improved performance of the CKF over conventional nonlinear filters.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KXRA7ESV/Arasaratnam and Haykin - 2009 - Cubature Kalman Filters.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/Z37QMZN4/4982682.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian filters,Bayesian methods,cubature Kalman filters,cubature points,cubature rules,Filtering,Gaussian processes,Gaussian quadrature rules,Gaussian random variable,Heart,high dimensional nonlinear filtering,high dimensional state estimation,invariant theory,Kalman filter,Kalman filters,maneuvering aircraft tracking,moment integrals,nonlinear Bayesian filter,nonlinear filtering,nonlinear filters,Nonlinear filters,nonlinear state estimation,numerical stability,Numerical stability,Random variables,second-order statistics,spherical-radial cubature rule,state estimation,State estimation,state vector dimension,Statistics,Testing},
  number = {6}
}

@article{ballardCommunicationoptimalParallelSequential2010,
  title = {Communication-Optimal {{Parallel}} and {{Sequential Cholesky Decomposition}}},
  author = {Ballard, Grey and Demmel, James and Holtz, Olga and Schwartz, Oded},
  year = {2010},
  month = jan,
  volume = {32},
  pages = {3495--3523},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/090760969},
  abstract = {Numerical algorithms have two kinds of costs: arithmetic and communication, by which we mean either moving data between levels of a memory hierarchy (in the sequential case) or over a network connecting processors (in the parallel case). Communication costs often dominate arithmetic costs, so it is of interest to design algorithms minimizing communication. In this paper we first extend known lower bounds on the communication cost (both for bandwidth and for latency) of conventional (\$O(n\^3)\$) matrix multiplication to Cholesky factorization, which is used for solving dense symmetric positive definite linear systems. Second, we compare the costs of various Cholesky decomposition implementations to these lower bounds and identify the algorithms and data structures that attain them. In the sequential case, we consider both the two-level and hierarchical memory models. Combined with prior results in [J. Demmel et al., Communication-optimal Parallel and Sequential QR and LU Factorizations, Technical report EECS-2008-89, University of California, Berkeley, CA, 2008], [J. Demmel et al., Implementing Communication-optimal Parallel and Sequential QR and LU Factorizations, SIAM. J. Sci. Comp., submitted], and [J. Demmel, L. Grigori, and H. Xiang, Communication-avoiding Gaussian Elimination, Proceedings of the 2008 ACM/IEEE Conference on Supercomputing, 2008] this gives a set of communication-optimal algorithms for \$O(n\^3)\$ implementations of the three basic factorizations of dense linear algebra: LU with pivoting, QR, and Cholesky. But it goes beyond this prior work on sequential LU by optimizing communication for any number of levels of memory hierarchy.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/YF9YZKAW/Ballard et al. - 2010 - Communication-optimal Parallel and Sequential Chol.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/YQ3YZ746/090760969.html},
  journal = {SIAM Journal on Scientific Computing},
  number = {6}
}

@phdthesis{bealVariationalAlgorithmsApproximate2003a,
  title = {Variational Algorithms for Approximate {{Bayesian}} Inference},
  author = {Beal, Matthew J.},
  year = {2003},
  abstract = {The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. Unfortunately the computations required are usually intractable. This thesis presents a unified variational Bayesian (VB) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood. Chapter 1 presents background material on Bayesian inference, graphical models, and propagation algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation- maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM algorithm which integrates over model parameters. The algorithm is then specialised to the large family of conjugate-exponential (CE) graphical models, and several theorems are presented to pave the road for automated VB derivation procedures in both directed and undirected graphs (Bayesian and Markov networks, respectively). Chapters 3\textendash 5 derive and apply the VB EM algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models. It is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using VB approximations. Also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. Throughout, the VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood.},
  copyright = {open},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/DISILN6V/Beal - 2003 - Variational algorithms for approximate Bayesian in.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/KJDHW239/10101435.html;/home/fdamken/snap/zotero-snap/common/Zotero/storage/TEJ584VK/10101435.html},
  journal = {Doctoral thesis, UCL (University College London).},
  language = {eng},
  school = {UCL (University College London)},
  type = {Doctoral}
}

@techreport{bealVariationalKalmanSmoother2000,
  title = {The {{Variational Kalman Smoother}}},
  author = {Beal, Matthew J. and Ghahramani, Zoubin},
  year = {2000},
  month = may,
  pages = {15},
  institution = {{Gatsby Computational Neuroscience Unit}},
  abstract = {In this note we outline the derivation of the variational Kalman smoother, in the context of Bayesian Linear Dynamical Systems. The smoother is an efficient algorithm for the E-step in the Expectation-Maximisation (EM) algorithm for linear-Gaussian state-space models. However, inference approximations are required if we hold distributions over parameters. We derive the E-step updates for the hidden states (the variational smoother), and the M-step updates for the parameter distributions. We shpw that inference of the hidden state is tractable for any distribution over parameters, provided the expectations of certain quantities available, analytically or otherwise.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/APDQSUZN/Beal and Ghahramani - 2000 - The Variational Kalman Smoother.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UZDVPPBQ/vbssm.tgz},
  language = {en}
}

@article{beckerRecurrentKalmanNetworks2019a,
  title = {Recurrent {{Kalman Networks}}: {{Factorized Inference}} in {{High}}-{{Dimensional Deep Feature Spaces}}},
  shorttitle = {Recurrent {{Kalman Networks}}},
  author = {Becker, Philipp and Pandya, Harit and Gebhardt, Gregor and Zhao, Cheng and Taylor, James and Neumann, Gerhard},
  year = {2019},
  month = may,
  abstract = {In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models, however, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time step. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter \& Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.},
  archivePrefix = {arXiv},
  eprint = {1905.07357},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/ZJGTK3VI/Becker et al. - 2019 - Recurrent Kalman Networks Factorized Inference in.pdf},
  journal = {arXiv:1905.07357 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{biermanSequentialSquareRoot1974,
  title = {Sequential Square Root Filtering and Smoothing of Discrete Linear Systems},
  author = {Bierman, Gerald J.},
  year = {1974},
  month = mar,
  volume = {10},
  pages = {147--158},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(74)90020-X},
  abstract = {Square-root information estimation algorithms are immensely important estimation analysis tools that are not sufficiently well understood nor adequately exploited. In an endeavor to rectify this state of affairs an expository derivation of the square-root information filter/smoother is given. It is based on the recursive least-squares method and is easier to grasp, interpret and generalize than are the dynamic programming arguments previously used. Backward smoothing algorithms, both square-root and covariance recursions, are derived as direct and consequences of the method. A comparison of smoothing algorithms indicates that those presented in this paper are the most efficient. Partitioning the results to separate bias parameters provides further computational economies and reduction of storage requirements. The principal objective of this paper is to inspire greater utilization of square-root estimation algorithms. Arguments supporting this thesis are the new least-squares filter/smoother derivations, enhanced numerical accuracy, reduced computation, and lower storage requirements.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/YUWGRB8R/Bierman - 1974 - Sequential square root filtering and smoothing of .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XZBHHJW2/000510987490020X.html},
  journal = {Automatica},
  language = {en},
  number = {2}
}

@book{birkhoffDynamicalSystems1927,
  title = {Dynamical {{Systems}}},
  author = {Birkhoff, George David},
  year = {1927},
  month = dec,
  publisher = {{American Mathematical Soc.}},
  abstract = {His research in dynamics constitutes the middle period of Birkhoff\&\#39;s scientific career, that of maturity and greatest power. --Yearbook of the American Philosophical Society The author\&\#39;s great book ... is well known to all, and the diverse active modern developments in mathematics which have been inspired by this volume bear the most eloquent testimony to its quality and influence. --Zentralblatt MATH In 1927, G. D. Birkhoff wrote a remarkable treatise on the theory of dynamical systems that would inspire many later mathematicians to do great work. To a large extent, Birkhoff was writing about his own work on the subject, which was itself strongly influenced by Poincare\&\#39;s approach to dynamical systems. With this book, Birkhoff also demonstrated that the subject was a beautiful theory, much more than a compendium of individual results. The influence of this work can be found in many fields, including differential equations, mathematical physics, and even what is now known as Morse theory. The present volume is the revised 1966 reprinting of the book, including a new addendum, some footnotes, references added by Jurgen Moser, and a special preface by Marston Morse. Although dynamical systems has thrived in the decades since Birkhoff\&\#39;s book was published, this treatise continues to offer insight and inspiration for still more generations of mathematicians.},
  googlebooks = {ygmWAwAAQBAJ},
  isbn = {978-0-8218-1009-5},
  keywords = {Mathematics / Geometry / Differential},
  language = {en}
}

@article{brockmanOpenAIGym2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  month = jun,
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archivePrefix = {arXiv},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KNNCTET2/Brockman et al. - 2016 - OpenAI Gym.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/6FBEAA3B/1606.html},
  journal = {arXiv:1606.01540 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{bruntonKoopmanInvariantSubspaces2016,
  title = {Koopman {{Invariant Subspaces}} and {{Finite Linear Representations}} of {{Nonlinear Dynamical Systems}} for {{Control}}},
  author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kutz, J. Nathan},
  year = {2016},
  month = feb,
  volume = {11},
  pages = {e0150171},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0150171},
  abstract = {In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by {$\mathscr{l}$}1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/L4YWR84B/Brunton et al. - 2016 - Koopman Invariant Subspaces and Finite Linear Repr.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/R4MFTEA7/article.html},
  journal = {PLOS ONE},
  keywords = {Algorithms,Dynamical systems,Eigenvalues,Eigenvectors,Fluid dynamics,Nonlinear dynamics,Nonlinear systems,Polynomials},
  language = {en},
  number = {2}
}

@article{ceppelliniEstimationGeneFrequencies1955,
  title = {The {{Estimation}} of {{Gene Frequencies}} in a {{Random}}-{{Mating Population}}},
  author = {Ceppellini, By R. and Siniscalco, M. and Smith, C. a. B.},
  year = {1955},
  volume = {20},
  pages = {97--115},
  issn = {1469-1809},
  doi = {10.1111/j.1469-1809.1955.tb01360.x},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1955.tb01360.x},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/GUHENABM/j.1469-1809.1955.tb01360.html},
  journal = {Annals of Human Genetics},
  language = {en},
  number = {2}
}

@article{debezenacNormalizingKalmanFilters2020,
  title = {Normalizing {{Kalman Filters}} for {{Multivariate Time Series Analysis}}},
  author = {{de B{\'e}zenac}, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and {Bohlke-Schneider}, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
  year = {2020},
  volume = {33},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/WVKBPKZM/de Bézenac et al. - 2020 - Normalizing Kalman Filters for Multivariate Time S.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XQCL8H9Q/1f47cef5e38c952f94c5d61726027439-Abstract.html},
  journal = {Advances in Neural Information Processing Systems},
  language = {en}
}

@article{deisenrothProbabilisticPerspectiveGaussian2011,
  title = {A {{Probabilistic Perspective}} on {{Gaussian Filtering}} and {{Smoothing}}},
  author = {Deisenroth, Marc Peter and Ohlsson, Henrik},
  year = {2011},
  month = jun,
  abstract = {We present a general probabilistic perspective on Gaussian filtering and smoothing. This allows us to show that common approaches to Gaussian filtering/smoothing can be distinguished solely by their methods of computing/approximating the means and covariances of joint probabilities. This implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments. Based on this insight, we derive the cubature Kalman smoother and propose a novel robust filtering and smoothing algorithm based on Gibbs sampling.},
  archivePrefix = {arXiv},
  eprint = {1006.2165},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/SZAL384A/Deisenroth and Ohlsson - 2011 - A Probabilistic Perspective on Gaussian Filtering .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/EGK5Y7DI/1006.html},
  journal = {arXiv:1006.2165 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, math, stat}
}

@article{dempsterMaximumLikelihoodIncomplete1977a,
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  volume = {39},
  pages = {1--22},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/V9DKJVDW/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data Via the EM.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/Y6HWM7RS/j.2517-6161.1977.tb01600.html},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
  language = {en},
  number = {1}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  issn = {1533-7928},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/BT6LLCJH/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HHB9TDLA/duchi11a.html},
  journal = {Journal of Machine Learning Research},
  number = {61}
}

@book{elhadjDynamicalSystemsTheories2019,
  title = {Dynamical {{Systems}}: {{Theories}} and {{Applications}}},
  shorttitle = {Dynamical {{Systems}}},
  author = {Elhadj, Zeraoulia},
  year = {2019},
  month = jan,
  publisher = {{CRC Press}},
  abstract = {Chaos is the idea that a system will produce very different long-term behaviors when the initial conditions are perturbed only slightly. Chaos is used for novel, time- or energy-critical interdisciplinary applications. Examples include high-performance circuits and devices, liquid mixing, chemical reactions, biological systems, crisis management, secure information processing, and critical decision-making in politics, economics, as well as military applications, etc. This book presents the latest investigations in the theory of chaotic systems and their dynamics. The book covers some theoretical aspects of the subject arising in the study of both discrete and continuous-time chaotic dynamical systems. This book presents the state-of-the-art of the more advanced studies of chaotic dynamical systems.},
  googlebooks = {mFupDwAAQBAJ},
  isbn = {978-0-429-65006-2},
  keywords = {Mathematics / Arithmetic,Mathematics / Differential Equations / General,Science / Life Sciences / General,Science / Physics / Mathematical \& Computational},
  language = {en}
}

@article{fesslerSpacealternatingGeneralizedExpectationmaximization1994,
  title = {Space-Alternating Generalized Expectation-Maximization Algorithm},
  author = {Fessler, J.A. and Hero, A.O.},
  year = {1994},
  month = oct,
  volume = {42},
  pages = {2664--2677},
  issn = {1941-0476},
  doi = {10.1109/78.324732},
  abstract = {The expectation-maximization (EM) method can facilitate maximizing likelihood functions that arise in statistical estimation problems. In the classical EM paradigm, one iteratively maximizes the conditional log-likelihood of a single unobservable complete data space, rather than maximizing the intractable likelihood function for the measured or incomplete data. EM algorithms update all parameters simultaneously, which has two drawbacks: 1) slow convergence, and 2) difficult maximization steps due to coupling when smoothness penalties are used. The paper describes the space-alternating generalized EM (SAGE) method, which updates the parameters sequentially by alternating between several small hidden-data spaces defined by the algorithm designer. The authors prove that the sequence of estimates monotonically increases the penalized-likelihood objective, derive asymptotic convergence rates, and provide sufficient conditions for monotone convergence in norm. Two signal processing applications illustrate the method: estimation of superimposed signals in Gaussian noise, and image reconstruction from Poisson measurements. In both applications, the SAGE algorithms easily accommodate smoothness penalties and converge faster than the EM algorithms.{$<>$}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IB6LW2P7/Fessler and Hero - 1994 - Space-alternating generalized expectation-maximiza.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/7I3KD9JZ/324732.html},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {Algorithm design and analysis,asymptotic convergence rates,classical EM paradigm,conditional log-likelihood,Convergence,convergence of numerical methods,Expectation-maximization algorithms,Extraterrestrial measurements,Gaussian noise,image reconstruction,Image reconstruction,intractable likelihood function,Iterative algorithms,iterative methods,maximization steps,maximum likelihood estimation,monotone convergence,parameter estimation,penalized-likelihood objective,Poisson measurements,random noise,SAGE method,sequence of estimates,signal processing,Signal processing,Signal processing algorithms,signal processing applications,small hidden-data spaces,smoothness penalties,space-alternating generalized expectation-maximization algorithm,statistical analysis,statistical estimation problems,stochastic processes,sufficient conditions,Sufficient conditions,superimposed signals,unobservable complete data space},
  number = {10}
}

@misc{FirstHandUnscentedTransform,
  title = {First-{{Hand}}:{{The Unscented Transform}} - {{Engineering}} and {{Technology History Wiki}}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KUKVLL58/First-HandThe_Unscented_Transform.html},
  howpublished = {https://ethw.org/First-Hand:The\_Unscented\_Transform}
}

@book{florianCorrectEquationsDynamics2005,
  title = {Correct Equations for the Dynamics of the Cart-Pole},
  author = {Florian, R{\u a}zvan V.},
  year = {2005},
  abstract = {system},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/NVTZGS2D/Florian - 2005 - Correct equations for the dynamics of the cart-pol.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/4Y4U5B5I/summary.html}
}

@techreport{ghahramaniParameterEstimationLinear1996,
  title = {Parameter Estimation for Linear Dynamical Systems},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {Februrary 22, 1996},
  institution = {{University of Toronto}},
  abstract = {Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Stoffer, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RHES3QAG/Ghahramani and Hinton - 1996 - Parameter estimation for linear dynamical systems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/V49S4PIV/lds.tar.gz},
  number = {CRG-TR-96-2}
}

@article{greffSacredInfrastructureComputational2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"u}rgen},
  year = {2017},
  pages = {49--56},
  doi = {10.25080/shinma-7f4c6e7-008},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/M3TBILUM/Greff et al. - 2017 - The Sacred Infrastructure for Computational Resear.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/FYWSTP2F/klaus_greff.html},
  journal = {Proceedings of the 16th Python in Science Conference}
}

@article{guglielmiImplementingRadauIIA2001,
  title = {Implementing {{Radau IIA Methods}} for {{Stiff Delay Differential Equations}}},
  author = {Guglielmi, Nicola and Hairer, Ernst},
  year = {2001},
  month = jul,
  volume = {67},
  pages = {1--12},
  issn = {1436-5057},
  doi = {10.1007/s006070170013},
  abstract = {This article discusses the numerical solution of a general class of delay differential equations, including stiff problems, differential-algebraic delay equations, and neutral problems. The delays can be state dependent, and they are allowed to become small and vanish during the integration. Difficulties encountered in the implementation of implicit Runge\textendash Kutta methods are explained, and it is shown how they can be overcome. The performance of the resulting code \textendash{} RADAR5 \textendash{} is illustrated on several examples, and it is compared to existing programs.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/ASBU9KQH/Guglielmi and Hairer - 2001 - Implementing Radau IIA Methods for Stiff Delay Dif.pdf},
  journal = {Computing},
  language = {en},
  number = {1}
}

@article{hanDeepLearningKoopman2020,
  title = {Deep {{Learning}} of {{Koopman Representation}} for {{Control}}},
  author = {Han, Yiqiang and Hao, Wenjian and Vaidya, Umesh},
  year = {2020},
  month = oct,
  abstract = {We develop a data-driven, model-free approach for the optimal control of the dynamical system. The proposed approach relies on the Deep Neural Network (DNN) based learning of Koopman operator for the purpose of control. In particular, DNN is employed for the data-driven identification of basis function used in the linear lifting of nonlinear control system dynamics. The controller synthesis is purely data-driven and does not rely on a priori domain knowledge. The OpenAI Gym environment, employed for Reinforcement Learning-based control design, is used for data generation and learning of Koopman operator in control setting. The method is applied to two classic dynamical systems on OpenAI Gym environment to demonstrate the capability.},
  archivePrefix = {arXiv},
  eprint = {2010.07546},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/HEZJLA89/Han et al. - 2020 - Deep Learning of Koopman Representation for Contro.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/U6KU3ZQT/2010.html},
  journal = {arXiv:2010.07546 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  primaryClass = {cs, eess}
}

@article{harrisArrayProgrammingNumPy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  volume = {585},
  pages = {357--362},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  copyright = {2020 The Author(s)},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/2LXWVXCL/Harris et al. - 2020 - Array programming with NumPy.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5J836N6I/s41586-020-2649-2.html},
  journal = {Nature},
  language = {en},
  number = {7825}
}

@article{itoGaussianFiltersNonlinear2000,
  title = {Gaussian Filters for Nonlinear Filtering Problems},
  author = {Ito, K. and Xiong, K.},
  year = {2000},
  month = may,
  volume = {45},
  pages = {910--927},
  issn = {1558-2523},
  doi = {10.1109/9.855552},
  abstract = {We develop and analyze real-time and accurate filters for nonlinear filtering problems based on the Gaussian distributions. We present the systematic formulation of Gaussian filters and develop efficient and accurate numerical integration of the optimal filter. We also discuss the mixed Gaussian filters in which the conditional probability density is approximated by the sum of Gaussian distributions. A new update rule of weights for Gaussian sum filters is proposed. Our numerical tests demonstrate that new filters significantly improve the extended Kalman filter with no additional cost, and the new Gaussian sum filter has a nearly optimal performance.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MSWJ85KN/Ito and Xiong - 2000 - Gaussian filters for nonlinear filtering problems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/A4S4RVGQ/855552.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian methods,Cost function,Filtering,filtering theory,Filters,Gaussian distribution,Gaussian distributions,Gaussian filters,Gaussian processes,Indium tin oxide,Kalman filter,Kalman filters,nonlinear filtering,probability density,Sonar navigation,Testing},
  number = {5}
}

@article{jensenFonctionsConvexesInegalites1906,
  title = {{Sur les fonctions convexes et les in\'egalit\'es entre les valeurs moyennes}},
  author = {Jensen, J. L. W. V.},
  year = {1906},
  volume = {30},
  pages = {175--193},
  publisher = {{Institut Mittag-Leffler}},
  issn = {0001-5962, 1871-2509},
  doi = {10.1007/BF02418571},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4G6N6HBP/Jensen - 1906 - Sur les fonctions convexes et les inégalités entre.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5MWXFHEI/1485887155.html},
  journal = {Acta Mathematica},
  language = {FR},
  mrnumber = {MR1555027},
  zmnumber = {0087.27403}
}

@inproceedings{julierNewApproachFiltering1995,
  title = {A New Approach for Filtering Nonlinear Systems},
  booktitle = {Proceedings of 1995 {{American Control Conference}} - {{ACC}}'95},
  author = {Julier, S. J. and Uhlmann, J. K. and {Durrant-Whyte}, H. F.},
  year = {1995},
  month = jun,
  volume = {3},
  pages = {1628-1632 vol.3},
  doi = {10.1109/ACC.1995.529783},
  abstract = {In this paper we describe a new recursive linear estimator for filtering systems with nonlinear process and observation models. This method uses a new parameterisation of the mean and covariance which can be transformed directly by the system equations to give predictions of the transformed mean and covariance. We show that this technique is more accurate and far easier to implement than an extended Kalman filter. Specifically, we present empirical results for the application of the new filter to the highly nonlinear kinematics of maneuvering vehicles.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/PUVEWXDS/Julier et al. - 1995 - A new approach for filtering nonlinear systems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/3ATPD9YH/529783.html},
  keywords = {Additive noise,covariance parameterisation,Filtering,filtering theory,highly nonlinear kinematics,maneuvering vehicles,mean parameterisation,Nonlinear dynamical systems,Nonlinear equations,nonlinear filters,Nonlinear filters,nonlinear observation models,nonlinear process models,nonlinear system filtering,Nonlinear systems,recursive estimation,recursive filters,recursive linear estimator,Sensor fusion,Sensor systems,State estimation,Vehicle dynamics}
}

@article{kaiserDatadrivenDiscoveryKoopman2020,
  title = {Data-Driven Discovery of {{Koopman}} Eigenfunctions for Control},
  author = {Kaiser, Eurika and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2020},
  month = may,
  abstract = {Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to represent the dynamics directly in eigenfunction coordinates. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control is formulated in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including 1) a nonlinear system with a known linear embedding, 2) a variety of Hamiltonian systems, and 3) a high-dimensional double-gyre model for ocean mixing.},
  archivePrefix = {arXiv},
  eprint = {1707.01146},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/Y9HAHYJN/Kaiser et al. - 2020 - Data-driven discovery of Koopman eigenfunctions fo.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/N4K6NUEV/1707.html},
  journal = {arXiv:1707.01146 [math]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control},
  primaryClass = {math}
}

@article{kalmanNewApproachLinear1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  year = {1960},
  month = mar,
  volume = {82},
  pages = {35--45},
  publisher = {{American Society of Mechanical Engineers Digital Collection}},
  issn = {0021-9223},
  doi = {10.1115/1.3662552},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/43KCL2NE/Kalman - 1960 - A New Approach to Linear Filtering and Prediction .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/3BFEZUS4/397706.html},
  journal = {Journal of Basic Engineering},
  language = {en},
  number = {1}
}

@article{karlDeepVariationalBayes2017a,
  title = {Deep {{Variational Bayes Filters}}: {{Unsupervised Learning}} of {{State Space Models}} from {{Raw Data}}},
  shorttitle = {Deep {{Variational Bayes Filters}}},
  author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2017},
  month = mar,
  abstract = {We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
  archivePrefix = {arXiv},
  eprint = {1605.06432},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FY927WUQ/Karl et al. - 2017 - Deep Variational Bayes Filters Unsupervised Learn.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UQR3SNLA/1605.html},
  journal = {arXiv:1605.06432 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3QLZVKLI/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HEHEXW8E/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/XNWXKDKZ/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/VMLNAGA4/1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{koopmanHamiltonianSystemsTransformation1931,
  title = {Hamiltonian {{Systems}} and {{Transformation}} in {{Hilbert Space}}},
  author = {Koopman, B. O.},
  year = {1931},
  month = may,
  volume = {17},
  pages = {315--318},
  issn = {0027-8424},
  abstract = {In recent years the theory of Hilbert space and its linear transformations has come into prominence. It has been recognized to an increasing extent that many of the most important departments of mathematical physics can be subsumed under this theory. In classical physics, for example in those phenomena which are governed by linear conditions \textendash{} linear differential or integral equations and the like, in those relating to harmonic analysis, and in many phenomena due to the opetation of the laws of chance, the essential role is played by certain linear transformations in Hilbert space. And the importance of the theory in quantum mechanics is known to all. It is the object of this note to outline certain investigations of our own in which the domain of this theory has been extended in such a way as to include classical Hamiltonian mechanics, or, more generally, systems defining a stead n-dimensional flow of a fluid of positive density.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/U7CXQKGD/Koopman - 1931 - Hamiltonian Systems and Transformation in Hilbert .pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {5},
  pmcid = {PMC1076052},
  pmid = {16577368}
}

@article{luschDeepLearningUniversal2018,
  title = {Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics},
  author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {4950},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07210-0},
  abstract = {Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.},
  archivePrefix = {arXiv},
  eprint = {1712.09707},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RRYUXGIZ/Lusch et al. - 2018 - Deep learning for universal linear embeddings of n.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/LS6VRGYR/1712.html},
  journal = {Nature Communications},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  number = {1}
}

@inproceedings{maOptimalQuadraticRegulation2019,
  title = {Optimal {{Quadratic Regulation}} of {{Nonlinear System Using Koopman Operator}}},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Ma, Xu and Huang, Bowen and Vaidya, Umesh},
  year = {2019},
  month = jul,
  pages = {4911--4916},
  issn = {2378-5861},
  doi = {10.23919/ACC.2019.8814903},
  abstract = {In this paper, we study the optimal quadratic regulation problem for nonlinear systems. The linear operator theoretic framework involving the Koopman operator is used to lift the dynamics of nonlinear control system to an infinite dimensional bilinear system. Optimal quadratic regulation problem for nonlinear system is formulated in terms of the finite dimensional approximation of the bilinear system. A convex optimization-based approach is proposed for solving the quadratic regulator problem for bilinear system. Simulation results are presented to demonstrate the application of the developed framework.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/SPTN6ZG4/Ma et al. - 2019 - Optimal Quadratic Regulation of Nonlinear System U.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HA9GRSPU/8814903.html},
  keywords = {Aerospace electronics,bilinear systems,convex optimization,convex programming,Eigenvalues and eigenfunctions,infinite dimensional bilinear system,Koopman operator,linear operator theoretic framework,linear quadratic control,mathematical operators,multidimensional systems,nonlinear control system,nonlinear control systems,Nonlinear dynamical systems,nonlinear system,optimal control,Optimal control,optimal quadratic regulation problem,optimisation,quadratic regulator problem,Regulation}
}

@book{mauroyKoopmanOperatorSystems2020,
  title = {The {{Koopman Operator}} in {{Systems}} and {{Control}}: {{Concepts}}, {{Methodologies}}, and {{Applications}}},
  shorttitle = {The {{Koopman Operator}} in {{Systems}} and {{Control}}},
  editor = {Mauroy, Alexandre and Mezic, Igor and Susuki, Yoshihiko},
  year = {2020},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-35713-9},
  abstract = {This book provides a broad overview of state-of-the-art research at the intersection of the Koopman operator theory and control theory. It also reviews novel theoretical results obtained and efficient numerical methods developed within the framework of Koopman operator theory.The contributions discuss the latest findings and techniques in several areas of control theory, including model predictive control, optimal control, observer design, systems identification and structural analysis of controlled systems, addressing both theoretical and numerical aspects and presenting open research directions, as well as detailed numerical schemes and data-driven methods. Each contribution addresses a specific problem. After a brief introduction of the Koopman operator framework, including basic notions and definitions, the book explores numerical methods, such as the dynamic mode decomposition (DMD) algorithm and Arnoldi-based methods, which are used to represent the operator in a finite-dimensional basis and to compute its spectral properties from data. The main body of the book is divided into three parts:theoretical results and numerical techniques for observer design, synthesis analysis, stability analysis, parameter estimation, and identification;data-driven techniques based on DMD, which extract the spectral properties of the Koopman operator from data for the structural analysis of controlled systems; andKoopman operator techniques with specific applications in systems and control, which range from heat transfer analysis to robot control.A useful reference resource on the Koopman operator theory for control theorists and practitioners, the book is also of interest to graduate students, researchers, and engineers looking for an introduction to a novel and comprehensive approach to systems and control, from pure theory to data-driven methods.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/LP9ANR5W/9783030357122.html},
  isbn = {978-3-030-35712-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}}
}

@techreport{minkaHiddenMarkovModels1999,
  title = {From {{Hidden Markov Models}} to {{Linear Dynamical Systems}}},
  author = {Minka, Thomas P.},
  year = {1999},
  month = jul,
  abstract = {Hidden Markov Models (HMMs) and Linear Dynamical Systems (LDSs) ate based on the same assumption: a hidden state variable, of which we can make noisy measurements, evolves with Markovian dynamics. Both have the same independency diagram and consequently the learning and inference algorithms for both have the same structure. The only difference is that the HMM uses a discrete state variable with arbitrary dynamics and arbitrary measurements while the LDS uses a continuous state variable with linear-Gaussian dynamics and measurements. We show how the forward-backward equations for the HMM, specialized to linear-Gaussian assumptions, lead directly to Kalman filtering and Rauch-Tung-Streibel smoothing. We also investigate the most general possible modeling assumptions which can lead to efficient recursion in the case of continuous state variables.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3Y5XFY3M/Minka - 1999 - From Hidden Markov Models to Linear Dynamical Syst.pdf},
  number = {TR-531}
}

@article{mortonDeepVariationalKoopman2019a,
  title = {Deep {{Variational Koopman Models}}: {{Inferring Koopman Observations}} for {{Uncertainty}}-{{Aware Dynamics Modeling}} and {{Control}}},
  shorttitle = {Deep {{Variational Koopman Models}}},
  author = {Morton, Jeremy and Witherden, Freddie D. and Kochenderfer, Mykel J.},
  year = {2019},
  month = jun,
  abstract = {Koopman theory asserts that a nonlinear dynamical system can be mapped to a linear system, where the Koopman operator advances observations of the state forward in time. However, the observable functions that map states to observations are generally unknown. We introduce the Deep Variational Koopman (DVK) model, a method for inferring distributions over observations that can be propagated linearly in time. By sampling from the inferred distributions, we obtain a distribution over dynamical models, which in turn provides a distribution over possible outcomes as a modeled system advances in time. Experiments show that the DVK model is effective at long-term prediction for a variety of dynamical systems. Furthermore, we describe how to incorporate the learned models into a control framework, and demonstrate that accounting for the uncertainty present in the distribution over dynamical models enables more effective control.},
  archivePrefix = {arXiv},
  eprint = {1902.09742},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/5IU4LHDL/Morton et al. - 2019 - Deep Variational Koopman Models Inferring Koopman.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AEEQ9MTN/1902.html},
  journal = {arXiv:1902.09742 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\aftergroup\ignorespaces} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8026--8037},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/NKBF8PL6/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5VVE4H5D/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{rauchMaximumLikelihoodEstimates1965,
  title = {Maximum Likelihood Estimates of Linear Dynamic Systems},
  author = {Rauch, H. E. and Tung, F. and Striebel, C. T.},
  year = {1965},
  volume = {3},
  pages = {1445--1450},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  issn = {0001-1452},
  doi = {10.2514/3.3166},
  annotation = {\_eprint: https://doi.org/10.2514/3.3166},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FJPDZRFJ/RAUCH et al. - 1965 - Maximum likelihood estimates of linear dynamic sys.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UBE6VQV3/3.html},
  journal = {AIAA Journal},
  number = {8}
}

@inproceedings{ruttenSquarerootUnscentedFiltering2013,
  title = {Square-Root Unscented Filtering and Smoothing},
  booktitle = {2013 {{IEEE Eighth International Conference}} on {{Intelligent Sensors}}, {{Sensor Networks}} and {{Information Processing}}},
  author = {Rutten, Mark G.},
  year = {2013},
  month = apr,
  pages = {294--299},
  doi = {10.1109/ISSNIP.2013.6529805},
  abstract = {A square-root Kalman filter propagates the square-root (often the Cholesky factor) of the state covariance, rather than the full covariance matrix. Propagating these factors offers both computational efficiencies and greatly improved numerical properties. This paper introduces a new method of implementing the square-root unscented filter and the square-root unscented Rauch-Tung-Striebel smoother, which provide similar computational and numerical advantages over their traditional implementations. The new algorithms rely on the QR factorisation for calculating the covariance square-roots. A comparison with the previous development of the square-root unscented filter shows similar computational cost, while dramatically simplifying the implementation and improving numerical stability.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/C3KCE5E8/Rutten - 2013 - Square-root unscented filtering and smoothing.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/FJVC6S25/6529805.html},
  keywords = {Cholesky factor,Computational efficiency,covariance matrices,Covariance matrices,covariance matrix,covariance square-roots,Kalman filters,Mathematical model,Matrix decomposition,QR factorisation,smoothing methods,Smoothing methods,sqrt,square-root Kalman filter,square-root unscented filtering,square-root unscented Rauch-Tung-Striebel smoother,square-root unscented smoothing,state covariance,Time measurement}
}

@article{schonSystemIdentificationNonlinear2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  volume = {47},
  pages = {39--49},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.10.013},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provides arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MPU6JLFX/Schön et al. - 2011 - System identification of nonlinear state-space mod.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/22NXXPUY/S0005109810004279.html},
  journal = {Automatica},
  keywords = {Dynamic systems,Expectation maximisation algorithm,Monte Carlo method,Nonlinear models,Particle methods,Smoothing filters,System identification},
  language = {en},
  number = {1}
}

@article{shannonCommunicationPresenceNoise1949,
  title = {Communication in the {{Presence}} of {{Noise}}},
  author = {Shannon, C.E.},
  year = {1949},
  month = jan,
  volume = {37},
  pages = {10--21},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1949.232969},
  abstract = {A method is developed for representing any communication system geometrically. Messages and the corresponding signals are points in two "function spaces," and the modulation process is a mapping of one space into the other. Using this representation, a number of results in communication theory are deduced concerning expansion and compression of bandwidth and the threshold effect. Formulas are found for the maxmum rate of transmission of binary digits over a system when the signal is perturbed by various types of noise. Some of the properties of "ideal" systems which transmit at this maxmum rate are discussed. The equivalent number of binary digits per second for certain information sources is calculated.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3TND3SGW/Shannon - 1949 - Communication in the Presence of Noise.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/42RI2TYP/1697831.html},
  journal = {Proceedings of the IRE},
  keywords = {Bandwidth,Circuits,Communication systems,Electron tubes,Frequency measurement,Gain measurement,Klystrons,Shape,Telephony,Voltage},
  number = {1}
}

@article{shinbrotChaosDoublePendulum1992,
  title = {Chaos in a Double Pendulum},
  author = {Shinbrot, Troy and Grebogi, Celso and Wisdom, Jack and Yorke, James A.},
  year = {1992},
  month = jun,
  volume = {60},
  pages = {491--499},
  publisher = {{American Association of Physics Teachers}},
  issn = {0002-9505},
  doi = {10.1119/1.16860},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IRMN5ING/Shinbrot et al. - 1992 - Chaos in a double pendulum.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/827SUHI9/1.html},
  journal = {American Journal of Physics},
  number = {6}
}

@article{shumwayApproachTimeSeries1982a,
  title = {An {{Approach}} to {{Time Series Smoothing}} and {{Forecasting Using}} the {{Em Algorithm}}},
  author = {Shumway, R. H. and Stoffer, D. S.},
  year = {1982},
  volume = {3},
  pages = {253--264},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1982.tb00349.x},
  abstract = {Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.1982.tb00349.x},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/8D3FI89A/Shumway and Stoffer - 1982 - An Approach to Time Series Smoothing and Forecasti.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/NIT2IV25/full-text_ocr.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/U56SZFGC/full-text.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/IC35UB4V/j.1467-9892.1982.tb00349.html},
  journal = {Journal of Time Series Analysis},
  keywords = {EM algorithm,forecasting,Kalman filter,maximum likelihood,Missing data},
  language = {en},
  number = {4}
}

@phdthesis{solinCubatureIntegrationMethods2010,
  title = {Cubature {{Integration Methods}} in {{Non}}-{{Linear Kalman Filtering}} and {{Smoothing}}},
  author = {Solin, Arno},
  year = {2010},
  abstract = {Optimal estimation problems arise in various different settings where indirect noisy observations are used to determine the underlying state of a time-varying system. For systems with non-linear dynamics there exist various methods that extend linear filtering and smoothing methods to handle non-linearities. In this thesis the non-linear optimal estimation framework is presented with the help of an assumed density approach. The Gaussian integrals that arise in this setting are solved using two different cubature integration methods. Cubature integration extends the weighted sum approach from univariate quadrature methods to multidimensional cubature methods. In this thesis the focus is put on two methods that use deterministically chosen sigma points to form the desired approximation. The Gauss\textendash Hermite rule uses a simple product rule method to fill the multidimensional space with cubature points, whereas the spherical\textendash radial rule uses invariant theory to diminish the number of points by utilizing symmetries. The},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3ATM3DZ6/Solin - 2010 - Cubature Integration Methods in Non-Linear Kalman .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HQDQY8HG/summary.html},
  type = {Bachelor's {{Thesis}}}
}

@inproceedings{vandermerweSquarerootUnscentedKalman2001,
  title = {The Square-Root Unscented {{Kalman}} Filter for State and Parameter-Estimation},
  booktitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  author = {{Van der Merwe}, R. and Wan, E.A.},
  year = {2001},
  month = may,
  volume = {6},
  pages = {3461-3464 vol.6},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940586},
  abstract = {Over the last 20-30 years, the extended Kalman filter (EKF) has become the algorithm of choice in numerous nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system as well estimating parameters for nonlinear system identification (eg, learning the weights of a neural network). The EKF applies the standard linear Kalman filter methodology to a linearization of the true nonlinear system. This approach is sub-optimal, and can easily lead to divergence. Julier et al. (1997), proposed the unscented Kalman filter (UKF) as a derivative-free alternative to the extended Kalman filter in the framework of state estimation. This was extended to parameter estimation by Wan and Van der Merwe et al., (2000). The UKF consistently outperforms the EKF in terms of prediction and estimation error, at an equal computational complexity of (OL/sup 3/)/sup l/ for general state-space problems. When the EKF is applied to parameter estimation, the special form of the state-space equations allows for an O(L/sup 2/) implementation. This paper introduces the square-root unscented Kalman filter (SR-UKF) which is also O(L/sup 3/) for general state estimation and O(L/sup 2/) for parameter estimation (note the original formulation of the UKF for parameter-estimation was O(L/sup 3/)). In addition, the square-root forms have the added benefit of numerical stability and guaranteed positive semi-definiteness of the state covariances.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/I257B749/Van der Merwe and Wan - 2001 - The square-root unscented Kalman filter for state .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/8QAY2AYU/940586.html},
  keywords = {Computational complexity,covariance analysis,EKF,Equations,Estimation error,extended Kalman filter,filtering theory,Kalman filters,Machine learning,Machine learning algorithms,Neural networks,nonlinear dynamic system,nonlinear dynamical systems,Nonlinear dynamical systems,nonlinear estimation,nonlinear system identification,Nonlinear systems,numerical stability,parameter estimation,Parameter estimation,positive semi-definiteness,sqrt,square-root unscented Kalman filter,SR-UKF,state covariances,state estimation,State estimation}
}

@article{wanUnscentedKalmanFilter2001,
  title = {The {{Unscented Kalman Filter}}},
  author = {Wan, Eric A. and {Van der Merwe}, Rudolph and Haykin, Simon},
  year = {2001},
  volume = {5},
  pages = {221--280},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/ZLQKKV6L/Wan and Merwe - 2002 - The Unscented Kalman Filter.pdf},
  journal = {Wiley Online Library},
  number = {2007},
  series = {Kalman Filtering and Neural Networks}
}

@article{williamsDataDrivenApproximation2015,
  title = {A {{Data}}\textendash{{Driven Approximation}} of the {{Koopman Operator}}: {{Extending Dynamic Mode Decomposition}}},
  shorttitle = {A {{Data}}\textendash{{Driven Approximation}} of the {{Koopman Operator}}},
  author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
  year = {2015},
  month = dec,
  volume = {25},
  pages = {1307--1346},
  issn = {1432-1467},
  doi = {10.1007/s00332-015-9258-5},
  abstract = {The Koopman operator is a linear but infinite-dimensional operator that governs the evolution of scalar observables defined on the state space of an autonomous dynamical system and is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. In this manuscript, we present a data-driven method for approximating the leading eigenvalues, eigenfunctions, and modes of the Koopman operator. The method requires a data set of snapshot pairs and a dictionary of scalar observables, but does not require explicit governing equations or interaction with a ``black box'' integrator. We will show that this approach is, in effect, an extension of dynamic mode decomposition (DMD), which has been used to approximate the Koopman eigenvalues and modes. Furthermore, if the data provided to the method are generated by a Markov process instead of a deterministic dynamical system, the algorithm approximates the eigenfunctions of the Kolmogorov backward equation, which could be considered as the ``stochastic Koopman operator'' (Mezic in Nonlinear Dynamics 41(1\textendash 3): 309\textendash 325,~2005). Finally, four illustrative examples are presented: two that highlight the quantitative performance of the method when presented with either deterministic or stochastic data and two that show potential applications of the Koopman eigenfunctions.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MACAC5NG/Williams et al. - 2015 - A Data–Driven Approximation of the Koopman Operato.pdf},
  journal = {Journal of Nonlinear Science},
  language = {en},
  number = {6}
}

@article{williamsExtendingDataDrivenKoopman2016,
  title = {Extending {{Data}}-{{Driven Koopman Analysis}} to {{Actuated Systems}}},
  author = {Williams, Matthew O. and Hemati, Maziar S. and Dawson, Scott T. M. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
  year = {2016},
  month = jan,
  volume = {49},
  pages = {704--709},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2016.10.248},
  abstract = {In recent years, methods for data-driven Koopman spectral analysis, such as Dynamic Mode Decomposition (DMD), have become increasingly popular approaches for extracting dynamically relevant features from data sets. However to establish the connection between techniques like DMD or Extended DMD (EDMD) and the Koopman operator, assumptions are made about the nature of the supplied data. In particular, both methods assume the data were generated by an autonomous dynamical system, which can be limiting in certain experimental or computational settings, such as when system actuation is present. We present a modification of EDMD that overcomes this limitation by compensating for the effects of actuation, and is capable of recovering the leading Koopman eigenvalues, eigenfunctions, and modes of the unforced system. To highlight the efficacy of this approach, we apply it to two examples with (quasi)-periodic forcing: the first is the Duffing oscillator, which demonstrates eigenfunction approximation, and the second is a lattice Boltzmann code that approximates the FitzHugh-Nagumo partial differential equation and shows Koopman mode and eigenvalue computation.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/9KF7K547/Williams et al. - 2016 - Extending Data-Driven Koopman Analysis to Actuated.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/9FSGCVCR/S2405896316318286.html},
  journal = {IFAC-PapersOnLine},
  keywords = {data processing,Koopman operator,model reduction,nonlinear analysis,nonlinear theory,system identification},
  language = {en},
  number = {18},
  series = {10th {{IFAC Symposium}} on {{Nonlinear Control Systems NOLCOS}} 2016}
}

@article{williamsKernelBasedApproachDataDriven2015,
  title = {A {{Kernel}}-{{Based Approach}} to {{Data}}-{{Driven Koopman Spectral Analysis}}},
  author = {Williams, Matthew O. and Rowley, Clarence W. and Kevrekidis, Ioannis G.},
  year = {2015},
  month = jul,
  abstract = {A data driven, kernel-based method for approximating the leading Koopman eigenvalues, eigenfunctions, and modes in problems with high dimensional state spaces is presented. This approach approximates the Koopman operator using a set of scalar observables, which are functions defined on state space, that is determined \{\textbackslash em implicitly\} by the choice of a kernel. This circumvents the computational issues that arise due to the number of basis functions required to span a "sufficiently rich" subspace of the space of scalar observables in these problems. We illustrate this method on the FitzHugh-Nagumo PDE, a prototypical example of a one-dimensional reaction diffusion system, and compare our results with related methods such as Dynamic Mode Decomposition (DMD) that have the same computational cost as our approach. In this example, the resulting approximations of the leading Koopman eigenvalues, eigenfunctions, and modes are both more accurate and less sensitive to the distribution of the data used in the computation than those produced by DMD.},
  archivePrefix = {arXiv},
  eprint = {1411.2260},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/HVXQUQHZ/Williams et al. - 2015 - A Kernel-Based Approach to Data-Driven Koopman Spe.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/WDI2AFQP/1411.html},
  journal = {arXiv:1411.2260 [math]},
  keywords = {Mathematics - Dynamical Systems},
  primaryClass = {math}
}

@phdthesis{williamsonLearningNonLinearDynamical2020,
  title = {Learning {{Non}}-{{Linear Dynamical Systems}} with the {{Koopman Operator}}},
  author = {Williamson, Len Cewa},
  year = {2020},
  month = apr,
  address = {{Darmstadt, Germany}},
  abstract = {Transforming coordinates, that make non-linear dynamics approximately linear has the potential to enable non-linear prediction, estimation, and control using linear theory. The Koopman operator is a data-driven embedding of a dynamical system. The eigenfunctions of the Koopman operator globally linearize the dynamics. Approximating the eigenfunction remains an open question. This work leverages Fourier series to discover representations of Koopman eigenfunctions from data. Our introduced algorithm learns an embedding for the underlying dynamics while approximating the Koopman operator. We identify non-linear coordinates on which the dynamics are globally linear. Using line search we can recover the true trajectory of the dynamical system. We linearly approximate the simple pendulum, a dynamical system with a continuous spectrum.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4N7ICF5P/Williamson - 2020 - Learning Non-Linear Dynamical Systems with the Koo.pdf},
  language = {en},
  school = {TU Darmstadt},
  type = {Master's {{Thesis}}}
}

@inproceedings{yeungLearningDeepNeural2019a,
  title = {Learning {{Deep Neural Network Representations}} for {{Koopman Operators}} of {{Nonlinear Dynamical Systems}}},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Yeung, Enoch and Kundu, Soumya and Hodas, Nathan},
  year = {2019},
  month = jul,
  pages = {4832--4839},
  issn = {2378-5861},
  doi = {10.23919/ACC.2019.8815339},
  abstract = {The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a computational framework for learning Koopman operators of nonlinear dynamical systems using deep learning. We show that this novel method automatically selects efficient deep dictionaries, requiring much lower dimensional dictionaries while outperforming state-of-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict on test data quantitatively 100 steps into the future, using only a single timepoint as an initial condition, and quantitative oscillatory behavior 400 steps into the future.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FXYMB2MH/Yeung et al. - 2019 - Learning Deep Neural Network Representations for K.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AQNUXP6J/8815339.html;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XWCLGWYK/8815339.html},
  keywords = {biological networks,computational complexity,control engineering computing,cyber-physical infrastructure systems,data-driven model discovery,deep dictionaries,deep learning,deep neural network representations,domain-specific knowledge,dynamical systems analysis,extended dynamic mode decomposition,fluid dynamics,glycolytic oscillator,Koopman operator,Koopman operator learning,learning (artificial intelligence),neural nets,nonlinear dynamical systems,painstaking tuning,partially observed nonlinear systems,social systems}
}

@inproceedings{zhangSOLARDeepStructured2019a,
  title = {{{SOLAR}}: {{Deep Structured Representations}} for {{Model}}-{{Based Reinforcement Learning}}},
  shorttitle = {{{SOLAR}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zhang, Marvin and Vikram, Sharad and Smith, Laura and Abbeel, Pieter and Johnson, Matthew and Levine, Sergey},
  year = {2019},
  month = may,
  pages = {7444--7453},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In thi...},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/EDC3DWVC/Zhang et al. - 2019 - SOLAR Deep Structured Representations for Model-B.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AFMMMLZD/zhang19m.html},
  language = {en}
}


