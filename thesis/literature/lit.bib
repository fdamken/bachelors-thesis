
@book{abrahamManifoldsTensorAnalysis2012,
  title = {Manifolds, {{Tensor Analysis}}, and {{Applications}}},
  author = {Abraham, Ralph and Marsden, Jerrold E. and Ratiu, Tudor},
  year = {2012},
  month = dec,
  publisher = {{Springer Science \& Business Media}},
  abstract = {The purpose of this book is to provide core material in nonlinear analysis for mathematicians, physicists, engineers, and mathematical biologists. The main goal is to provide a working knowledge of manifolds, dynamical systems, tensors, and differential forms. Some applications to Hamiltonian mechanics, fluid me chanics, electromagnetism, plasma dynamics and control thcory arc given in Chapter 8, using both invariant and index notation. The current edition of the book does not deal with Riemannian geometry in much detail, and it does not treat Lie groups, principal bundles, or Morse theory. Some of this is planned for a subsequent edition. Meanwhile, the authors will make available to interested readers supplementary chapters on Lie Groups and Differential Topology and invite comments on the book's contents and development. Throughout the text supplementary topics are given, marked with the symbols \textasciitilde{} and \{l:;J. This device enables the reader to skip various topics without disturbing the main flow of the text. Some of these provide additional background material intended for completeness, to minimize the necessity of consulting too many outside references. We treat finite and infinite-dimensional manifolds simultaneously. This is partly for efficiency of exposition. Without advanced applications, using manifolds of mappings, the study of infinite-dimensional manifolds can be hard to motivate.\vphantom\}},
  isbn = {978-1-4612-1029-0},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / General,Mathematics / Calculus,Mathematics / Functional Analysis,Mathematics / Geometry / Analytic,Mathematics / Geometry / General,Mathematics / Mathematical Analysis,Mathematics / Topology,Science / Physics / Mathematical \& Computational,Science / System Theory},
  language = {en}
}

@inproceedings{andersonCommunicationAvoidingQRDecomposition2011a,
  title = {Communication-{{Avoiding QR Decomposition}} for {{GPUs}}},
  booktitle = {2011 {{IEEE International Parallel Distributed Processing Symposium}}},
  author = {Anderson, Michael and Ballard, Grey and Demmel, James and Keutzer, Kurt},
  year = {2011},
  month = may,
  pages = {48--58},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2011.15},
  abstract = {We describe an implementation of the Communication-Avoiding QR (CAQR) factorization that runs entirely on a single graphics processor (GPU). We show that the reduction in memory traffic provided by CAQR allows us to outperform existing parallel GPU implementations of QR for a large class of tall-skinny matrices. Other GPU implementations of QR handle panel factorizations by either sending the work to a general-purpose processor or using entirely bandwidth-bound operations, incurring data transfer overheads. In contrast, our QR is done entirely on the GPU using compute-bound kernels, meaning performance is good regardless of the width of the matrix. As a result, we outperform CULA, a parallel linear algebra library for GPUs by up to 17x for tall-skinny matrices and Intel's Math Kernel Library (MKL) by up to 12x. We also discuss stationary video background subtraction as a motivating application. We apply a recent statistical approach, which requires many iterations of computing the singular value decomposition of a tall-skinny matrix. Using CAQR as a first step to getting the singular value decomposition, we are able to get the answer 3x faster than if we use a traditional bandwidth-bound GPU QR factorization tuned specifically for that matrix size, and 30x faster than if we use Intel's Math Kernel Library (MKL) singular value decomposition routine on a multicore CPU.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/TWHMT39R/Anderson et al. - 2011 - Communication-Avoiding QR Decomposition for GPUs.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/SHP5S74T/6012824.html},
  keywords = {bandwidth-bound operations,communication-avoiding QR decomposition,communication-avoiding QR factorization,compute-bound kernels,computer graphic equipment,coprocessors,CULA parallel linear algebra library,data transfer overheads,general-purpose processor,GPU,graphics processing unit,Graphics processing unit,Instruction sets,Intel math kernel library,Kernel,Libraries,Matrix decomposition,principal component analysis,QR handle panel factorization,singular value decomposition,statistical approach,tall-skinny matrix,Vegetation,video background subtraction,video signal processing}
}

@article{arasaratnamCubatureKalmanFilters2009,
  title = {Cubature {{Kalman Filters}}},
  author = {Arasaratnam, Ienkaran and Haykin, Simon},
  year = {2009},
  month = jun,
  volume = {54},
  pages = {1254--1269},
  issn = {1558-2523},
  doi = {10.1109/TAC.2009.2019800},
  abstract = {In this paper, we present a new nonlinear filter for high-dimensional state estimation, which we have named the cubature Kalman filter (CKF). The heart of the CKF is a spherical-radial cubature rule, which makes it possible to numerically compute multivariate moment integrals encountered in the nonlinear Bayesian filter. Specifically, we derive a third-degree spherical-radial cubature rule that provides a set of cubature points scaling linearly with the state-vector dimension. The CKF may therefore provide a systematic solution for high-dimensional nonlinear filtering problems. The paper also includes the derivation of a square-root version of the CKF for improved numerical stability. The CKF is tested experimentally in two nonlinear state estimation problems. In the first problem, the proposed cubature rule is used to compute the second-order statistics of a nonlinearly transformed Gaussian random variable. The second problem addresses the use of the CKF for tracking a maneuvering aircraft. The results of both experiments demonstrate the improved performance of the CKF over conventional nonlinear filters.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KXRA7ESV/Arasaratnam and Haykin - 2009 - Cubature Kalman Filters.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/Z37QMZN4/4982682.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian filters,Bayesian methods,cubature Kalman filters,cubature points,cubature rules,Filtering,Gaussian processes,Gaussian quadrature rules,Gaussian random variable,Heart,high dimensional nonlinear filtering,high dimensional state estimation,invariant theory,Kalman filter,Kalman filters,maneuvering aircraft tracking,moment integrals,nonlinear Bayesian filter,nonlinear filtering,nonlinear filters,Nonlinear filters,nonlinear state estimation,numerical stability,Numerical stability,Random variables,second-order statistics,spherical-radial cubature rule,state estimation,State estimation,state vector dimension,Statistics,Testing},
  number = {6}
}

@article{bartoNeuronlikeAdaptiveElements1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  year = {1983},
  month = sep,
  volume = {SMC-13},
  pages = {834--846},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/EZKXPZ9K/Barto et al. - 1983 - Neuronlike adaptive elements that can solve diffic.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/9U5EUL8W/6313077.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {adaptive control,adaptive critic element,Adaptive systems,animal learning studies,associative search element,Biological neural networks,learning control problem,learning systems,movable cart,neural nets,neuronlike adaptive elements,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  number = {5}
}

@article{baumMaximizationTechniqueOccurring1970,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  volume = {41},
  pages = {164--171},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3IZ55TRL/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf},
  journal = {The Annals of Mathematical Statistics},
  number = {1}
}

@phdthesis{bealVariationalAlgorithmsApproximate2003,
  title = {Variational {{Algorithms}} for {{Approximate Bayesian Inference}}},
  author = {Beal, Matthew J.},
  year = {2003},
  month = may,
  address = {{London}},
  abstract = {The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. Unfortunately the computations required are usually intractable. This thesis presents a unified variational Bayesian (VB) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood. Chapter 1 presents background material on Bayesian inference, graphical models, and propagation algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation-maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM algorithm which integrates over model parameters. The algorithm is then specialised to the large family of conjugate-exponential (CE) graphical models, and several theorems are presented to pave the road for automated VB derivation procedures in both directed and undirected graphs (Bayesian and Markov networks, respectively). Chapters 3-5 derive and apply the VB EM algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models. It is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using VB approximations. Also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. Throughout, the VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/9WBWXIVY/Beal - 2003 - Variational Algorithms for Approximate Bayesian In.pdf},
  language = {en},
  school = {University of Cambridge},
  type = {{{PhD}}. {{Thesis}}}
}

@techreport{bealVariationalKalmanSmoother2000,
  title = {The {{Variational Kalman Smoother}}},
  author = {Beal, Matthew J. and Ghahramani, Zoubin},
  year = {2000},
  month = may,
  pages = {15},
  institution = {{Gatsby Computational Neuroscience Unit}},
  abstract = {In this note we outline the derivation of the variational Kalman smoother, in the context of Bayesian Linear Dynamical Systems. The smoother is an efficient algorithm for the E-step in the Expectation-Maximisation (EM) algorithm for linear-Gaussian state-space models. However, inference approximations are required if we hold distributions over parameters. We derive the E-step updates for the hidden states (the variational smoother), and the M-step updates for the parameter distributions. We shpw that inference of the hidden state is tractable for any distribution over parameters, provided the expectations of certain quantities available, analytically or otherwise.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/APDQSUZN/Beal and Ghahramani - 2000 - The Variational Kalman Smoother.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UZDVPPBQ/vbssm.tgz},
  language = {en}
}

@article{beckerRecurrentKalmanNetworks2019,
  title = {Recurrent {{Kalman Networks}}: {{Factorized Inference}} in {{High}}-{{Dimensional Deep Feature Spaces}}},
  shorttitle = {Recurrent {{Kalman Networks}}},
  author = {Becker, Philipp and Pandya, Harit and Gebhardt, Gregor and Zhao, Cheng and Taylor, James and Neumann, Gerhard},
  year = {2019},
  month = may,
  abstract = {In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models, however, such approaches typically rely on approximate inference techniques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time step. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter \& Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.},
  archivePrefix = {arXiv},
  eprint = {1905.07357},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/383LD3ZE/Becker et al. - 2019 - Recurrent Kalman Networks Factorized Inference in.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/SRU9EZ2Y/1905.html},
  journal = {arXiv:1905.07357 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bellmanDynamicProgramming1966,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard},
  year = {1966},
  month = jul,
  volume = {153},
  pages = {34--37},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.153.3731.34},
  abstract = {Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a "theory." What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology. The more we study the information processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them. In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?},
  chapter = {Articles},
  copyright = {\textcopyright{} 1966 by the American Association for the Advancement of Science},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/VMED5EJF/Bellman - 1966 - Dynamic Programming.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/YY4JK3DI/tab-pdf.html},
  journal = {Science},
  language = {en},
  number = {3731},
  pmid = {17730601}
}

@article{biermanSequentialSquareRoot1974a,
  title = {Sequential Square Root Filtering and Smoothing of Discrete Linear Systems},
  author = {Bierman, Gerald J.},
  year = {1974},
  month = mar,
  volume = {10},
  pages = {147--158},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(74)90020-X},
  abstract = {Square-root information estimation algorithms are immensely important estimation analysis tools that are not sufficiently well understood nor adequately exploited. In an endeavor to rectify this state of affairs an expository derivation of the square-root information filter/smoother is given. It is based on the recursive least-squares method and is easier to grasp, interpret and generalize than are the dynamic programming arguments previously used. Backward smoothing algorithms, both square-root and covariance recursions, are derived as direct and consequences of the method. A comparison of smoothing algorithms indicates that those presented in this paper are the most efficient. Partitioning the results to separate bias parameters provides further computational economies and reduction of storage requirements. The principal objective of this paper is to inspire greater utilization of square-root estimation algorithms. Arguments supporting this thesis are the new least-squares filter/smoother derivations, enhanced numerical accuracy, reduced computation, and lower storage requirements.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/YUWGRB8R/Bierman - 1974 - Sequential square root filtering and smoothing of .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XZBHHJW2/000510987490020X.html},
  journal = {Automatica},
  language = {en},
  number = {2}
}

@book{birkhoffDynamicalSystems1927,
  title = {Dynamical {{Systems}}},
  author = {Birkhoff, George David},
  year = {1927},
  month = dec,
  publisher = {{American Mathematical Soc.}},
  abstract = {His research in dynamics constitutes the middle period of Birkhoff\&\#39;s scientific career, that of maturity and greatest power. --Yearbook of the American Philosophical Society The author\&\#39;s great book ... is well known to all, and the diverse active modern developments in mathematics which have been inspired by this volume bear the most eloquent testimony to its quality and influence. --Zentralblatt MATH In 1927, G. D. Birkhoff wrote a remarkable treatise on the theory of dynamical systems that would inspire many later mathematicians to do great work. To a large extent, Birkhoff was writing about his own work on the subject, which was itself strongly influenced by Poincare\&\#39;s approach to dynamical systems. With this book, Birkhoff also demonstrated that the subject was a beautiful theory, much more than a compendium of individual results. The influence of this work can be found in many fields, including differential equations, mathematical physics, and even what is now known as Morse theory. The present volume is the revised 1966 reprinting of the book, including a new addendum, some footnotes, references added by Jurgen Moser, and a special preface by Marston Morse. Although dynamical systems has thrived in the decades since Birkhoff\&\#39;s book was published, this treatise continues to offer insight and inspiration for still more generations of mathematicians.},
  googlebooks = {ygmWAwAAQBAJ},
  isbn = {978-0-8218-1009-5},
  keywords = {Mathematics / Geometry / Differential},
  language = {en}
}

@article{bruntonKoopmanInvariantSubspaces2016,
  title = {Koopman {{Invariant Subspaces}} and {{Finite Linear Representations}} of {{Nonlinear Dynamical Systems}} for {{Control}}},
  author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kutz, J. Nathan},
  year = {2016},
  month = feb,
  volume = {11},
  pages = {e0150171},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0150171},
  abstract = {In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by {$\mathscr{l}$}1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/L4YWR84B/Brunton et al. - 2016 - Koopman Invariant Subspaces and Finite Linear Repr.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/R4MFTEA7/article.html},
  journal = {PLOS ONE},
  keywords = {Algorithms,Dynamical systems,Eigenvalues,Eigenvectors,Fluid dynamics,Nonlinear dynamics,Nonlinear systems,Polynomials},
  language = {en},
  number = {2}
}

@inproceedings{bugejaNonlinearSwingupStabilizing2003,
  title = {Non-Linear Swing-up and Stabilizing Control of an Inverted Pendulum System},
  booktitle = {The {{IEEE Region}} 8 {{EUROCON}} 2003. {{Computer}} as a {{Tool}}.},
  author = {Bugeja, M.},
  year = {2003},
  month = sep,
  volume = {2},
  pages = {437-441 vol.2},
  doi = {10.1109/EURCON.2003.1248235},
  abstract = {This paper presents the design and implementation of a complete control system for the swing-up and stabilizing control of an inverted pendulum. In particular, this work outlines the effectiveness of a particular swing-up method, based on feedback linearization and energy considerations. The power of modern state-space techniques for the analysis and control of Multiple Input Multiple Output (MIMO) systems is also investigated and a state-feedback controller is employed for stabilizing the pendulum. Cascade control is then utilized to reduce the complexity of the complete controller by splitting it into two separate control loops operating at well distinct bandwidths.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/VAY9LGLQ/Bugeja - 2003 - Non-linear swing-up and stabilizing control of an .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/3CJM6EP3/1248235.html},
  keywords = {cascade control,control loops,Control systems,Control theory,Design engineering,feedback,feedback linearization,inverted pendulum,Linear feedback control systems,Mechanical systems,Modems,multiple input multiple output,non-linear swing-up control,nonlinear control systems,Nonlinear control systems,pendulums,Power engineering and energy,Rails,stability,stabilization,state-feedback control,state-space,state-space methods,System testing}
}

@article{ceppelliniEstimationGeneFrequencies1955,
  title = {The {{Estimation}} of {{Gene Frequencies}} in a {{Random}}-{{Mating Population}}},
  author = {Ceppellini, By R. and Siniscalco, M. and Smith, C. a. B.},
  year = {1955},
  volume = {20},
  pages = {97--115},
  issn = {1469-1809},
  doi = {10.1111/j.1469-1809.1955.tb01360.x},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1955.tb01360.x},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/GUHENABM/j.1469-1809.1955.tb01360.html},
  journal = {Annals of Human Genetics},
  language = {en},
  number = {2}
}

@article{deanRobustGuaranteesPerceptionBased2019,
  title = {Robust {{Guarantees}} for {{Perception}}-{{Based Control}}},
  author = {Dean, Sarah and Matni, Nikolai and Recht, Benjamin and Ye, Vickie},
  year = {2019},
  month = dec,
  abstract = {Motivated by vision-based control of autonomous vehicles, we consider the problem of controlling a known linear dynamical system for which partial state information, such as vehicle position, is extracted from complex and nonlinear data, such as a camera image. Our approach is to use a learned perception map that predicts some linear function of the state and to design a corresponding safe set and robust controller for the closed loop system with this sensing scheme. We show that under suitable smoothness assumptions on both the perception map and the generative model relating state to complex and nonlinear data, parameters of the safe set can be learned via appropriately dense sampling of the state space. We then prove that the resulting perception-control loop has favorable generalization properties. We illustrate the usefulness of our approach on a synthetic example and on the self-driving car simulation platform CARLA.},
  archivePrefix = {arXiv},
  eprint = {1907.03680},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QBJRRF8R/Dean et al. - 2019 - Robust Guarantees for Perception-Based Control.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XZ74IRW8/1907.html},
  journal = {arXiv:1907.03680 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{deisenrothProbabilisticPerspectiveGaussian2011,
  title = {A {{Probabilistic Perspective}} on {{Gaussian Filtering}} and {{Smoothing}}},
  author = {Deisenroth, Marc Peter and Ohlsson, Henrik},
  year = {2011},
  month = jun,
  abstract = {We present a general probabilistic perspective on Gaussian filtering and smoothing. This allows us to show that common approaches to Gaussian filtering/smoothing can be distinguished solely by their methods of computing/approximating the means and covariances of joint probabilities. This implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments. Based on this insight, we derive the cubature Kalman smoother and propose a novel robust filtering and smoothing algorithm based on Gibbs sampling.},
  archivePrefix = {arXiv},
  eprint = {1006.2165},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/SZAL384A/Deisenroth and Ohlsson - 2011 - A Probabilistic Perspective on Gaussian Filtering .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/EGK5Y7DI/1006.html},
  journal = {arXiv:1006.2165 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, math, stat}
}

@article{dempsterMaximumLikelihoodIncomplete1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  volume = {39},
  pages = {1--22},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
  copyright = {\textcopyright{} 1977 The Authors},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KXGHYPKV/j.2517-6161.1977.tb01600.html},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
  language = {en},
  number = {1}
}

@article{depierroModifiedExpectationMaximization1995,
  title = {A Modified Expectation Maximization Algorithm for Penalized Likelihood Estimation in Emission Tomography},
  author = {De Pierro, A.R.},
  year = {1995},
  month = mar,
  volume = {14},
  pages = {132--137},
  issn = {1558-254X},
  doi = {10.1109/42.370409},
  abstract = {The maximum likelihood (ML) expectation maximization (EM) approach in emission tomography has been very popular in medical imaging for several years. In spite of this, no satisfactory convergent modifications have been proposed for the regularized approach. Here, a modification of the EM algorithm is presented. The new method is a natural extension of the EM for maximizing likelihood with concave priors. Convergence proofs are given.{$<>$}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4HT4SEGT/De Pierro - 1995 - A modified expectation maximization algorithm for .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/P75TPH55/370409.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {Biomedical imaging,Computed tomography,concave priors,Convergence,convergence proofs,diagnostic nuclear medicine,Electrical capacitance tomography,emission tomography,image reconstruction,Image reconstruction,Isotopes,Maximum likelihood detection,Maximum likelihood estimation,medical diagnostic imaging,medical image processing,modified expectation maximization algorithm,penalized likelihood estimation,Positron emission tomography,regularized approach,Single photon emission computed tomography},
  number = {1}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  issn = {1533-7928},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/BT6LLCJH/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HHB9TDLA/duchi11a.html},
  journal = {Journal of Machine Learning Research},
  number = {61}
}

@book{elhadjDynamicalSystemsTheories2019,
  title = {Dynamical {{Systems}}: {{Theories}} and {{Applications}}},
  shorttitle = {Dynamical {{Systems}}},
  author = {Elhadj, Zeraoulia},
  year = {2019},
  month = jan,
  publisher = {{CRC Press}},
  abstract = {Chaos is the idea that a system will produce very different long-term behaviors when the initial conditions are perturbed only slightly. Chaos is used for novel, time- or energy-critical interdisciplinary applications. Examples include high-performance circuits and devices, liquid mixing, chemical reactions, biological systems, crisis management, secure information processing, and critical decision-making in politics, economics, as well as military applications, etc. This book presents the latest investigations in the theory of chaotic systems and their dynamics. The book covers some theoretical aspects of the subject arising in the study of both discrete and continuous-time chaotic dynamical systems. This book presents the state-of-the-art of the more advanced studies of chaotic dynamical systems.},
  googlebooks = {mFupDwAAQBAJ},
  isbn = {978-0-429-65006-2},
  keywords = {Mathematics / Arithmetic,Mathematics / Differential Equations / General,Science / Life Sciences / General,Science / Physics / Mathematical \& Computational},
  language = {en}
}

@article{fesslerSpacealternatingGeneralizedExpectationmaximization1994,
  title = {Space-Alternating Generalized Expectation-Maximization Algorithm},
  author = {Fessler, J.A. and Hero, A.O.},
  year = {1994},
  month = oct,
  volume = {42},
  pages = {2664--2677},
  issn = {1941-0476},
  doi = {10.1109/78.324732},
  abstract = {The expectation-maximization (EM) method can facilitate maximizing likelihood functions that arise in statistical estimation problems. In the classical EM paradigm, one iteratively maximizes the conditional log-likelihood of a single unobservable complete data space, rather than maximizing the intractable likelihood function for the measured or incomplete data. EM algorithms update all parameters simultaneously, which has two drawbacks: 1) slow convergence, and 2) difficult maximization steps due to coupling when smoothness penalties are used. The paper describes the space-alternating generalized EM (SAGE) method, which updates the parameters sequentially by alternating between several small hidden-data spaces defined by the algorithm designer. The authors prove that the sequence of estimates monotonically increases the penalized-likelihood objective, derive asymptotic convergence rates, and provide sufficient conditions for monotone convergence in norm. Two signal processing applications illustrate the method: estimation of superimposed signals in Gaussian noise, and image reconstruction from Poisson measurements. In both applications, the SAGE algorithms easily accommodate smoothness penalties and converge faster than the EM algorithms.{$<>$}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IB6LW2P7/Fessler and Hero - 1994 - Space-alternating generalized expectation-maximiza.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/7I3KD9JZ/324732.html},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {Algorithm design and analysis,asymptotic convergence rates,classical EM paradigm,conditional log-likelihood,Convergence,convergence of numerical methods,Expectation-maximization algorithms,Extraterrestrial measurements,Gaussian noise,image reconstruction,Image reconstruction,intractable likelihood function,Iterative algorithms,iterative methods,maximization steps,maximum likelihood estimation,monotone convergence,parameter estimation,penalized-likelihood objective,Poisson measurements,random noise,SAGE method,sequence of estimates,signal processing,Signal processing,Signal processing algorithms,signal processing applications,small hidden-data spaces,smoothness penalties,space-alternating generalized expectation-maximization algorithm,statistical analysis,statistical estimation problems,stochastic processes,sufficient conditions,Sufficient conditions,superimposed signals,unobservable complete data space},
  number = {10}
}

@techreport{ghahramaniParameterEstimationLinear1996,
  title = {Parameter Estimation for Linear Dynamical Systems},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {Februrary 22, 1996},
  institution = {{University of Toronto}},
  abstract = {Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Stoffer, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RHES3QAG/Ghahramani and Hinton - 1996 - Parameter estimation for linear dynamical systems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/V49S4PIV/lds.tar.gz},
  number = {CRG-TR-96-2}
}

@article{gibsonRobustMaximumlikelihoodEstimation2005,
  title = {Robust Maximum-Likelihood Estimation of Multivariable Dynamic Systems},
  author = {Gibson, Stuart and Ninness, Brett},
  year = {2005},
  month = oct,
  volume = {41},
  pages = {1667--1682},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2005.05.008},
  abstract = {This paper examines the problem of estimating linear time-invariant state-space system models. In particular, it addresses the parametrization and numerical robustness concerns that arise in the multivariable case. These difficulties are well recognised in the literature, resulting (for example) in extensive study of subspace-based techniques, as well as recent interest in `data driven' local co-ordinate approaches to gradient search solutions. The paper here proposes a different strategy that employs the expectation\textendash maximisation (EM) technique. The consequence is an algorithm that is iterative, with associated likelihood values that are locally convergent to stationary points of the (Gaussian) likelihood function. Furthermore, theoretical and empirical evidence presented here establishes additional attractive properties such as numerical robustness, avoidance of difficult parametrization choices, the ability to naturally and easily estimate non-zero initial conditions, and moderate computational cost. Moreover, since the methods here are maximum-likelihood based, they have associated known and asymptotically optimal statistical properties.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/2ZV49YZM/Gibson and Ninness - 2005 - Robust maximum-likelihood estimation of multivaria.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XIYEQI7L/S0005109805001810.html},
  journal = {Automatica},
  keywords = {Parameter estimation,System identification},
  language = {en},
  number = {10}
}

@book{hairerSolvingOrdinaryDifferential1996,
  title = {Solving {{Ordinary Differential Equations II}}. {{Stiff}} and {{Differential}}-{{Algebraic Problems}}},
  author = {Hairer, Ernst and Wanner, G.},
  year = {1996},
  month = jan,
  volume = {14},
  doi = {10.1007/978-3-662-09947-6},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/EI3I4VUA/Hairer and Wanner - 1996 - Solving Ordinary Differential Equations II. Stiff .pdf},
  journal = {Springer Verlag Series in Comput. Math.}
}

@article{itoGaussianFiltersNonlinear2000,
  title = {Gaussian Filters for Nonlinear Filtering Problems},
  author = {Ito, K. and Xiong, K.},
  year = {2000},
  month = may,
  volume = {45},
  pages = {910--927},
  issn = {1558-2523},
  doi = {10.1109/9.855552},
  abstract = {We develop and analyze real-time and accurate filters for nonlinear filtering problems based on the Gaussian distributions. We present the systematic formulation of Gaussian filters and develop efficient and accurate numerical integration of the optimal filter. We also discuss the mixed Gaussian filters in which the conditional probability density is approximated by the sum of Gaussian distributions. A new update rule of weights for Gaussian sum filters is proposed. Our numerical tests demonstrate that new filters significantly improve the extended Kalman filter with no additional cost, and the new Gaussian sum filter has a nearly optimal performance.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MSWJ85KN/Ito and Xiong - 2000 - Gaussian filters for nonlinear filtering problems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/A4S4RVGQ/855552.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian methods,Cost function,Filtering,filtering theory,Filters,Gaussian distribution,Gaussian distributions,Gaussian filters,Gaussian processes,Indium tin oxide,Kalman filter,Kalman filters,nonlinear filtering,probability density,Sonar navigation,Testing},
  number = {5}
}

@article{jensenFonctionsConvexesInegalites1906,
  title = {{Sur les fonctions convexes et les in\'egalit\'es entre les valeurs moyennes}},
  author = {Jensen, J. L. W. V.},
  year = {1906},
  volume = {30},
  pages = {175--193},
  publisher = {{Institut Mittag-Leffler}},
  issn = {0001-5962, 1871-2509},
  doi = {10.1007/BF02418571},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4G6N6HBP/Jensen - 1906 - Sur les fonctions convexes et les inégalités entre.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5MWXFHEI/1485887155.html},
  journal = {Acta Mathematica},
  language = {FR},
  mrnumber = {MR1555027},
  zmnumber = {0087.27403}
}

@article{kaiserDatadrivenDiscoveryKoopman2020a,
  title = {Data-Driven Discovery of {{Koopman}} Eigenfunctions for Control},
  author = {Kaiser, Eurika and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2020},
  month = may,
  abstract = {Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to represent the dynamics directly in eigenfunction coordinates. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control is formulated in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including 1) a nonlinear system with a known linear embedding, 2) a variety of Hamiltonian systems, and 3) a high-dimensional double-gyre model for ocean mixing.},
  archivePrefix = {arXiv},
  eprint = {1707.01146},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/Y9HAHYJN/Kaiser et al. - 2020 - Data-driven discovery of Koopman eigenfunctions fo.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/N4K6NUEV/1707.html},
  journal = {arXiv:1707.01146 [math]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control},
  primaryClass = {math}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3QLZVKLI/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HEHEXW8E/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/XNWXKDKZ/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/VMLNAGA4/1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{koopmanHamiltonianSystemsTransformation1931a,
  title = {Hamiltonian {{Systems}} and {{Transformation}} in {{Hilbert Space}}},
  author = {Koopman, B. O.},
  year = {1931},
  month = may,
  volume = {17},
  pages = {315--318},
  issn = {0027-8424},
  abstract = {In recent years the theory of Hilbert space and its linear transformations has come into prominence. It has been recognized to an increasing extent that many of the most important departments of mathematical physics can be subsumed under this theory. In classical physics, for example in those phenomena which are governed by linear conditions \textendash{} linear differential or integral equations and the like, in those relating to harmonic analysis, and in many phenomena due to the opetation of the laws of chance, the essential role is played by certain linear transformations in Hilbert space. And the importance of the theory in quantum mechanics is known to all. It is the object of this note to outline certain investigations of our own in which the domain of this theory has been extended in such a way as to include classical Hamiltonian mechanics, or, more generally, systems defining a stead n-dimensional flow of a fluid of positive density.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/U7CXQKGD/Koopman - 1931 - Hamiltonian Systems and Transformation in Hilbert .pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {5},
  pmcid = {PMC1076052},
  pmid = {16577368}
}

@article{luschDeepLearningUniversal2018,
  title = {Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics},
  author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {4950},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07210-0},
  abstract = {Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.},
  archivePrefix = {arXiv},
  eprint = {1712.09707},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RRYUXGIZ/Lusch et al. - 2018 - Deep learning for universal linear embeddings of n.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/LS6VRGYR/1712.html},
  journal = {Nature Communications},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  number = {1}
}

@inproceedings{maOptimalQuadraticRegulation2019,
  title = {Optimal {{Quadratic Regulation}} of {{Nonlinear System Using Koopman Operator}}},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Ma, Xu and Huang, Bowen and Vaidya, Umesh},
  year = {2019},
  month = jul,
  pages = {4911--4916},
  issn = {2378-5861},
  doi = {10.23919/ACC.2019.8814903},
  abstract = {In this paper, we study the optimal quadratic regulation problem for nonlinear systems. The linear operator theoretic framework involving the Koopman operator is used to lift the dynamics of nonlinear control system to an infinite dimensional bilinear system. Optimal quadratic regulation problem for nonlinear system is formulated in terms of the finite dimensional approximation of the bilinear system. A convex optimization-based approach is proposed for solving the quadratic regulator problem for bilinear system. Simulation results are presented to demonstrate the application of the developed framework.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/SPTN6ZG4/Ma et al. - 2019 - Optimal Quadratic Regulation of Nonlinear System U.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HA9GRSPU/8814903.html},
  keywords = {Aerospace electronics,bilinear systems,convex optimization,convex programming,Eigenvalues and eigenfunctions,infinite dimensional bilinear system,Koopman operator,linear operator theoretic framework,linear quadratic control,mathematical operators,multidimensional systems,nonlinear control system,nonlinear control systems,Nonlinear dynamical systems,nonlinear system,optimal control,Optimal control,optimal quadratic regulation problem,optimisation,quadratic regulator problem,Regulation}
}

@book{mauroyKoopmanOperatorSystems2020,
  title = {The {{Koopman Operator}} in {{Systems}} and {{Control}}: {{Concepts}}, {{Methodologies}}, and {{Applications}}},
  shorttitle = {The {{Koopman Operator}} in {{Systems}} and {{Control}}},
  editor = {Mauroy, Alexandre and Mezic, Igor and Susuki, Yoshihiko},
  year = {2020},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-35713-9},
  abstract = {This book provides a broad overview of state-of-the-art research at the intersection of the Koopman operator theory and control theory. It also reviews novel theoretical results obtained and efficient numerical methods developed within the framework of Koopman operator theory.The contributions discuss the latest findings and techniques in several areas of control theory, including model predictive control, optimal control, observer design, systems identification and structural analysis of controlled systems, addressing both theoretical and numerical aspects and presenting open research directions, as well as detailed numerical schemes and data-driven methods. Each contribution addresses a specific problem. After a brief introduction of the Koopman operator framework, including basic notions and definitions, the book explores numerical methods, such as the dynamic mode decomposition (DMD) algorithm and Arnoldi-based methods, which are used to represent the operator in a finite-dimensional basis and to compute its spectral properties from data. The main body of the book is divided into three parts:theoretical results and numerical techniques for observer design, synthesis analysis, stability analysis, parameter estimation, and identification;data-driven techniques based on DMD, which extract the spectral properties of the Koopman operator from data for the structural analysis of controlled systems; andKoopman operator techniques with specific applications in systems and control, which range from heat transfer analysis to robot control.A useful reference resource on the Koopman operator theory for control theorists and practitioners, the book is also of interest to graduate students, researchers, and engineers looking for an introduction to a novel and comprehensive approach to systems and control, from pure theory to data-driven methods.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/LP9ANR5W/9783030357122.html},
  isbn = {978-3-030-35712-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}}
}

@article{mezicSpectralPropertiesDynamical2005,
  title = {Spectral {{Properties}} of {{Dynamical Systems}}, {{Model Reduction}} and {{Decompositions}}},
  author = {Mezi{\'c}, Igor},
  year = {2005},
  month = aug,
  volume = {41},
  pages = {309--325},
  issn = {1573-269X},
  doi = {10.1007/s11071-005-2824-x},
  abstract = {In this paper we discuss two issues related to model reduction of deterministic or stochastic processes. The first is the relationship of the spectral properties of the dynamics on the attractor of the original, high-dimensional dynamical system with the properties and possibilities for model reduction. We review some elements of the spectral theory of dynamical systems. We apply this theory to obtain a decomposition of the process that utilizes spectral properties of the linear Koopman operator associated with the asymptotic dynamics on the attractor. This allows us to extract the almost periodic part of the evolving process. The remainder of the process has continuous spectrum. The second topic we discuss is that of model validation, where the original, possibly high-dimensional dynamics and the dynamics of the reduced model \textendash{} that can be deterministic or stochastic \textendash{} are compared in some norm. Using the ``statistical Takens theorem'' proven in (Mezi\'c, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of average energy contained in the finite-dimensional projection is one in the hierarchy of functionals of the field that need to be checked in order to assess the accuracy of the projection.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3Q35VGPG/Mezić - 2005 - Spectral Properties of Dynamical Systems, Model Re.pdf},
  journal = {Nonlinear Dynamics},
  language = {en},
  number = {1}
}

@techreport{minkaBayesianLinearRegression1999,
  title = {Bayesian {{Linear Regression}}},
  author = {Minka, Thomas P.},
  year = {1999},
  institution = {{3594 Security Ticket Control}},
  abstract = {This note derives the posterior, evidence, and predictive density for linear multivariate regression under zero-mean Gaussian noise. Many Bayesian texts, such as Box \& Tiao (1973), cover linear regression. This note contributes to the discussion by paying careful attention to invariance issues, demonstrating model selection based on the evidence, and illustrating the shape of the predictive density. Piecewise regression and basis function regression are also discussed. 1 Introduction The data model is that an input vector x of length m multiplies a coefficient matrix A to produce an output vector y of length d, with Gaussian noise added:  y = Ax + e (1)  e N (0; V) (2)  p(yjx; A;V) N (Ax; V) (3) This is a conditional model for y only: the distribution of x is not needed and in fact irrelevant to all inferences in this paper. As we shall see, conditional models create subtleties in Bayesian inference. In the special case x = 1 and m = 1, the conditioning disappears and we simply have a ...},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QSGJ23H4/Minka - 1999 - Bayesian Linear Regression.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/T4PYCVH9/summary.html}
}

@techreport{minkaHiddenMarkovModels1999,
  title = {From {{Hidden Markov Models}} to {{Linear Dynamical Systems}}},
  author = {Minka, Thomas P.},
  year = {1999},
  month = jul,
  abstract = {Hidden Markov Models (HMMs) and Linear Dynamical Systems (LDSs) ate based on the same assumption: a hidden state variable, of which we can make noisy measurements, evolves with Markovian dynamics. Both have the same independency diagram and consequently the learning and inference algorithms for both have the same structure. The only difference is that the HMM uses a discrete state variable with arbitrary dynamics and arbitrary measurements while the LDS uses a continuous state variable with linear-Gaussian dynamics and measurements. We show how the forward-backward equations for the HMM, specialized to linear-Gaussian assumptions, lead directly to Kalman filtering and Rauch-Tung-Streibel smoothing. We also investigate the most general possible modeling assumptions which can lead to efficient recursion in the case of continuous state variables.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3Y5XFY3M/Minka - 1999 - From Hidden Markov Models to Linear Dynamical Syst.pdf},
  number = {TR-531}
}

@article{moonExpectationmaximizationAlgorithm1996,
  title = {The Expectation-Maximization Algorithm},
  author = {Moon, T.K.},
  year = {1996},
  month = nov,
  volume = {13},
  pages = {47--60},
  issn = {1558-0792},
  doi = {10.1109/79.543975},
  abstract = {A common task in signal processing is the estimation of the parameters of a probability distribution function. Perhaps the most frequently encountered estimation problem is the estimation of the mean of a signal in noise. In many parameter estimation problems the situation is more complicated because direct access to the data necessary to estimate the parameters is impossible, or some of the data are missing. Such difficulties arise when an outcome is a result of an accumulation of simpler outcomes, or when outcomes are clumped together, for example, in a binning or histogram operation. There may also be data dropouts or clustering in such a way that the number of underlying data points is unknown (censoring and/or truncation). The EM (expectation-maximization) algorithm is ideally suited to problems of this sort, in that it produces maximum-likelihood (ML) estimates of parameters when there is a many-to-one mapping from an underlying distribution to the distribution governing the observation. The EM algorithm is presented at a level suitable for signal processing practitioners who have had some exposure to estimation theory.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QQ2VZ39Q/Moon - 1996 - The expectation-maximization algorithm.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/P54PZ9N2/543975.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {binning,censoring,Convergence,data clustering,data dropouts,EM algorithm,estimation theory,Estimation theory,expectation-maximization algorithm,Hidden Markov models,histogram,Histograms,Image reconstruction,maximum likelihood estimation,Maximum likelihood estimation,maximum-likelihood estimates,mean,noise,parameter estimation,Parameter estimation,Phase detection,probability,Probability distribution,probability distribution function,signal processing,Signal processing algorithms,truncation},
  number = {6}
}

@article{mortonDeepVariationalKoopman2019,
  title = {Deep {{Variational Koopman Models}}: {{Inferring Koopman Observations}} for {{Uncertainty}}-{{Aware Dynamics Modeling}} and {{Control}}},
  shorttitle = {Deep {{Variational Koopman Models}}},
  author = {Morton, Jeremy and Witherden, Freddie D. and Kochenderfer, Mykel J.},
  year = {2019},
  month = jun,
  abstract = {Koopman theory asserts that a nonlinear dynamical system can be mapped to a linear system, where the Koopman operator advances observations of the state forward in time. However, the observable functions that map states to observations are generally unknown. We introduce the Deep Variational Koopman (DVK) model, a method for inferring distributions over observations that can be propagated linearly in time. By sampling from the inferred distributions, we obtain a distribution over dynamical models, which in turn provides a distribution over possible outcomes as a modeled system advances in time. Experiments show that the DVK model is effective at long-term prediction for a variety of dynamical systems. Furthermore, we describe how to incorporate the learned models into a control framework, and demonstrate that accounting for the uncertainty present in the distribution over dynamical models enables more effective control.},
  archivePrefix = {arXiv},
  eprint = {1902.09742},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/5IU4LHDL/Morton et al. - 2019 - Deep Variational Koopman Models Inferring Koopman.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AEEQ9MTN/1902.html},
  journal = {arXiv:1902.09742 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{paisleyVariationalBayesianInference2012a,
  title = {Variational {{Bayesian Inference}} with {{Stochastic Search}}},
  author = {Paisley, John and Blei, David and Jordan, Michael},
  year = {2012},
  month = jun,
  abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
  archivePrefix = {arXiv},
  eprint = {1206.6430},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/LBD3YVM8/Paisley et al. - 2012 - Variational Bayesian Inference with Stochastic Sea.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/2Z9Z7ZLE/1206.html},
  journal = {arXiv:1206.6430 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8026--8037},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/NKBF8PL6/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5VVE4H5D/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{rauchMaximumLikelihoodEstimates1965,
  title = {Maximum Likelihood Estimates of Linear Dynamic Systems},
  author = {RAUCH, H. E. and TUNG, F. and STRIEBEL, C. T.},
  year = {1965},
  volume = {3},
  pages = {1445--1450},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  issn = {0001-1452},
  doi = {10.2514/3.3166},
  annotation = {\_eprint: https://doi.org/10.2514/3.3166},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FJPDZRFJ/RAUCH et al. - 1965 - Maximum likelihood estimates of linear dynamic sys.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UBE6VQV3/3.html},
  journal = {AIAA Journal},
  number = {8}
}

@inproceedings{ruttenSquarerootUnscentedFiltering2013,
  title = {Square-Root Unscented Filtering and Smoothing},
  booktitle = {2013 {{IEEE Eighth International Conference}} on {{Intelligent Sensors}}, {{Sensor Networks}} and {{Information Processing}}},
  author = {Rutten, Mark G.},
  year = {2013},
  month = apr,
  pages = {294--299},
  doi = {10.1109/ISSNIP.2013.6529805},
  abstract = {A square-root Kalman filter propagates the square-root (often the Cholesky factor) of the state covariance, rather than the full covariance matrix. Propagating these factors offers both computational efficiencies and greatly improved numerical properties. This paper introduces a new method of implementing the square-root unscented filter and the square-root unscented Rauch-Tung-Striebel smoother, which provide similar computational and numerical advantages over their traditional implementations. The new algorithms rely on the QR factorisation for calculating the covariance square-roots. A comparison with the previous development of the square-root unscented filter shows similar computational cost, while dramatically simplifying the implementation and improving numerical stability.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/C3KCE5E8/Rutten - 2013 - Square-root unscented filtering and smoothing.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/FJVC6S25/6529805.html},
  keywords = {Cholesky factor,Computational efficiency,covariance matrices,Covariance matrices,covariance matrix,covariance square-roots,Kalman filters,Mathematical model,Matrix decomposition,QR factorisation,smoothing methods,Smoothing methods,sqrt,square-root Kalman filter,square-root unscented filtering,square-root unscented Rauch-Tung-Striebel smoother,square-root unscented smoothing,state covariance,Time measurement}
}

@article{schonSystemIdentificationNonlinear2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  volume = {47},
  pages = {39--49},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.10.013},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provides arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MPU6JLFX/Schön et al. - 2011 - System identification of nonlinear state-space mod.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/22NXXPUY/S0005109810004279.html},
  journal = {Automatica},
  keywords = {Dynamic systems,Expectation maximisation algorithm,Monte Carlo method,Nonlinear models,Particle methods,Smoothing filters,System identification},
  language = {en},
  number = {1}
}

@article{shannonCommunicationPresenceNoise1949,
  title = {Communication in the {{Presence}} of {{Noise}}},
  author = {Shannon, C.E.},
  year = {1949},
  month = jan,
  volume = {37},
  pages = {10--21},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1949.232969},
  abstract = {A method is developed for representing any communication system geometrically. Messages and the corresponding signals are points in two "function spaces," and the modulation process is a mapping of one space into the other. Using this representation, a number of results in communication theory are deduced concerning expansion and compression of bandwidth and the threshold effect. Formulas are found for the maxmum rate of transmission of binary digits over a system when the signal is perturbed by various types of noise. Some of the properties of "ideal" systems which transmit at this maxmum rate are discussed. The equivalent number of binary digits per second for certain information sources is calculated.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3TND3SGW/Shannon - 1949 - Communication in the Presence of Noise.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/42RI2TYP/1697831.html},
  journal = {Proceedings of the IRE},
  keywords = {Bandwidth,Circuits,Communication systems,Electron tubes,Frequency measurement,Gain measurement,Klystrons,Shape,Telephony,Voltage},
  number = {1}
}

@article{shumwayApproachTimeSeries1982,
  title = {An {{Approach}} to {{Time Series Smoothing}} and {{Forecasting Using}} the {{Em Algorithm}}},
  author = {Shumway, R. H. and Stoffer, D. S.},
  year = {1982},
  volume = {3},
  pages = {253--264},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1982.tb00349.x},
  abstract = {Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.1982.tb00349.x},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/8D3FI89A/Shumway and Stoffer - 1982 - An Approach to Time Series Smoothing and Forecasti.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/NIT2IV25/full-text_ocr.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/U56SZFGC/full-text.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/IC35UB4V/j.1467-9892.1982.tb00349.html},
  journal = {Journal of Time Series Analysis},
  keywords = {EM algorithm,forecasting,Kalman filter,maximum likelihood,Missing data},
  language = {en},
  number = {4}
}

@phdthesis{solinCubatureIntegrationMethods2010,
  title = {Cubature {{Integration Methods}} in {{Non}}-{{Linear Kalman Filtering}} and {{Smoothing}}},
  author = {Solin, Arno},
  year = {2010},
  abstract = {Optimal estimation problems arise in various different settings where indirect noisy observations are used to determine the underlying state of a time-varying system. For systems with non-linear dynamics there exist various methods that extend linear filtering and smoothing methods to handle non-linearities. In this thesis the non-linear optimal estimation framework is presented with the help of an assumed density approach. The Gaussian integrals that arise in this setting are solved using two different cubature integration methods. Cubature integration extends the weighted sum approach from univariate quadrature methods to multidimensional cubature methods. In this thesis the focus is put on two methods that use deterministically chosen sigma points to form the desired approximation. The Gauss\textendash Hermite rule uses a simple product rule method to fill the multidimensional space with cubature points, whereas the spherical\textendash radial rule uses invariant theory to diminish the number of points by utilizing symmetries. The},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3ATM3DZ6/Solin - 2010 - Cubature Integration Methods in Non-Linear Kalman .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HQDQY8HG/summary.html},
  type = {Bachelor's {{Thesis}}}
}

@inproceedings{vandermerweSquarerootUnscentedKalman2001,
  title = {The Square-Root Unscented {{Kalman}} Filter for State and Parameter-Estimation},
  booktitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  author = {{Van der Merwe}, R. and Wan, E.A.},
  year = {2001},
  month = may,
  volume = {6},
  pages = {3461-3464 vol.6},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940586},
  abstract = {Over the last 20-30 years, the extended Kalman filter (EKF) has become the algorithm of choice in numerous nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system as well estimating parameters for nonlinear system identification (eg, learning the weights of a neural network). The EKF applies the standard linear Kalman filter methodology to a linearization of the true nonlinear system. This approach is sub-optimal, and can easily lead to divergence. Julier et al. (1997), proposed the unscented Kalman filter (UKF) as a derivative-free alternative to the extended Kalman filter in the framework of state estimation. This was extended to parameter estimation by Wan and Van der Merwe et al., (2000). The UKF consistently outperforms the EKF in terms of prediction and estimation error, at an equal computational complexity of (OL/sup 3/)/sup l/ for general state-space problems. When the EKF is applied to parameter estimation, the special form of the state-space equations allows for an O(L/sup 2/) implementation. This paper introduces the square-root unscented Kalman filter (SR-UKF) which is also O(L/sup 3/) for general state estimation and O(L/sup 2/) for parameter estimation (note the original formulation of the UKF for parameter-estimation was O(L/sup 3/)). In addition, the square-root forms have the added benefit of numerical stability and guaranteed positive semi-definiteness of the state covariances.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/I257B749/Van der Merwe and Wan - 2001 - The square-root unscented Kalman filter for state .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/8QAY2AYU/940586.html},
  keywords = {Computational complexity,covariance analysis,EKF,Equations,Estimation error,extended Kalman filter,filtering theory,Kalman filters,Machine learning,Machine learning algorithms,Neural networks,nonlinear dynamic system,nonlinear dynamical systems,Nonlinear dynamical systems,nonlinear estimation,nonlinear system identification,Nonlinear systems,numerical stability,parameter estimation,Parameter estimation,positive semi-definiteness,sqrt,square-root unscented Kalman filter,SR-UKF,state covariances,state estimation,State estimation}
}

@misc{watsonGeneralViewGaussian,
  title = {General {{View}} of {{Gaussian Input Inference}} for {{Control}}},
  author = {Watson, Joe},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/AB6CWLTV/Watson - General View of Gaussian Input Inference for Contr.pdf}
}

@phdthesis{williamsonLearningNonLinearDynamical2020,
  title = {Learning {{Non}}-{{Linear Dynamical Systems}} with the {{Koopman Operator}}},
  author = {Williamson, Len Cewa},
  year = {2020},
  month = apr,
  address = {{Darmstadt, Germany}},
  abstract = {Transforming coordinates, that make non-linear dynamics approximately linear has the potential to enable non-linear prediction, estimation, and control using linear theory. The Koopman operator is a data-driven embedding of a dynamical system. The eigenfunctions of the Koopman operator globally linearize the dynamics. Approximating the eigenfunction remains an open question. This work leverages Fourier series to discover representations of Koopman eigenfunctions from data. Our introduced algorithm learns an embedding for the underlying dynamics while approximating the Koopman operator. We identify non-linear coordinates on which the dynamics are globally linear. Using line search we can recover the true trajectory of the dynamical system. We linearly approximate the simple pendulum, a dynamical system with a continuous spectrum.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4N7ICF5P/Williamson - 2020 - Learning Non-Linear Dynamical Systems with the Koo.pdf},
  language = {en},
  school = {TU Darmstadt},
  type = {Master's {{Thesis}}}
}

@article{zhangSOLARDeepStructured2019,
  title = {{{SOLAR}}: {{Deep Structured Representations}} for {{Model}}-{{Based Reinforcement Learning}}},
  shorttitle = {{{SOLAR}}},
  author = {Zhang, Marvin and Vikram, Sharad and Smith, Laura and Abbeel, Pieter and Johnson, Matthew J. and Levine, Sergey},
  year = {2019},
  month = jun,
  abstract = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.},
  archivePrefix = {arXiv},
  eprint = {1808.09105},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/A94JVATH/Zhang et al. - 2019 - SOLAR Deep Structured Representations for Model-B.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/FI8F9TJK/1808.html},
  journal = {arXiv:1808.09105 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}


