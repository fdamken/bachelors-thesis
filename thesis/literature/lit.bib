
@article{arasaratnamCubatureKalmanFilters2009,
  title = {Cubature {{Kalman Filters}}},
  author = {Arasaratnam, Ienkaran and Haykin, Simon},
  year = {2009},
  month = jun,
  volume = {54},
  pages = {1254--1269},
  issn = {1558-2523},
  doi = {10.1109/TAC.2009.2019800},
  abstract = {In this paper, we present a new nonlinear filter for high-dimensional state estimation, which we have named the cubature Kalman filter (CKF). The heart of the CKF is a spherical-radial cubature rule, which makes it possible to numerically compute multivariate moment integrals encountered in the nonlinear Bayesian filter. Specifically, we derive a third-degree spherical-radial cubature rule that provides a set of cubature points scaling linearly with the state-vector dimension. The CKF may therefore provide a systematic solution for high-dimensional nonlinear filtering problems. The paper also includes the derivation of a square-root version of the CKF for improved numerical stability. The CKF is tested experimentally in two nonlinear state estimation problems. In the first problem, the proposed cubature rule is used to compute the second-order statistics of a nonlinearly transformed Gaussian random variable. The second problem addresses the use of the CKF for tracking a maneuvering aircraft. The results of both experiments demonstrate the improved performance of the CKF over conventional nonlinear filters.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/KXRA7ESV/Arasaratnam and Haykin - 2009 - Cubature Kalman Filters.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/Z37QMZN4/4982682.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian filters,Bayesian methods,cubature Kalman filters,cubature points,cubature rules,Filtering,Gaussian processes,Gaussian quadrature rules,Gaussian random variable,Heart,high dimensional nonlinear filtering,high dimensional state estimation,invariant theory,Kalman filter,Kalman filters,maneuvering aircraft tracking,moment integrals,nonlinear Bayesian filter,nonlinear filtering,nonlinear filters,Nonlinear filters,nonlinear state estimation,numerical stability,Numerical stability,Random variables,second-order statistics,spherical-radial cubature rule,state estimation,State estimation,state vector dimension,Statistics,Testing},
  number = {6}
}

@article{baumMaximizationTechniqueOccurring1970,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  volume = {41},
  pages = {164--171},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3IZ55TRL/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf},
  journal = {The Annals of Mathematical Statistics},
  number = {1}
}

@techreport{bealVariationalKalmanSmoother2000,
  title = {The {{Variational Kalman Smoother}}},
  author = {Beal, Matthew J. and Ghahramani, Zoubin},
  year = {2000},
  month = may,
  pages = {15},
  institution = {{Gatsby Computational Neuroscience Unit}},
  abstract = {In this note we outline the derivation of the variational Kalman smoother, in the context of Bayesian Linear Dynamical Systems. The smoother is an efficient algorithm for the E-step in the Expectation-Maximisation (EM) algorithm for linear-Gaussian state-space models. However, inference approximations are required if we hold distributions over parameters. We derive the E-step updates for the hidden states (the variational smoother), and the M-step updates for the parameter distributions. We shpw that inference of the hidden state is tractable for any distribution over parameters, provided the expectations of certain quantities available, analytically or otherwise.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/APDQSUZN/Beal and Ghahramani - 2000 - The Variational Kalman Smoother.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UZDVPPBQ/vbssm.tgz},
  language = {en}
}

@article{bellmanDynamicProgramming1966,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard},
  year = {1966},
  month = jul,
  volume = {153},
  pages = {34--37},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.153.3731.34},
  abstract = {Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a "theory." What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology.
The more we study the information processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them.
In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?},
  chapter = {Articles},
  copyright = {\textcopyright{} 1966 by the American Association for the Advancement of Science},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/VMED5EJF/Bellman - 1966 - Dynamic Programming.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/YY4JK3DI/tab-pdf.html},
  journal = {Science},
  language = {en},
  number = {3731},
  pmid = {17730601}
}

@book{birkhoffDynamicalSystems1927,
  title = {Dynamical {{Systems}}},
  author = {Birkhoff, George David},
  year = {1927},
  month = dec,
  publisher = {{American Mathematical Soc.}},
  abstract = {His research in dynamics constitutes the middle period of Birkhoff\&\#39;s scientific career, that of maturity and greatest power. --Yearbook of the American Philosophical Society The author\&\#39;s great book ... is well known to all, and the diverse active modern developments in mathematics which have been inspired by this volume bear the most eloquent testimony to its quality and influence. --Zentralblatt MATH In 1927, G. D. Birkhoff wrote a remarkable treatise on the theory of dynamical systems that would inspire many later mathematicians to do great work. To a large extent, Birkhoff was writing about his own work on the subject, which was itself strongly influenced by Poincare\&\#39;s approach to dynamical systems. With this book, Birkhoff also demonstrated that the subject was a beautiful theory, much more than a compendium of individual results. The influence of this work can be found in many fields, including differential equations, mathematical physics, and even what is now known as Morse theory. The present volume is the revised 1966 reprinting of the book, including a new addendum, some footnotes, references added by Jurgen Moser, and a special preface by Marston Morse. Although dynamical systems has thrived in the decades since Birkhoff\&\#39;s book was published, this treatise continues to offer insight and inspiration for still more generations of mathematicians.},
  googlebooks = {ygmWAwAAQBAJ},
  isbn = {978-0-8218-1009-5},
  keywords = {Mathematics / Geometry / Differential},
  language = {en}
}

@article{deanRobustGuaranteesPerceptionBased2019,
  title = {Robust {{Guarantees}} for {{Perception}}-{{Based Control}}},
  author = {Dean, Sarah and Matni, Nikolai and Recht, Benjamin and Ye, Vickie},
  year = {2019},
  month = dec,
  abstract = {Motivated by vision-based control of autonomous vehicles, we consider the problem of controlling a known linear dynamical system for which partial state information, such as vehicle position, is extracted from complex and nonlinear data, such as a camera image. Our approach is to use a learned perception map that predicts some linear function of the state and to design a corresponding safe set and robust controller for the closed loop system with this sensing scheme. We show that under suitable smoothness assumptions on both the perception map and the generative model relating state to complex and nonlinear data, parameters of the safe set can be learned via appropriately dense sampling of the state space. We then prove that the resulting perception-control loop has favorable generalization properties. We illustrate the usefulness of our approach on a synthetic example and on the self-driving car simulation platform CARLA.},
  archivePrefix = {arXiv},
  eprint = {1907.03680},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QBJRRF8R/Dean et al. - 2019 - Robust Guarantees for Perception-Based Control.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XZ74IRW8/1907.html},
  journal = {arXiv:1907.03680 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{deisenrothProbabilisticPerspectiveGaussian2011,
  title = {A {{Probabilistic Perspective}} on {{Gaussian Filtering}} and {{Smoothing}}},
  author = {Deisenroth, Marc Peter and Ohlsson, Henrik},
  year = {2011},
  month = jun,
  abstract = {We present a general probabilistic perspective on Gaussian filtering and smoothing. This allows us to show that common approaches to Gaussian filtering/smoothing can be distinguished solely by their methods of computing/approximating the means and covariances of joint probabilities. This implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments. Based on this insight, we derive the cubature Kalman smoother and propose a novel robust filtering and smoothing algorithm based on Gibbs sampling.},
  archivePrefix = {arXiv},
  eprint = {1006.2165},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/SZAL384A/Deisenroth and Ohlsson - 2011 - A Probabilistic Perspective on Gaussian Filtering .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/EGK5Y7DI/1006.html},
  journal = {arXiv:1006.2165 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, math, stat}
}

@article{depierroModifiedExpectationMaximization1995,
  title = {A Modified Expectation Maximization Algorithm for Penalized Likelihood Estimation in Emission Tomography},
  author = {De Pierro, A.R.},
  year = {1995},
  month = mar,
  volume = {14},
  pages = {132--137},
  issn = {1558-254X},
  doi = {10.1109/42.370409},
  abstract = {The maximum likelihood (ML) expectation maximization (EM) approach in emission tomography has been very popular in medical imaging for several years. In spite of this, no satisfactory convergent modifications have been proposed for the regularized approach. Here, a modification of the EM algorithm is presented. The new method is a natural extension of the EM for maximizing likelihood with concave priors. Convergence proofs are given.{$<>$}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4HT4SEGT/De Pierro - 1995 - A modified expectation maximization algorithm for .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/P75TPH55/370409.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {Biomedical imaging,Computed tomography,concave priors,Convergence,convergence proofs,diagnostic nuclear medicine,Electrical capacitance tomography,emission tomography,image reconstruction,Image reconstruction,Isotopes,Maximum likelihood detection,Maximum likelihood estimation,medical diagnostic imaging,medical image processing,modified expectation maximization algorithm,penalized likelihood estimation,Positron emission tomography,regularized approach,Single photon emission computed tomography},
  number = {1}
}

@article{doerschTutorialVariationalAutoencoders2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = aug,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archivePrefix = {arXiv},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/F4N3MJ8B/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/WBAC9J3J/1606.html},
  journal = {arXiv:1606.05908 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  issn = {1533-7928},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/BT6LLCJH/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HHB9TDLA/duchi11a.html},
  journal = {Journal of Machine Learning Research},
  number = {61}
}

@book{elhadjDynamicalSystemsTheories2019,
  title = {Dynamical {{Systems}}: {{Theories}} and {{Applications}}},
  shorttitle = {Dynamical {{Systems}}},
  author = {Elhadj, Zeraoulia},
  year = {2019},
  month = jan,
  publisher = {{CRC Press}},
  abstract = {Chaos is the idea that a system will produce very different long-term behaviors when the initial conditions are perturbed only slightly. Chaos is used for novel, time- or energy-critical interdisciplinary applications. Examples include high-performance circuits and devices, liquid mixing, chemical reactions, biological systems, crisis management, secure information processing, and critical decision-making in politics, economics, as well as military applications, etc. This book presents the latest investigations in the theory of chaotic systems and their dynamics. The book covers some theoretical aspects of the subject arising in the study of both discrete and continuous-time chaotic dynamical systems. This book presents the state-of-the-art of the more advanced studies of chaotic dynamical systems.},
  googlebooks = {mFupDwAAQBAJ},
  isbn = {978-0-429-65006-2},
  keywords = {Mathematics / Arithmetic,Mathematics / Differential Equations / General,Science / Life Sciences / General,Science / Physics / Mathematical \& Computational},
  language = {en}
}

@article{fesslerSpacealternatingGeneralizedExpectationmaximization1994,
  title = {Space-Alternating Generalized Expectation-Maximization Algorithm},
  author = {Fessler, J.A. and Hero, A.O.},
  year = {1994},
  month = oct,
  volume = {42},
  pages = {2664--2677},
  issn = {1941-0476},
  doi = {10.1109/78.324732},
  abstract = {The expectation-maximization (EM) method can facilitate maximizing likelihood functions that arise in statistical estimation problems. In the classical EM paradigm, one iteratively maximizes the conditional log-likelihood of a single unobservable complete data space, rather than maximizing the intractable likelihood function for the measured or incomplete data. EM algorithms update all parameters simultaneously, which has two drawbacks: 1) slow convergence, and 2) difficult maximization steps due to coupling when smoothness penalties are used. The paper describes the space-alternating generalized EM (SAGE) method, which updates the parameters sequentially by alternating between several small hidden-data spaces defined by the algorithm designer. The authors prove that the sequence of estimates monotonically increases the penalized-likelihood objective, derive asymptotic convergence rates, and provide sufficient conditions for monotone convergence in norm. Two signal processing applications illustrate the method: estimation of superimposed signals in Gaussian noise, and image reconstruction from Poisson measurements. In both applications, the SAGE algorithms easily accommodate smoothness penalties and converge faster than the EM algorithms.{$<>$}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IB6LW2P7/Fessler and Hero - 1994 - Space-alternating generalized expectation-maximiza.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/7I3KD9JZ/324732.html},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {Algorithm design and analysis,asymptotic convergence rates,classical EM paradigm,conditional log-likelihood,Convergence,convergence of numerical methods,Expectation-maximization algorithms,Extraterrestrial measurements,Gaussian noise,image reconstruction,Image reconstruction,intractable likelihood function,Iterative algorithms,iterative methods,maximization steps,maximum likelihood estimation,monotone convergence,parameter estimation,penalized-likelihood objective,Poisson measurements,random noise,SAGE method,sequence of estimates,signal processing,Signal processing,Signal processing algorithms,signal processing applications,small hidden-data spaces,smoothness penalties,space-alternating generalized expectation-maximization algorithm,statistical analysis,statistical estimation problems,stochastic processes,sufficient conditions,Sufficient conditions,superimposed signals,unobservable complete data space},
  number = {10}
}

@techreport{ghahramaniParameterEstimationLinear1996,
  title = {Parameter Estimation for Linear Dynamical Systems},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {Februrary 22, 1996},
  institution = {{University of Toronto}},
  abstract = {Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Stoffer, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RHES3QAG/Ghahramani and Hinton - 1996 - Parameter estimation for linear dynamical systems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/V49S4PIV/lds.tar.gz},
  number = {CRG-TR-96-2}
}

@article{gibsonRobustMaximumlikelihoodEstimation2005,
  title = {Robust Maximum-Likelihood Estimation of Multivariable Dynamic Systems},
  author = {Gibson, Stuart and Ninness, Brett},
  year = {2005},
  month = oct,
  volume = {41},
  pages = {1667--1682},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2005.05.008},
  abstract = {This paper examines the problem of estimating linear time-invariant state-space system models. In particular, it addresses the parametrization and numerical robustness concerns that arise in the multivariable case. These difficulties are well recognised in the literature, resulting (for example) in extensive study of subspace-based techniques, as well as recent interest in `data driven' local co-ordinate approaches to gradient search solutions. The paper here proposes a different strategy that employs the expectation\textendash maximisation (EM) technique. The consequence is an algorithm that is iterative, with associated likelihood values that are locally convergent to stationary points of the (Gaussian) likelihood function. Furthermore, theoretical and empirical evidence presented here establishes additional attractive properties such as numerical robustness, avoidance of difficult parametrization choices, the ability to naturally and easily estimate non-zero initial conditions, and moderate computational cost. Moreover, since the methods here are maximum-likelihood based, they have associated known and asymptotically optimal statistical properties.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/2ZV49YZM/Gibson and Ninness - 2005 - Robust maximum-likelihood estimation of multivaria.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/XIYEQI7L/S0005109805001810.html},
  journal = {Automatica},
  keywords = {Parameter estimation,System identification},
  language = {en},
  number = {10}
}

@article{itoGaussianFiltersNonlinear2000,
  title = {Gaussian Filters for Nonlinear Filtering Problems},
  author = {Ito, K. and Xiong, K.},
  year = {2000},
  month = may,
  volume = {45},
  pages = {910--927},
  issn = {1558-2523},
  doi = {10.1109/9.855552},
  abstract = {We develop and analyze real-time and accurate filters for nonlinear filtering problems based on the Gaussian distributions. We present the systematic formulation of Gaussian filters and develop efficient and accurate numerical integration of the optimal filter. We also discuss the mixed Gaussian filters in which the conditional probability density is approximated by the sum of Gaussian distributions. A new update rule of weights for Gaussian sum filters is proposed. Our numerical tests demonstrate that new filters significantly improve the extended Kalman filter with no additional cost, and the new Gaussian sum filter has a nearly optimal performance.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MSWJ85KN/Ito and Xiong - 2000 - Gaussian filters for nonlinear filtering problems.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/A4S4RVGQ/855552.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Bayesian methods,Cost function,Filtering,filtering theory,Filters,Gaussian distribution,Gaussian distributions,Gaussian filters,Gaussian processes,Indium tin oxide,Kalman filter,Kalman filters,nonlinear filtering,probability density,Sonar navigation,Testing},
  number = {5}
}

@article{kaiserDatadrivenDiscoveryKoopman2020,
  title = {Data-Driven Discovery of {{Koopman}} Eigenfunctions for Control},
  author = {Kaiser, Eurika and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2020},
  month = may,
  abstract = {Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to represent the dynamics directly in eigenfunction coordinates. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control is formulated in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including 1) a nonlinear system with a known linear embedding, 2) a variety of Hamiltonian systems, and 3) a high-dimensional double-gyre model for ocean mixing.},
  archivePrefix = {arXiv},
  eprint = {1707.01146},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/34VHYNBZ/Kaiser et al. - 2020 - Data-driven discovery of Koopman eigenfunctions fo.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/NAS8LVTV/1707.html},
  journal = {arXiv:1707.01146 [math]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control},
  primaryClass = {math}
}

@article{karlDeepVariationalBayes2017,
  title = {Deep {{Variational Bayes Filters}}: {{Unsupervised Learning}} of {{State Space Models}} from {{Raw Data}}},
  shorttitle = {Deep {{Variational Bayes Filters}}},
  author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2017},
  month = mar,
  abstract = {We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
  archivePrefix = {arXiv},
  eprint = {1605.06432},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/EVWLJ4FN/Karl et al. - 2017 - Deep Variational Bayes Filters Unsupervised Learn.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AWT822ZV/dvbfintro.html;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HWEUGJNN/1605.html},
  journal = {arXiv:1605.06432 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3QLZVKLI/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HEHEXW8E/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/XNWXKDKZ/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/VMLNAGA4/1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{koopmanHamiltonianSystemsTransformation1931a,
  title = {Hamiltonian {{Systems}} and {{Transformation}} in {{Hilbert Space}}},
  author = {Koopman, B. O.},
  year = {1931},
  month = may,
  volume = {17},
  pages = {315--318},
  issn = {0027-8424},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/U7CXQKGD/Koopman - 1931 - Hamiltonian Systems and Transformation in Hilbert .pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {5},
  pmcid = {PMC1076052},
  pmid = {16577368}
}

@article{luschDeepLearningUniversal2018,
  title = {Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics},
  author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {4950},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07210-0},
  abstract = {Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.},
  archivePrefix = {arXiv},
  eprint = {1712.09707},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/RRYUXGIZ/Lusch et al. - 2018 - Deep learning for universal linear embeddings of n.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/LS6VRGYR/1712.html},
  journal = {Nature Communications},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  number = {1}
}

@article{mezicSpectralPropertiesDynamical2005,
  title = {Spectral {{Properties}} of {{Dynamical Systems}}, {{Model Reduction}} and {{Decompositions}}},
  author = {Mezi{\'c}, Igor},
  year = {2005},
  month = aug,
  volume = {41},
  pages = {309--325},
  issn = {1573-269X},
  doi = {10.1007/s11071-005-2824-x},
  abstract = {In this paper we discuss two issues related to model reduction of deterministic or stochastic processes. The first is the relationship of the spectral properties of the dynamics on the attractor of the original, high-dimensional dynamical system with the properties and possibilities for model reduction. We review some elements of the spectral theory of dynamical systems. We apply this theory to obtain a decomposition of the process that utilizes spectral properties of the linear Koopman operator associated with the asymptotic dynamics on the attractor. This allows us to extract the almost periodic part of the evolving process. The remainder of the process has continuous spectrum. The second topic we discuss is that of model validation, where the original, possibly high-dimensional dynamics and the dynamics of the reduced model \textendash{} that can be deterministic or stochastic \textendash{} are compared in some norm. Using the ``statistical Takens theorem'' proven in (Mezi\'c, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of average energy contained in the finite-dimensional projection is one in the hierarchy of functionals of the field that need to be checked in order to assess the accuracy of the projection.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3Q35VGPG/Mezić - 2005 - Spectral Properties of Dynamical Systems, Model Re.pdf},
  journal = {Nonlinear Dynamics},
  language = {en},
  number = {1}
}

@techreport{minkaBayesianLinearRegression1999,
  title = {Bayesian {{Linear Regression}}},
  author = {Minka, Thomas P.},
  year = {1999},
  institution = {{3594 Security Ticket Control}},
  abstract = {This note derives the posterior, evidence, and predictive density for linear multivariate regression under zero-mean Gaussian noise. Many Bayesian texts, such as Box \& Tiao (1973), cover linear regression. This note contributes to the discussion by paying careful attention to invariance issues, demonstrating model selection based on the evidence, and illustrating the shape of the predictive density. Piecewise regression and basis function regression are also discussed. 1 Introduction The data model is that an input vector x of length m multiplies a coefficient matrix A to produce an output vector y of length d, with Gaussian noise added:  y = Ax + e (1)  e N (0; V) (2)  p(yjx; A;V) N (Ax; V) (3) This is a conditional model for y only: the distribution of x is not needed and in fact irrelevant to all inferences in this paper. As we shall see, conditional models create subtleties in Bayesian inference. In the special case x = 1 and m = 1, the conditioning disappears and we simply have a ...},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QSGJ23H4/Minka - 1999 - Bayesian Linear Regression.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/T4PYCVH9/summary.html}
}

@techreport{minkaHiddenMarkovModels1999,
  title = {From {{Hidden Markov Models}} to {{Linear Dynamical Systems}}},
  author = {Minka, Thomas P.},
  year = {1999},
  month = jul,
  abstract = {Hidden Markov Models (HMMs) and Linear Dynamical Systems (LDSs) ate based on the same assumption: a hidden state variable, of which we can make noisy measurements, evolves with Markovian dynamics. Both have the same independency diagram and consequently the learning and inference algorithms for both have the same structure. The only difference is that the HMM uses a discrete state variable with arbitrary dynamics and arbitrary measurements while the LDS uses a continuous state variable with linear-Gaussian dynamics and measurements. We show how the forward-backward equations for the HMM, specialized to linear-Gaussian assumptions, lead directly to Kalman filtering and Rauch-Tung-Streibel smoothing. We also investigate the most general possible modeling assumptions which can lead to efficient recursion in the case of continuous state variables.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3Y5XFY3M/Minka - 1999 - From Hidden Markov Models to Linear Dynamical Syst.pdf},
  number = {TR-531}
}

@article{moonExpectationmaximizationAlgorithm1996,
  title = {The Expectation-Maximization Algorithm},
  author = {Moon, T.K.},
  year = {1996},
  month = nov,
  volume = {13},
  pages = {47--60},
  issn = {1558-0792},
  doi = {10.1109/79.543975},
  abstract = {A common task in signal processing is the estimation of the parameters of a probability distribution function. Perhaps the most frequently encountered estimation problem is the estimation of the mean of a signal in noise. In many parameter estimation problems the situation is more complicated because direct access to the data necessary to estimate the parameters is impossible, or some of the data are missing. Such difficulties arise when an outcome is a result of an accumulation of simpler outcomes, or when outcomes are clumped together, for example, in a binning or histogram operation. There may also be data dropouts or clustering in such a way that the number of underlying data points is unknown (censoring and/or truncation). The EM (expectation-maximization) algorithm is ideally suited to problems of this sort, in that it produces maximum-likelihood (ML) estimates of parameters when there is a many-to-one mapping from an underlying distribution to the distribution governing the observation. The EM algorithm is presented at a level suitable for signal processing practitioners who have had some exposure to estimation theory.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/QQ2VZ39Q/Moon - 1996 - The expectation-maximization algorithm.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/P54PZ9N2/543975.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {binning,censoring,Convergence,data clustering,data dropouts,EM algorithm,estimation theory,Estimation theory,expectation-maximization algorithm,Hidden Markov models,histogram,Histograms,Image reconstruction,maximum likelihood estimation,Maximum likelihood estimation,maximum-likelihood estimates,mean,noise,parameter estimation,Parameter estimation,Phase detection,probability,Probability distribution,probability distribution function,signal processing,Signal processing algorithms,truncation},
  number = {6}
}

@article{mortonDeepVariationalKoopman2019,
  title = {Deep {{Variational Koopman Models}}: {{Inferring Koopman Observations}} for {{Uncertainty}}-{{Aware Dynamics Modeling}} and {{Control}}},
  shorttitle = {Deep {{Variational Koopman Models}}},
  author = {Morton, Jeremy and Witherden, Freddie D. and Kochenderfer, Mykel J.},
  year = {2019},
  month = jun,
  abstract = {Koopman theory asserts that a nonlinear dynamical system can be mapped to a linear system, where the Koopman operator advances observations of the state forward in time. However, the observable functions that map states to observations are generally unknown. We introduce the Deep Variational Koopman (DVK) model, a method for inferring distributions over observations that can be propagated linearly in time. By sampling from the inferred distributions, we obtain a distribution over dynamical models, which in turn provides a distribution over possible outcomes as a modeled system advances in time. Experiments show that the DVK model is effective at long-term prediction for a variety of dynamical systems. Furthermore, we describe how to incorporate the learned models into a control framework, and demonstrate that accounting for the uncertainty present in the distribution over dynamical models enables more effective control.},
  archivePrefix = {arXiv},
  eprint = {1902.09742},
  eprinttype = {arxiv},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/5IU4LHDL/Morton et al. - 2019 - Deep Variational Koopman Models Inferring Koopman.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/AEEQ9MTN/1902.html},
  journal = {arXiv:1902.09742 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8026--8037},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/NKBF8PL6/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/5VVE4H5D/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{rauchMaximumLikelihoodEstimates1965,
  title = {Maximum Likelihood Estimates of Linear Dynamic Systems},
  author = {RAUCH, H. E. and TUNG, F. and STRIEBEL, C. T.},
  year = {1965},
  volume = {3},
  pages = {1445--1450},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  issn = {0001-1452},
  doi = {10.2514/3.3166},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/FJPDZRFJ/RAUCH et al. - 1965 - Maximum likelihood estimates of linear dynamic sys.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/UBE6VQV3/3.html},
  journal = {AIAA Journal},
  number = {8}
}

@article{schonSystemIdentificationNonlinear2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  volume = {47},
  pages = {39--49},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.10.013},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provides arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/MPU6JLFX/Schön et al. - 2011 - System identification of nonlinear state-space mod.pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/22NXXPUY/S0005109810004279.html},
  journal = {Automatica},
  keywords = {Dynamic systems,Expectation maximisation algorithm,Monte Carlo method,Nonlinear models,Particle methods,Smoothing filters,System identification},
  language = {en},
  number = {1}
}

@article{shumwayApproachTimeSeries1982,
  title = {An {{Approach}} to {{Time Series Smoothing}} and {{Forecasting Using}} the {{Em Algorithm}}},
  author = {Shumway, R. H. and Stoffer, D. S.},
  year = {1982},
  volume = {3},
  pages = {253--264},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1982.tb00349.x},
  abstract = {Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/IC35UB4V/j.1467-9892.1982.tb00349.html},
  journal = {Journal of Time Series Analysis},
  keywords = {EM algorithm,forecasting,Kalman filter,maximum likelihood,Missing data},
  language = {en},
  number = {4}
}

@phdthesis{solinCubatureIntegrationMethods2010,
  title = {Cubature {{Integration Methods}} in {{Non}}-{{Linear Kalman Filtering}} and {{Smoothing}}},
  author = {Solin, Arno},
  year = {2010},
  abstract = {Optimal estimation problems arise in various different settings where indirect noisy observations are used to determine the underlying state of a time-varying system. For systems with non-linear dynamics there exist various methods that extend linear filtering and smoothing methods to handle non-linearities. In this thesis the non-linear optimal estimation framework is presented with the help of an assumed density approach. The Gaussian integrals that arise in this setting are solved using two different cubature integration methods. Cubature integration extends the weighted sum approach from univariate quadrature methods to multidimensional cubature methods. In this thesis the focus is put on two methods that use deterministically chosen sigma points to form the desired approximation. The Gauss\textendash Hermite rule uses a simple product rule method to fill the multidimensional space with cubature points, whereas the spherical\textendash radial rule uses invariant theory to diminish the number of points by utilizing symmetries. The},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/3ATM3DZ6/Solin - 2010 - Cubature Integration Methods in Non-Linear Kalman .pdf;/home/fdamken/snap/zotero-snap/common/Zotero/storage/HQDQY8HG/summary.html},
  type = {Bachelor's {{Thesis}}}
}

@phdthesis{williamsonLearningNonLinearDynamical2020,
  title = {Learning {{Non}}-{{Linear Dynamical Systems}} with the {{Koopman Operator}}},
  author = {Williamson, Len Cewa},
  year = {2020},
  month = apr,
  address = {{Darmstadt, Germany}},
  abstract = {Transforming coordinates, that make non-linear dynamics approximately linear has the potential to enable non-linear prediction, estimation, and control using linear theory. The Koopman operator is a data-driven embedding of a dynamical system. The eigenfunctions of the Koopman operator globally linearize the dynamics. Approximating the eigenfunction remains an open question. This work leverages Fourier series to discover representations of Koopman eigenfunctions from data. Our introduced algorithm learns an embedding for the underlying dynamics while approximating the Koopman operator. We identify non-linear coordinates on which the dynamics are globally linear. Using line search we can recover the true trajectory of the dynamical system. We linearly approximate the simple pendulum, a dynamical system with a continuous spectrum.},
  file = {/home/fdamken/snap/zotero-snap/common/Zotero/storage/4N7ICF5P/Williamson - 2020 - Learning Non-Linear Dynamical Systems with the Koo.pdf},
  language = {en},
  school = {TU Darmstadt},
  type = {Master's {{Thesis}}}
}


