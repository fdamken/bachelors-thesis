% !TeX spellcheck = en_US
% !TeX program = lualatex


% Do not surround the equal signs with spaces, it causes errors!
\documentclass[
	aspectratio=43,
	color={accentcolor=1c},
	logo=false,
	colorframetitle=true,
	%handout
]{tudabeamer}

% Include preamble to not clutter this file.
\input{preamble/packages}
\input{preamble/styles}
\input{preamble/tikzGraphs}


\title{Variational Autoencoders for Koopman Dynamical Systems}
\subtitle{B.Sc. Thesis Defense}
\author{Fabian Damken}
\department{Department of Computer Science}
\institute{Intelligent Autonomous Systems}
\date{December 10, 2020}

\logo*{\includegraphics{img/iasLogo}}

\titlegraphic*{\includegraphics{figures/title}}

\newcommand{\hence}{\(\longrightarrow\,\)}

\begin{document}
	\maketitle

	\section{Introduction}
		\begin{frame}{Motivation}
			\begin{itemize}
				\item Nonlinear systems are really hard to control and predict.
				\item We seek for models that are easier to control.
				\item Linear models are (relatively) easy to control!
				\item Koopman theory offers a way to get a linear model.
					\begin{itemize}
						\item Introduces a latent state advancing linearly.
						\item The transition to the latent dimension is nonlinear.
					\end{itemize}
			\end{itemize}
		\end{frame}

		\begin{frame}[c]{Koopman Dynamical System}
			\vspace{-0.5cm}
			\begin{columns}[c]
				\begin{column}{0.85\linewidth}
					\begin{center}
						\resizebox{\linewidth}{!}{\tikzKoopmanOperator}
					\end{center}
					\vspace{-0.5cm}
					\begin{center}
						\footnotesize
						Adopted from Brunton et al., "Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control".
					\end{center}
				\end{column}
				\begin{column}{0.15\linewidth}
					\begin{align*}
						\vec{y}_{t + 1} &= \vec{F}(\vec{y}_t) \\
						      \vec{x}_t &= \vec{h}(\vec{y}_t)
					\end{align*}
				\end{column}
			\end{columns}
			\vspace{0.5cm}

			\begin{equation*}
				\vec{h}(\vec{y}_{t + 1}) = \mathcal{K} \vec{h}(\vec{y}_t)
				\quad\iff\quad
				\vec{x}_{t + 1} = \mathcal{K} \vec{x}_t
			\end{equation*}

			This is possible for every nonlinear dynamical system, and linearizes globally! But the embedding \(\vec{h}\) is typically infinite-dimensional\dots
		\end{frame}

		\begin{frame}[t]{Linear Gaussian Dynamical System}
			\begin{center}
				\tikzLinearGaussianDynamicalSystem
			\end{center}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( \mat{C} \vec{s}_t + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					p(\vec{s}_{t + 1} \given \vec{s}_t) &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					      p(\vec{y}_t \given \vec{s}_t) &\sim \eqmakebox[c][l]{\(\normal(\mat{C} \vec{s}_t, \mat{R})\)}
				\end{aligned}
			\end{equation*}

			\vspace{0.95cm}
			\begin{center}
				\footnotesize
				Adopted from Minka, "From Hidden Markov Models to Linear Dynamical Systems".
			\end{center}
		\end{frame}
	% end

	\section{Koopman Inference}
		\begin{frame}[t]{The Koopman Inference Model}
			\begin{center}
				\tikzNonlinearGaussianKoopman
			\end{center}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( {\color{TUDa-9b} \vec{g}(\vec{s}_t)} + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					p(\vec{s}_{t + 1} \given \vec{s}_t) &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					      p(\vec{y}_t \given \vec{s}_t) &\sim \eqmakebox[c][l]{\(\normal({\color{TUDa-9b} \vec{g}(\vec{s}_t)}, \mat{R})\)}
				\end{aligned}
			\end{equation*}

			\vspace{0.15cm}
			\onslide<2->{
				\begin{align*}
					\vec{s}_t \approx \vec{x}_t \qquad
					\mat{A} \approx \mathcal{K} \qquad
					\vec{g}(\cdot) \approx \vec{h}^{-1}(\cdot)
				\end{align*}
			}
		\end{frame}

		\begin{frame}{How to learn?}
			Goal: Estimate all of the following:
			\begin{itemize}
				\item Latent dynamics matrix \( \mat{A} \).
				\item Measurement function \( \vec{g}_{\vec{\theta}}(\cdot) \) (\ie the parameters \(\vec{\theta}\)). \\
					We use a neural network here; like a VAE without an amortization network.
				\item Noise covariances \(\mat{Q}\), \(\mat{R}\).
			\end{itemize}
			We employ an EM-algorithm to do that.

			\onslide<2->{
				\begin{alertblock}{Core Problem}
					Some expectations cannot be evaluated in closed form for a nonlinear \( \vec{g}_{\vec{\theta}}(\cdot) \).
				\end{alertblock}
			}

			\onslide<3->{
				\begin{itemize}
					\item Use cubature rules for evaluating the intractable expectations.
					\item No closed form solution for maximizing w.r.t. function parameters \( \vec{\theta} \).
					\item<4->[] \(\qquad\longrightarrow\quad\) Use backpropagation and gradient descent!
				\end{itemize}
			}
		\end{frame}

		\begin{frame}{Quadrature for High Dimensions: Cubature Rules}{The Spherical-Radial Cubature Rule}
			\vspace{-0.5cm}
			\begin{columns}[c]
				\begin{column}{0.5\linewidth}
					\begin{flushright}
						\includegraphics[width=0.8\linewidth]{figures/spherical-radial-cubature-original.pdf}
					\end{flushright}
				\end{column}
				\begin{column}{0.5\linewidth}
					\begin{flushleft}
						\includegraphics[width=0.8\linewidth]{figures/spherical-radial-cubature-transformed.pdf}
					\end{flushleft}
				\end{column}
			\end{columns}

			\vspace{0.3cm}
			Approximation of an arbitrary Gaussian expectation (with \( \vec{\xi}_i = \sqrt{n} [\vec{1}]_i \)):
			\vspace{-0.1cm}
			\begin{equation*}
				\E_{\vec{x} \,\sim\, \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big]
					= \int_{\R^n} \! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}}
					\approx \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}\Big(\! \sqrt{\mat{\Sigma}} \vec{\xi}_i + \vec{\mu} \Big)
			\end{equation*}
		\end{frame}
	% end

	\section{Related Work}
		\begin{frame}{Related Work}
			\begin{itemize}
				\item<+-> Lusch et al. "Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics":
					\begin{itemize}
						\item Use an autoencoder approach with an encoder network.
						\item No probabilistic interpretation.
					\end{itemize}
				\item<+-> Morton et al. "Deep Variational Koopman Models: Inferring Koopman Observations for Uncertainty-Aware Dynamics Modeling and Control":
					\begin{itemize}
						\item Uses six different neural networks to perform variational inference.
						\item Very complex model, lots of parameters.
						\item Utilize much more training data than our method.
					\end{itemize}
				\item<+-> Becker et al. "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces"
					\begin{itemize}
						\item Proposal of a new type of recurrent neural networks: Kalman network.
						\item Uses high-dimensional linear embeddings with factorized covariances. \\
						\quad \(\longrightarrow\,\) Simple Kalman update rules.
					\end{itemize}
				%\item<+-> Zhang, Vikram et al. "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
			\end{itemize}
		\end{frame}
	% end

	\section{Experiments}
		\begin{frame}{Experimental Questions and Environments}
			\begin{enumerate}
				\item How does the latent dimensionality affect the model performance?
				\item How does our model perform in relation to the DVK model?
				\item How do different feature transformations of the environment affect the model performance?
			\end{enumerate}

			\onslide<2->{
				\vspace{0.5cm}
				Environments:
				\begin{itemize}
					\item \eqmakebox[envs][l]{Pendulum\footnotemark[1]:}            \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{\textbf<2->{Damped Pendulum:}}     \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{Gym Pendulum\footnotemark[1]:}        \(\cos\theta\),\, \(\sin\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{Gym Cartpole\footnotemark[1]:}        \(x\),\, \(\dot{x}\),\, \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{\textbf<2->{Gym Double Pendulum:}} \(\cos\varphi_1\),\, \(\sin\varphi_1\),\, \(\cos\varphi_2\),\, \(\sin\varphi_2\),\, \(\dot{\varphi}_1\),\, \(\dot{\varphi}_2\)
				\end{itemize}
			}

			\onslide<2->{
				\vspace{0.6cm}
				\footnoterule
				{ \footnotesize \footnotemark[1] See thesis for experiment results. }
			}
		\end{frame}

		\subsection{Damped Pendulum}
			\begin{frame}{Damped Pendulum}
				\vspace{-0.5cm}
				\begin{equation*}
					m\ddot{\theta} = \frac{g}{L} \sin\theta - d \dot{\theta}
					\quad\longrightarrow\quad
					\ddot{\theta} = \sin\theta - 0.1 \dot{\theta}
				\end{equation*}

				\begin{columns}[c]
					\begin{column}{0.35\linewidth}
						Observations:
						\begin{itemize}
							\item \eqmakebox[pendulum][l]{Displacement:} \(\theta\)
							\item \eqmakebox[pendulum][l]{Velocity:}     \(\dot{\theta}\)
						\end{itemize}
						\begin{center}
							\resizebox{0.85\linewidth}{!}{\tikzSimplePendulum}
						\end{center}
					\end{column}
					\begin{column}{0.65\linewidth}
						\begin{center}
							\includegraphics<2->[width=\linewidth]{figures/experiments/pendulum-damped/log-likelihood.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}[c]{Rollout in Observation Space}{Damped Pendulum}
				\begin{center}
					\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/different-latent-dims-rollouts.pdf}
				\end{center}
			\end{frame}

			\begin{frame}{Effect of the Latent Dimensionality}{Damped Pendulum}
				\vspace{-0.6cm}
				\begin{columns}[c]
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/latent-dim/comparison-rmse-rollout-train-normalized-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/latent-dim/comparison-rmse-rollout-prediction-normalized-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
				\end{columns}

				\vspace{-0.4cm}
				\begin{center}
					\includegraphics[width=0.36\linewidth]{figures/experiments/pendulum-damped/latent-dim/comparison-log-likelihood-mean-vs-latent-dim.pdf}
				\end{center}
			\end{frame}

			\begin{frame}[c]{Exemplary Run and Energy, 10 Latents}{Damped Pendulum}
				\begin{center}
					\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/rollout-observations-N0.pdf}
				\end{center}
				\vspace{-1cm}
				\begin{columns}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-total.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-kinetic.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-potential.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}
		% end

		\subsection{Double Pendulum}
			\begin{frame}{Double Pendulum}
				\begin{columns}[c]
					\begin{column}{0.35\linewidth}
						Observations:
						\begin{itemize}
							\item \eqmakebox[doublePendulum][l]{Displacements:} \\
								\quad \(\cos\varphi_1\),\, \(\sin\varphi_1\), \\
								\quad \(\cos\varphi_2\),\, \(\sin\varphi_2\)
							\item Velocities: \(\dot{\varphi}_1\),\, \(\dot{\varphi}_2\)
						\end{itemize}
						\begin{center}
							\resizebox{0.85\linewidth}{!}{\tikzDoublePendulum}
						\end{center}
					\end{column}
					\begin{column}{0.65\linewidth}
						\begin{center}
							\includegraphics<2->[width=\linewidth]{figures/experiments/acrobot-gym/log-likelihood.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}[c]{Rollout in Reduced Observation Space}{Double Pendulum}
				\vspace{-0.25cm}
				\begin{center}
					\includegraphics[width=0.875\linewidth]{figures/experiments/acrobot-gym/different-latent-dims-rollouts-reduced.pdf}
				\end{center}
			\end{frame}

			\begin{frame}{Effect of the Latent Dimensionality}{Double Pendulum}
				\vspace{-0.6cm}
				\begin{columns}[c]
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/acrobot-gym/latent-dim/comparison-rmse-rollout-train-normalized-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/acrobot-gym/latent-dim/comparison-rmse-rollout-prediction-normalized-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
				\end{columns}

				\vspace{-0.4cm}
				\begin{center}
					\includegraphics[width=0.36\linewidth]{figures/experiments/acrobot-gym/latent-dim/comparison-log-likelihood-mean-vs-latent-dim.pdf}
				\end{center}
			\end{frame}

			\begin{frame}{Exemplary Run, 18 Latents}{Double Pendulum}
				\vspace{-0.45cm}
				\begin{center}
					\includegraphics[width=0.79\linewidth]{figures/experiments/acrobot-gym/rollout-observations-N0.pdf}
				\end{center}
			\end{frame}
		% end

		\subsection{Comparison with Deep Variational Koopman}
			% Raw results from the morton.py script.
			\begin{comment}
				NRMSE, Our Pendulum vs. Their Gym Pendulum
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_pendulum/{157,206,10,108,59}
				Morton Run, 192 Trajs: benchmarking/morton/results/pendulum-50.json
				Our NRMSE:          0.11334±0.06053
				Our NRMSE (Train):  0.07613±0.04338
				Our NRMSE (Pred.):  0.14050±0.07465
				Morton NRMSE:          0.21042±0.00843
				Morton NRMSE (Train):  0.18867±0.01126
				Morton NRMSE (Pred.):  0.22862±0.01209
				Are we better?         yes
				Are we better (Train)? yes
				Are we better (Pred.)? yes

				NRMSE, Our Gym Pendulum vs. Their Gym Pendulum
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_pendulum_gym/{157,57,7,107,207}
				Morton Run, 192 Trajs: benchmarking/morton/results/pendulum-50.json
				Our NRMSE:          0.37470±0.09991
				Our NRMSE (Train):  0.00626±0.00373
				Our NRMSE (Pred.):  0.53010±0.14133
				Morton NRMSE:          0.21042±0.00843
				Morton NRMSE (Train):  0.18867±0.01126
				Morton NRMSE (Pred.):  0.22862±0.01209
				Are we better?         no
				Are we better (Train)? yes
				Are we better (Pred.)? no

				NRMSE, Our Gym Cartpole vs. Their Gym Cartpole
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_cartpole_gym/{210,160,110,60,10}
				Morton Run, 192 Trajs: benchmarking/morton/results/cartpole-32.json
				Our NRMSE:          0.27037±0.03661
				Our NRMSE (Train):  0.00757±0.00521
				Our NRMSE (Pred.):  0.46946±0.05713
				Morton NRMSE:          0.24579±0.04913
				Morton NRMSE (Train):  0.98522±0.24730
				Morton NRMSE (Pred.):  0.34998±0.08218
				Are we better?         no
				Are we better (Train)? yes
				Are we better (Pred.)? no

				NRMSE, Our Gym Acrobot vs. Their Gym Acrobot
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_acrobot_gym/{118,68,168,218,18}
				Morton Run, 192 Trajs: benchmarking/morton/results/acrobot-64.json
				Our NRMSE:          0.24000±0.04271
				Our NRMSE (Train):  0.15187±0.10722
				Our NRMSE (Pred.):  0.45149±0.05732
				Morton NRMSE:          0.10333±0.00103
				Morton NRMSE (Train):  0.10477±0.00127
				Morton NRMSE (Pred.):  0.10430±0.00094
				Are we better?         no
				Are we better (Train)? no
				Are we better (Pred.)? no
			\end{comment}

			\begin{frame}[c]{Comparison with DVK: Training Error}
				\begin{table}
					\centering
					\begin{tabular}{c|c|c}
						\textbf{Environment} & \textbf{Deep Variational Koopman} & \textbf{Koopman Inference} \\ \hline
						      Pendulum       &         \(0.19 \pm 0.01\)         &   \(\bm{0.08 \pm 0.04}\)   \\
						    Gym Pendulum     &         \(0.19 \pm 0.01\)         &  \(\bm{0.006 \pm 0.004}\)  \\
						    Gym Cartpole     &          \(1.0 \pm 0.2\)          &  \(\bm{0.008 \pm 0.005}\)  \\
						  Double Pendulum    &     \(\bm{0.105 \pm 0.001}\)      &      \(0.2 \pm 0.1\)
					\end{tabular}
				\end{table}
			\end{frame}

			\begin{frame}[c]{Comparison with DVK: Prediction Error}
				\begin{table}
					\centering
					\begin{tabular}{c|c|c}
						\textbf{Environment} & \textbf{Deep Variational Koopman} & \textbf{Koopman Inference} \\ \hline
						      Pendulum       &         \(0.23 \pm 0.01\)         &   \(\bm{0.14 \pm 0.07}\)   \\
						    Gym Pendulum     &      \(\bm{0.23 \pm 0.01}\)       &      \(0.5 \pm 0.1\)       \\
						    Gym Cartpole     &      \(\bm{0.35 \pm 0.08}\)       &     \(0.47 \pm 0.06\)      \\
						  Double Pendulum    &    \(\bm{0.1043 \pm 0.0009}\)     &     \(0.45 \pm 0.06\)
					\end{tabular}
				\end{table}
			\end{frame}

			\begin{frame}[c]{DVK: Rollout (Pendulum)}
				\begin{center}
					\includegraphics[width=\linewidth]{figures/experiments/morton-predictions.pdf}
				\end{center}
			\end{frame}
		% end
	% end

	\section{Conclusion and Future Work}
		\begin{frame}{Conclusion}
			\begin{itemize}
				\item We proposed the Koopman Inference model and algorithm.
				\item Uses an approximate EM algorithm with cubature rules to estimate the nonlinear dynamical system.
				\item Compared to DVK, we managed to get a lot simpler model.
				\item Results look promising for further research.
			\end{itemize}
		\end{frame}

		\begin{frame}{Future Work}
			\begin{itemize}
				\item Extend the Koopman Inference model for handling control inputs.
				\item Employ a Bayesian view on the parameters; gauge the uncertainty on \eg the state dynamics matrix.
				\item Use automatic relevance determination to detect how many latent dimensions are necessary.
				\item Test different network architectures and function approximators for the observation function, \eg residual networks.
				\item Investigate further into the potential overfitting problem.
				\item Also investigate higher latent dimensions.
			\end{itemize}
		\end{frame}
	% end





	\appendix

	\section{Backup Slides} \sectionslide
		\subsection{Expectation Maximization}
			\begin{frame}{Expectation Maximization}
				\begin{itemize}
					\item E-Step: Calculate the expected latents and correlations using filtering/smoothing.
					\item M-Step: Maximize the expected log-likelihood \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \).
				\end{itemize}
			\end{frame}
		% end

		\subsection{The Expected Log-Likelihood}
			\begin{frame}{The Expected Log-Likelihood}
				Markov property yields complete log-likelihood:
				\begin{align*}
					\ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) = \ln p(\vec{s}_1) + \sum_{t = 2}^{T} \ln p(\vec{s}_{t + 1} \given \vec{s}_t) + \sum_{t = 1}^{T} \ln p(\vec{y}_t \given \vec{s}_t)
				\end{align*}
				Expectation \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \) is based on five other expectations:
				\begin{align*}
					\hat{\vec{s}}_t \coloneqq \E\big[ \vec{s}_t \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_t \coloneqq \E\big[ \vec{s}_t \vec{s}_t^T \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_{t, t - 1} \coloneqq \E\big[ \vec{s}_t \vec{s}_{t - 1}^T \biggiven \vec{y}_{1:T} \big]
				\end{align*}
				\begin{align*}
					\hat{\vec{g}}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{G}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \, \vec{g}^T\!(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
				\end{align*}
			\end{frame}
		% end

		\subsection{Future Work}
			\begin{frame}{Future Work}
				\begin{itemize}
					\item Extend the Koopman Inference model for handling control inputs.
					\item Employ a Bayesian view on the parameters; gauge the uncertainty on \eg the state dynamics matrix.
					\item Use automatic relevance determination to detect how many latent dimensions are necessary.
					\item Test different network architectures and function approximators for the observation function, \eg residual networks.
				\end{itemize}
			\end{frame}
		% end
	% end
\end{document}
