% !TeX spellcheck = en_US
% !TeX program = lualatex


% Do not surround the equal signs with spaces, it causes errors!
\documentclass[
	aspectratio=43,
	color={accentcolor=1c},
	logo=false,
	colorframetitle=true,
	handout
]{tudabeamer}

% Include preamble to not clutter this file.
\input{preamble/packages}
\input{preamble/styles}
\input{preamble/tikzGraphs}


\title{Variational Autoencoders for Koopman Dynamical Systems}
\subtitle{B.Sc. Thesis Defense}
\author{Fabian Damken}
\department{Department of Computer Science}
\institute{Intelligent Autonomous Systems}
\date{December 10, 2020}

\logo*{\includegraphics{img/iasLogo}}

\titlegraphic*{\includegraphics{figures/title}}

\newcommand{\hence}{\(\longrightarrow\,\)}

\begin{document}
	\maketitle

	\section{Introduction}
		\begin{frame}{Motivation}
			\begin{itemize}
				\item Control theory for linear systems is highly evolved.
				\item But nonlinear systems are hard\dots
				\item We need ways to linearize a nonlinear system!
				\item<2-> "Classical" Linearization:
					\begin{itemize}
						\item Use small angle approximation (e.g. for pendulum)?
						\item Diverge fast with higher displacements\dots
						\item Only linearize locally, not globally.
					\end{itemize}
				\item<3-> Koopman theory offers a way to do that!
			\end{itemize}
		\end{frame}

		\begin{frame}{The Koopman Operator}
			For a \emph{nonlinear} dynamical system
			\begin{align*}
				\vec{x}_{t + 1} = \vec{F}(\vec{x}_{t})
			\end{align*}
			with \emph{nonlinear} measurements (an \emph{embedding})
			\begin{align*}
				\vec{y}_t = \vec{h}(\vec{x}_t)
			\end{align*}
			the Koopman operator advances these measurements forward in time \emph{linearly}:
			\begin{align*}
				\vec{h}(\vec{x}_{t + 1}) = \mathcal{K} \vec{h}(\vec{x}_t)
				\quad\iff\quad
				\vec{y}_{t + 1} = \mathcal{K} \vec{x}_t
			\end{align*}

			This is possible for every nonlinear dynamical system. And globalizes linearly! But the embedding \(\vec{h}\) is typically infinite-dimensional\dots
		\end{frame}

		\begin{frame}[c]{Koopman Dynamical System}
			\begin{center}
				\tikzKoopmanOperator
			\end{center}
			\begin{center}
				\small
				Adopted from Brunton et al. "Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control".
			\end{center}
		\end{frame}

		\begin{frame}[c]{Linear Gaussian Dynamical System}
			\begin{center}
				\tikzLinearGaussianDynamicalSystem
			\end{center}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( \mat{C} \vec{s}_t + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					\vec{s}_{t + 1} &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					\vec{y}_t &\sim \eqmakebox[c][l]{\(\normal(\mat{C} \vec{s}_t, \mat{R})\)}
				\end{aligned}
			\end{equation*}
		\end{frame}
	% end

	\section{Koopman Inference}
		\begin{frame}[c]{The Koopman Inference Model}
			\begin{center}
				\tikzNonlinearGaussianKoopman
			\end{center}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( {\color{TUDa-9b} \vec{g}(\vec{s}_t)} + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					\vec{s}_{t + 1} &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					\vec{y}_t &\sim \eqmakebox[c][l]{\(\normal({\color{TUDa-9b} \vec{g}(\vec{s}_t)}, \mat{R})\)}
				\end{aligned}
			\end{equation*}
		\end{frame}

		\begin{frame}{How to learn?}
			Goal: Estimate all of the following:
			\begin{itemize}
				\item Latent dynamics matrix \( \mat{A} \).
				\item Measurement function \( \vec{g}_{\vec{\theta}}(\cdot) \) (\ie the parameters \(\vec{\theta}\)). \\ Variational auto-encoder without an amortization network.
				\item Noise covariances \(\mat{Q}\), \(\mat{R}\).
			\end{itemize}
			We employ an EM-algorithm to do that.

			\onslide<2->{
				\begin{alertblock}{Core Problem}
					Some expectations cannot be evaluated in closed form for a nonlinear \( \vec{g}_{\vec{\theta}}(\cdot) \).
				\end{alertblock}
			}

			\onslide<3->{
				\begin{itemize}
					\item Use cubature rules for evaluating the intractable expectations.
					\item No closed form solution for maximizing w.r.t. function parameters \( \vec{\theta} \).
					\item<4->[] \(\qquad\longrightarrow\quad\) Use backpropagation and gradient descent!
				\end{itemize}
			}
		\end{frame}
	% end

	\section{Related Work}
		\begin{frame}{Related Work}
			\begin{itemize}
				\item<+-> Lusch et al. "Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics":
					\begin{itemize}
						\item Use an autoencoder approach with an encoder network.
						\item No probabilistic interpretation.
				\end{itemize}
				\item<+-> Morton et al. "Deep Variational Koopman Models: Inferring Koopman Observations for Uncertainty-Aware Dynamics Modeling and Control":
					\begin{itemize}
						\item Utilize six different neural networks to perform variational inference.
						\item Very complex model, lots of parameters.
						\item Utilize much more training data than our method.
					\end{itemize}
				\item<+-> Becker et al. "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces"
					\begin{itemize}
						\item Proposal of a new type of recurrent neural networks: Kalman network.
						\item Uses high-dimensional linear embeddings with factorized covariances. \\
							\quad \(\longrightarrow\,\) Simple Kalman update rules.
					\end{itemize}
%				\item<+-> Zhang, Vikram et al. "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
			\end{itemize}
		\end{frame}
	% end

	\section{Experiments}
		\begin{frame}{Experimental Questions and Environments}
			\begin{enumerate}
				\item How does the latent dimensionality affect the model performance?
				\item How does our model perform in relation to the DVK model?
				\item How do different feature transformations of the environment affect the model performance?
			\end{enumerate}

			\onslide<2->{
				\vspace{0.5cm}
				Environments:
				\begin{itemize}
					\item \eqmakebox[envs][l]{Pendulum:}                         \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{\textbf<2->{Damped Pendulum:}}     \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{Gym Pendulum:}                     \(\cos\theta\),\, \(\sin\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{Gym Cartpole:}                     \(x\),\, \(\dot{x}\),\, \(\theta\),\, \(\dot{\theta}\)
					\item \eqmakebox[envs][l]{\textbf<2->{Gym Double Pendulum:}} \(\cos\varphi_1\),\, \(\sin\varphi_1\),\, \(\cos\varphi_2\),\, \(\sin\varphi_2\),\, \(\dot{\varphi}_1\),\, \(\dot{\varphi}_2\)
				\end{itemize}
			}
		\end{frame}

		\subsection{Damped Pendulum}
			\begin{frame}{Damped Pendulum}
				\vspace{-0.5cm}
				\begin{equation*}
					m\ddot{\theta} = \frac{g}{L} \sin\theta - d \dot{\theta}
					\quad\longrightarrow\quad
					\ddot{\theta} = \sin\theta - 0.1 \dot{\theta}
				\end{equation*}

				\begin{columns}[c]
					\begin{column}{0.35\linewidth}
						Observations:
						\begin{itemize}
							\item \eqmakebox[pendulum][l]{Displacement:} \(\theta\)
							\item \eqmakebox[pendulum][l]{Velocity:}     \(\dot{\theta}\)
						\end{itemize}
						\begin{center}
							\resizebox{0.85\linewidth}{!}{\tikzSimplePendulum}
						\end{center}
					\end{column}
					\begin{column}{0.65\linewidth}
						\begin{center}
							\includegraphics<2->[width=\linewidth]{figures/experiments/pendulum-damped/log-likelihood.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}[c]{Rollout in Observation Space}{Damped Pendulum}
				\begin{center}
					\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/different-latent-dims-rollouts.pdf}
				\end{center}
			\end{frame}

			\begin{frame}[c]{Effect of the Latent Dimensionality}{Damped Pendulum}
				\begin{columns}[c]
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/latent-dim/comparison-rmse-rollout-train-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/latent-dim/comparison-rmse-rollout-prediction-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}[c]{Exemplary Run and Energy, 10 Latents}{Damped Pendulum}
				\begin{center}
					\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/rollout-observations-N0.pdf}
				\end{center}
				\vspace{-1cm}
				\begin{columns}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-total.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-kinetic.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.333\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/pendulum-damped/energy-R110-N0-potential.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}
		% end

		\subsection{Double Pendulum}
			\begin{frame}{Double Pendulum}
				\begin{columns}[c]
					\begin{column}{0.35\linewidth}
						Observations:
						\begin{itemize}
							\item \eqmakebox[doublePendulum][l]{Displacements:} \\
								\quad \(\cos\varphi_1\),\, \(\sin\varphi_1\), \\
								\quad \(\cos\varphi_2\),\, \(\sin\varphi_2\)
							\item Velocities: \(\dot{\varphi}_1\),\, \(\dot{\varphi}_2\)
						\end{itemize}
						\begin{center}
							\resizebox{0.85\linewidth}{!}{\tikzDoublePendulum}
						\end{center}
					\end{column}
					\begin{column}{0.65\linewidth}
						\begin{center}
							\includegraphics<2->[width=\linewidth]{figures/experiments/acrobot-gym/log-likelihood.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}[c]{Rollout in Reduced Observation Space}{Double Pendulum}
				\vspace{-0.25cm}
				\begin{center}
					\includegraphics[width=0.875\linewidth]{figures/experiments/acrobot-gym/different-latent-dims-rollouts-reduced.pdf}
				\end{center}
			\end{frame}

			\begin{frame}[c]{Effect of the Latent Dimensionality}{Double Pendulum}
				\begin{columns}[c]
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/acrobot-gym/latent-dim/comparison-rmse-rollout-train-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
					\begin{column}{0.5\linewidth}
						\begin{center}
							\includegraphics[width=\linewidth]{figures/experiments/acrobot-gym/latent-dim/comparison-rmse-rollout-prediction-mean-vs-latent-dim.pdf}
						\end{center}
					\end{column}
				\end{columns}
			\end{frame}

			\begin{frame}{Exemplary Run, 18 Latents}{Double Pendulum}
				\vspace{-0.45cm}
				\begin{center}
					\includegraphics[width=0.79\linewidth]{figures/experiments/acrobot-gym/rollout-observations-N0.pdf}
				\end{center}
			\end{frame}
		% end

		\subsection{Comparison with Deep Variational Koopman}
			% Raw results from the morton.py script.
			\begin{comment}
				NRMSE, Our Pendulum vs. Their Gym Pendulum
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_pendulum/{157,206,10,108,59}
				Morton Run, 192 Trajs: benchmarking/morton/results/pendulum-50.json
				Our NRMSE:          0.11334±0.06053
				Our NRMSE (Train):  0.07613±0.04338
				Our NRMSE (Pred.):  0.14050±0.07465
				Morton NRMSE:          0.21042±0.00843
				Morton NRMSE (Train):  0.18867±0.01126
				Morton NRMSE (Pred.):  0.22862±0.01209
				Are we better?         yes
				Are we better (Train)? yes
				Are we better (Pred.)? yes

				NRMSE, Our Gym Pendulum vs. Their Gym Pendulum
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_pendulum_gym/{157,57,7,107,207}
				Morton Run, 192 Trajs: benchmarking/morton/results/pendulum-50.json
				Our NRMSE:          0.37470±0.09991
				Our NRMSE (Train):  0.00626±0.00373
				Our NRMSE (Pred.):  0.53010±0.14133
				Morton NRMSE:          0.21042±0.00843
				Morton NRMSE (Train):  0.18867±0.01126
				Morton NRMSE (Pred.):  0.22862±0.01209
				Are we better?         no
				Are we better (Train)? yes
				Are we better (Pred.)? no

				NRMSE, Our Gym Cartpole vs. Their Gym Cartpole
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_cartpole_gym/{210,160,110,60,10}
				Morton Run, 192 Trajs: benchmarking/morton/results/cartpole-32.json
				Our NRMSE:          0.27037±0.03661
				Our NRMSE (Train):  0.00757±0.00521
				Our NRMSE (Pred.):  0.46946±0.05713
				Morton NRMSE:          0.24579±0.04913
				Morton NRMSE (Train):  0.98522±0.24730
				Morton NRMSE (Pred.):  0.34998±0.08218
				Are we better?         no
				Are we better (Train)? yes
				Are we better (Pred.)? no

				NRMSE, Our Gym Acrobot vs. Their Gym Acrobot
				Our Run,      5 Seeds: tmp_results_grid_search/latent-dim_acrobot_gym/{118,68,168,218,18}
				Morton Run, 192 Trajs: benchmarking/morton/results/acrobot-64.json
				Our NRMSE:          0.24000±0.04271
				Our NRMSE (Train):  0.15187±0.10722
				Our NRMSE (Pred.):  0.45149±0.05732
				Morton NRMSE:          0.10333±0.00103
				Morton NRMSE (Train):  0.10477±0.00127
				Morton NRMSE (Pred.):  0.10430±0.00094
				Are we better?         no
				Are we better (Train)? no
				Are we better (Pred.)? no
			\end{comment}

			\begin{frame}[c]{Comparison with DVK: Training Error}
				\begin{table}
					\centering
					\begin{tabular}{c|c|c}
						\textbf{Environment} & \textbf{Deep Variational Koopman} & \textbf{Koopman Inference} \\ \hline
						      Pendulum       &         \(0.19 \pm 0.01\)         &   \(\bm{0.08 \pm 0.04}\)   \\
						    Gym Pendulum     &         \(0.19 \pm 0.01\)         &  \(\bm{0.006 \pm 0.004}\)  \\
						    Gym Cartpole     &          \(1.0 \pm 0.2\)          &  \(\bm{0.008 \pm 0.005}\)  \\
						  Double Pendulum    &     \(\bm{0.105 \pm 0.001}\)      &      \(0.2 \pm 0.1\)
					\end{tabular}
				\end{table}

				\vspace{0.5cm}
				\begin{equation*}
					\mathit{NRMSE} = \frac{1}{D} \sum_{d = 1}^{D} \frac{1}{y_{\mathrm{max}, d} - y_{\mathrm{min}, d}} \sqrt{\frac{1}{T} \sum\nolimits_{t = 1}^{T} (y_{t, d} - \hat{y}_{t, d})^2}
				\end{equation*}
			\end{frame}

			\begin{frame}[c]{Comparison with DVK: Prediction Error}
				\begin{table}
					\centering
					\begin{tabular}{c|c|c}
						\textbf{Environment} & \textbf{Deep Variational Koopman} & \textbf{Koopman Inference} \\ \hline
						      Pendulum       &         \(0.23 \pm 0.01\)         &   \(\bm{0.14 \pm 0.07}\)   \\
						    Gym Pendulum     &      \(\bm{0.23 \pm 0.01}\)       &      \(0.5 \pm 0.1\)       \\
						    Gym Cartpole     &      \(\bm{0.35 \pm 0.08}\)       &     \(0.47 \pm 0.06\)      \\
						  Double Pendulum    &    \(\bm{0.1043 \pm 0.0009}\)     &     \(0.45 \pm 0.06\)
					\end{tabular}
				\end{table}

				\vspace{0.5cm}
				\begin{equation*}
					\mathit{NRMSE} = \frac{1}{D} \sum_{d = 1}^{D} \frac{1}{y_{\mathrm{max}, d} - y_{\mathrm{min}, d}} \sqrt{\frac{1}{T} \sum\nolimits_{t = 1}^{T} (y_{t, d} - \hat{y}_{t, d})^2}
				\end{equation*}
			\end{frame}
		% end
	% end

	\section{Future Work}
		\begin{frame}{Future Work}
			\begin{itemize}
				\item Extend the Koopman Inference model for handling control inputs.
				\item Employ a Bayesian view on the parameters; gauge the uncertainty on \eg the state dynamics matrix.
				\item Use automatic relevance determination to detect how many latent dimensions are necessary.
				\item Test different network architectures and function approximators for the observation function, \eg residual networks.
			\end{itemize}
		\end{frame}

		\begin{frame}{Conclusion}
			\begin{itemize}
				\item We proposed the Koopman Inference model and algorithm.
				\item Uses an approximate EM algorithm with cubature rules to estimate the nonlinear dynamical system.
				\item Further research is needed into \eg different network architectures.
				\item Results look promising for further research.
			\end{itemize}
		\end{frame}
	% end





	\appendix

	\section{Backup Slides} \sectionslide
		\subsection{Expectation Maximization}
			\begin{frame}{Expectation Maximization}
				\begin{itemize}
					\item E-Step: Calculate the expected latents and correlations using filtering/smoothing.
					\item M-Step: Maximize the expected log-likelihood \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \).
				\end{itemize}
			\end{frame}
		% end

		\subsection{The Expected Log-Likelihood}
			\begin{frame}{The Expected Log-Likelihood}
				Markov property yields complete log-likelihood:
				\begin{align*}
					\ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) = \ln p(\vec{s}_1) + \sum_{t = 2}^{T} \ln p(\vec{s}_{t + 1} \given \vec{s}_t) + \sum_{t = 1}^{T} \ln p(\vec{y}_t \given \vec{s}_t)
				\end{align*}
				Expectation \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \) is based on five other expectations:
				\begin{align*}
					\hat{\vec{s}}_t \coloneqq \E\big[ \vec{s}_t \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_t \coloneqq \E\big[ \vec{s}_t \vec{s}_t^T \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_{t, t - 1} \coloneqq \E\big[ \vec{s}_t \vec{s}_{t - 1}^T \biggiven \vec{y}_{1:T} \big]
				\end{align*}
				\begin{align*}
					\hat{\vec{g}}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{G}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \, \vec{g}^T\!(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
				\end{align*}
			\end{frame}
		% end

		\subsection{Cubature Rules}
			\begin{frame}{Quadrature for High Dimensions: Cubature Rules \\ The Spherical-Radial Cubature Rule}
				Approximation of an arbitrary Gaussian expectation:
				\begin{align*}
					\E_{\vec{x} \,\sim\, \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big]
						&= \int_{\R^n} \! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \\
						&\approx \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}\Big(\! \sqrt{\mat{\Sigma}} \vec{\xi}_i + \vec{\mu} \Big),\quad \vec{\xi}_i = \sqrt{n} [\vec{1}]_i
				\end{align*}
				This finite sum can be evaluated!
			\end{frame}
		% end
	% end
\end{document}
