% !TeX spellcheck = en_US
% !TeX program = lualatex


% Do not surround the equal signs with spaces, it causes errors!
\documentclass[
	aspectratio=43,
	color={accentcolor=1c},
	logo=false,
	colorframetitle=true,
	%handout
]{tudabeamer}

% Include preamble to not clutter this file.
\input{preamble/packages}
\input{preamble/styles}
\input{preamble/tikzGraphs}


\title{Variational Autoencoders for Koopman Dynamical Systems}
\subtitle{B.Sc. Thesis Defense}
\author{Fabian Damken}
\department{Department of Computer Science}
\institute{Intelligent Autonomous Systems}
\date{December 10, 2020}

\logo*{\includegraphics{img/iasLogo}}

\titlegraphic*{\includegraphics{figures/title}}

\newcommand{\hence}{\(\longrightarrow\,\)}

\begin{document}
	\maketitle

	\section{Introduction}
		\begin{frame}{Motivation}
			\begin{itemize}
				\item Control theory for linear systems is highly evolved.
				\item But nonlinear systems are hard\dots
				\item We need ways to linearize a nonlinear system!
				\item<2-> "Classical" Linearization:
					\begin{itemize}
						\item Use small angle approximation (e.g. for pendulum)?
						\item Diverge fast with higher displacements\dots
						\item Only linearize locally, not globally.
					\end{itemize}
				\item<3-> Koopman theory offers a way to do that!
			\end{itemize}
		\end{frame}

		\begin{frame}{The Koopman Operator}
			For a \emph{nonlinear} dynamical system
			\begin{align*}
				\vec{x}_{t + 1} = \vec{F}(\vec{x}_{t})
			\end{align*}
			with \emph{nonlinear} measurements (an \emph{embedding})
			\begin{align*}
				\vec{y}_t = \vec{h}(\vec{x}_t)
			\end{align*}
			the Koopman operator advances these measurements forward in time \emph{linearly}:
			\begin{align*}
				\vec{h}(\vec{x}_{t + 1}) = \mathcal{K} \vec{h}(\vec{x}_t)
				\quad\iff\quad
				\vec{y}_{t + 1} = \mathcal{K} \vec{x}_t
			\end{align*}

			This is possible for every nonlinear dynamical system. And globalizes linearly! But the embedding \(\vec{h}\) is typically infinite-dimensional\dots
		\end{frame}

		\begin{frame}{Koopman Dynamical System}
			\begin{figure}
				\centering
				\tikzKoopmanOperator
				\caption{Adopted from Brunton et al. "Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control".}
			\end{figure}
		\end{frame}

		\begin{frame}{Linear Gaussian Dynamical System}
			\begin{figure}
				\centering
				\tikzLinearGaussianDynamicalSystem
			\end{figure}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( \mat{C} \vec{s}_t + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					\vec{s}_{t + 1} &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					\vec{y}_t &\sim \normal(\mat{C} \vec{s}_t, \mat{R})
				\end{aligned}
			\end{equation*}
		\end{frame}
	% end

	\section{Koopman Inference}
		\begin{frame}{The Koopman Inference Model}
			\begin{figure}
				\centering
				\tikzNonlinearGaussianKoopman
			\end{figure}

			\begin{equation*}
				\begin{aligned}
					\vec{s}_{t + 1} &= \eqmakebox[a][l]{\( \mat{A} \vec{s}_t + \vec{w}, \)}\quad \eqmakebox[b][r]{\( \vec{w} \)} \sim \normal(\vec{0}, \mat{Q}) \\
					\vec{y}_t &= \eqmakebox[a][l]{\( \vec{g}_{\vec{\theta}}(\vec{s}_t) + \vec{v}, \)}\quad \eqmakebox[b][r]{\( \vec{v} \)} \sim \normal(\vec{0}, \mat{R})
				\end{aligned}
				\qquad\iff\qquad
				\begin{aligned}
					\vec{s}_{t + 1} &\sim \normal(\mat{A} \vec{s}_t, \mat{Q}) \\
					\vec{y}_t &\sim \normal\big(\vec{g}_{\vec{\theta}}(\vec{s}_t), \mat{R}\big)
				\end{aligned}
			\end{equation*}

			\onslide<2->{
				Observation function \( \vec{g}(\cdot) \) represents inverse Koopman observation \( \vec{h}^{-1}(\cdot) \).
			}
		\end{frame}

		\begin{frame}{How to learn?}
			Goal: Estimate all of the following:
			\begin{itemize}
				\item Latent dynamics matrix \( \mat{A} \).
				\item Measurement function \( \vec{g}_{\vec{\theta}}(\cdot) \) (\ie the parameters \(\vec{\theta}\)). \\ Variational auto-encoder without an amortization network.
				\item Noise covariances \(\mat{Q}\), \(\mat{R}\).
			\end{itemize}
			We employ an EM-algorithm to do that.

			\onslide<2->{
				\begin{alertblock}{Core Problem}
					Some expectations cannot be evaluated in closed form for a nonlinear \( \vec{g}_{\vec{\theta}}(\cdot) \).
				\end{alertblock}
			}

			\onslide<3->{
				\begin{itemize}
					\item Use cubature rules for evaluating the intractable expectations.
					\item No closed form solution for maximizing w.r.t. function parameters \( \vec{\theta} \).
					\item<4->[] \(\qquad\longrightarrow\quad\) Use backpropagation and gradient descent!
				\end{itemize}
			}
		\end{frame}
	% end

	\section{Related Work}
		\begin{frame}{Related Work}
			\begin{itemize}
				\item<+-> Lusch et al. "Deep Learning for Universal Linear Embeddings of Nonlinear Dynamics":
					\begin{itemize}
						\item Use an autoencoder approach with an encoder network.
						\item No probabilistic interpretation.
				\end{itemize}
				\item<+-> Morton et al. "Deep Variational Koopman Models: Inferring Koopman Observations for Uncertainty-Aware Dynamics Modeling and Control":
					\begin{itemize}
						\item Utilize six different neural networks to perform variational inference.
						\item Very complex model, lots of parameters.
						\item Utilize much more training data than our method.
					\end{itemize}
				\item<+-> Becker et al. "Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces"
					\begin{itemize}
						\item Proposal of a new type of recurrent neural networks: Kalman network.
						\item Uses high-dimensional linear embeddings with factorized covariances. \\
							\quad \(\longrightarrow\,\) Simple Kalman update rules.
					\end{itemize}
%				\item<+-> Zhang, Vikram et al. "SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning"
			\end{itemize}
		\end{frame}
	% end

	\section{Experiments}
		\begin{frame}{Experiments \\ Questions}
			\begin{enumerate}
				\item How does the latent dimensionality affect the model performance?
				\item How does our model perform in relation to the DVK model?
				\item How do different feature transformations of the environment affect the model performance?
			\end{enumerate}

			We will focus on a small subset of the experiments here!
		\end{frame}

		\begin{frame}{Used Environments}{Experiments}
			\begin{itemize}
				\item \eqmakebox[envs][l]{Pendulum:}            \qquad \(\theta\), \(\dot{\theta}\)
				\item \eqmakebox[envs][l]{Damped Pendulum:}     \qquad \(\theta\), \(\dot{\theta}\)
				\item \eqmakebox[envs][l]{Gym Pendulum:}        \qquad \(\cos\theta\), \(\sin\theta\), \(\dot{\theta}\)
				\item \eqmakebox[envs][l]{Gym Cartpole:}        \qquad \(x\), \(\dot{x}\), \(\theta\), \(\dot{\theta}\)
				\item \eqmakebox[envs][l]{Gym Double Pendulum:} \qquad \(\cos\varphi_1\), \(\sin\varphi_1\), \(\cos\varphi_2\), \(\sin\varphi_2\), \(\dot{\varphi}_1\), \(\dot{\varphi}_2\)
			\end{itemize}
		\end{frame}
	% end

	\section{Results}
		\subsection{The Pendulum}
			\begin{frame}{Results: Damped Inverted Pendulum \\ Dynamical System}
				\vspace{-0.7cm}
				\begin{align*}
					\ddot{\varphi} = \frac{g}{L} \sin(\varphi) - d \dot{\varphi}
					\quad\longrightarrow\quad
					\ddot{\varphi} = \sin(\varphi) - 0.1 \dot{\varphi}
				\end{align*}

				\begin{minipage}{0.34\textwidth}
					Observations:
					\begin{itemize}
						\item Displacement \tabto{2.5cm} \(\varphi\)
						\item Velocity     \tabto{2.5cm} \(\dot{\varphi}\)
					\end{itemize}
					\begin{figure}
						\centering
						\begin{tikzpicture}
							\node [draw, circle, fill, minimum width = 0.5cm] (C) at (0, 0) {};
							\node [draw, circle] (mass) at (120:3cm) {\(m\)};
							\coordinate (A) at (90:3cm);
							\draw (C) -- node[left]{\(L\)} (mass);
							\draw [dashed] (C) -- (A);
							\draw pic [draw, angle radius = 1.5cm] {angle=A--C--mass};
							\node at (-0.27, 1.0) {\(\varphi\)};

							\draw [<-] (0.5, 1) -- node[right]{\(g\)} (0.5, 2);
						\end{tikzpicture}
					\end{figure}
				\end{minipage}
				\begin{minipage}{0.65\textwidth}
					\begin{figure}
						\centering
						\onslide<2->{\includegraphics[width=\textwidth]{figures/results/log-likelihood}}
					\end{figure}
				\end{minipage}
			\end{frame}

			\begin{frame}{Results: Damped Inverted Pendulum \\ Rollout/Prediction in Observation Space, 10 Latents}
				\vspace{-0.4cm}
				\begin{figure}
					\centering
					\includegraphics[height = 0.8\textheight]{figures/results/latent-dimensions-experiment_rollout}
				\end{figure}
			\end{frame}

			\begin{frame}{Results: Damped Inverted Pendulum \\ Different Sizes of Latent Dimension}
				\vspace{-0.4cm}
				\begin{figure}
					\centering
					\includegraphics[height = 0.8\textheight]{figures/results/latent-dimensions-experiment_mse}
				\end{figure}
			\end{frame}
		% end
	% end

	\section{Future Work}
		\begin{frame}{Future Work}
			\begin{itemize}
				\item Extend the Koopman Inference model for handling control inputs.
				\item Employ a Bayesian view on the parameters; gauge the uncertainty on \eg the state dynamics matrix.
				\item Use automatic relevance determination to detect how many latent dimensions are necessary.
				\item Test different network architectures and function approximators for the observation function, \eg residual networks.
			\end{itemize}
		\end{frame}

		\begin{frame}{Conclusion}
			\begin{itemize}
				\item We proposed the Koopman Inference model and algorithm.
				\item Uses an approximate EM algorithm with cubature rules to estimate the nonlinear dynamical system.
				\item Further research is needed into \eg different network architectures.
				\item Results look promising for further research.
			\end{itemize}
		\end{frame}
	% end





	\appendix

	\section{Backup Slides} \sectionslide
		\subsection{Expectation Maximization}
			\begin{frame}{Expectation Maximization}
				\begin{itemize}
					\item E-Step: Calculate the expected latents and correlations using filtering/smoothing.
					\item M-Step: Maximize the expected log-likelihood \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \).
				\end{itemize}
			\end{frame}
		% end

		\subsection{The Expected Log-Likelihood}
			\begin{frame}{The Expected Log-Likelihood}
				Markov property yields complete log-likelihood:
				\begin{align*}
					\ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) = \ln p(\vec{s}_1) + \sum_{t = 2}^{T} \ln p(\vec{s}_{t + 1} \given \vec{s}_t) + \sum_{t = 1}^{T} \ln p(\vec{y}_t \given \vec{s}_t)
				\end{align*}
				Expectation \( \E\big[\! \ln p(\vec{s}_{1:T}, \vec{y}_{1:T}) \given \vec{y}_{1:T} \big] \) is based on five other expectations:
				\begin{align*}
					\hat{\vec{s}}_t \coloneqq \E\big[ \vec{s}_t \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_t \coloneqq \E\big[ \vec{s}_t \vec{s}_t^T \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{P}_{t, t - 1} \coloneqq \E\big[ \vec{s}_t \vec{s}_{t - 1}^T \biggiven \vec{y}_{1:T} \big]
				\end{align*}
				\begin{align*}
					\hat{\vec{g}}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
					\qquad
					\mat{G}_t \coloneqq \E\big[ \vec{g}(\vec{s}_t) \, \vec{g}^T\!(\vec{s}_t) \biggiven \vec{y}_{1:T} \big]
				\end{align*}
			\end{frame}
		% end

		\subsection{Cubature Rules}
			\begin{frame}{Quadrature for High Dimensions: Cubature Rules \\ The Spherical-Radial Cubature Rule}
				Approximation of an arbitrary Gaussian expectation:
				\begin{align*}
					\E_{\vec{x} \,\sim\, \normal(\vec{\mu}, \mat{\Sigma})}\big[ \vec{f}(\vec{x}) \big]
						&= \int_{\R^n} \! \vec{f}(\vec{x}) \, \normal(\vec{x} \given \vec{\mu}, \mat{\Sigma}) \dd{\vec{x}} \\
						&\approx \frac{1}{2n} \sum_{i = 1}^{2n} \vec{f}\Big(\! \sqrt{\mat{\Sigma}} \vec{\xi}_i + \vec{\mu} \Big),\quad \vec{\xi}_i = \sqrt{n} [\vec{1}]_i
				\end{align*}
				This finite sum can be evaluated!
			\end{frame}
		% end
	% end
\end{document}
